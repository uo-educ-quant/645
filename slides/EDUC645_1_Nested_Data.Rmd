---
title: "Nested Data"
subtitle: "EDUC 645 - Unit 1"
#author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  editor_options: 
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(easystats, emmeans, DHARMa, ggeffects, haven,
       here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, DiagrammeR, lme4,
       performance, lmerTest, gtsummary)

i_am("slides/EDUC645_1_Nested_Data.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

```


# What is Nested Data?


--
Nesting occurs when variability in our outcome occurs at multiple levels.


--
A common example we've already discussed is student outcomes: If Oregon were interested in looking at factors that influence students' academic performance, performance is expected to differ from student to student. Can performance vary at other levels?


--
- Performance could also vary from classroom to classroom (on average) and from school to school (on average). If the federal government were interested in this question, performance may also vary from state to state.


--
What are other examples of nesting you can think of?


--
Another form of nesting occurs in longitudinal studies, where the outcome is assessed multiple times for each participant. 


---
# What is Nested Data?

One way of thinking of nesting is as a form of <span style = "color:green"> clustering</span>: There are clusters of observations in our data that share something in common. 


--
- Observations share the same context (e.g., the same classroom or school, when there are many classrooms or schools in our data) or could be clustered within each person in our dataset (multiple observations from the same person). 


--
Clustered observations are dependent, meaning they are more similar than they would be if they were unclustered (independent) observations.


--
In the academic performance example, what do you think happens if we don't account for variability in performance at the classroom and school levels? And what about the longitudinal study example, where we have dependent observations within each person?


--
- We get biased estimates of effects and precision.



---
# Analyzing Nested Data

--
In addition to more accurate estimates when we model nested or clustered data appropriately, we might be interested in different relationships at each level:


--
- Student level: Do students who are not routinely bullied have better performance than students who are routinely bullied?


--
- Classroom level: Does the presence of a teaching assistance result in better student performance, compared with classrooms that don't have a teaching assistant?


--
- School level: Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
With a model for nested data - generally called a multilevel model (MLM) or hierarchical linear model (HLM) - we can account for variability at multiple levels *and* test hypotheses at one or more levels using a single model. 


---
# Aggregation Bias

A final concern with nested data is that when we ignore nesting, we risk <span style = "color:green"> aggregation bias</span>. 


--
Imagine we were studying the school-level question on the previous slide, Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
- In this example, school-level data was the mean student performance level for each school, and classroom-level data was the mean student performance level for each classroom. The higher level data are <span style = "color:green"> aggregate </span>data, in the sense that means are aggregates of more granular data. 


--
If we modeled the relationship between two variables - whether or not the school had a sufficient number of academic counselors (IV) and mean student performance for each school (DV) - we might find that having adequate academic counseling was associated with greater average student performance. 


--
- Would this approach give us enough insight to conclude that policy-makers should provide more resources for academic counseling?


--
It might, but what if this relationship was different in each classroom? 


---
# Aggregation Bias

Maybe there was little or no relationship in small classrooms and a stronger relationship in larger classrooms. What's a plausible reason why this could be the case?


--
- Perhaps in small classrooms, teachers can give more attention to students, while in larger classrooms, students are not getting adequate support. 


--
- So, academic counseling makes more of a difference in the larger classrooms, because it is making up for support the teacher is not able to provide. In the smaller classrooms, the students don't need more support and are already high performing without academic counseling. 


--
Aggregation bias would occur if we concluded from our model that adequate academic counseling improves average student performance at the school level, when the relationship is actually more complex and depends on lower-level factors like class size (or presence of teaching assistants, etc). 


--
- If a policy-maker concluded from our school-level findings that the solution was for schools to have more funding for academic counseling, that might be wasting resources if more students would benefit from addressing the underlying, classroom-level issue (i.e., reducing class size by increasing funding to hire more teachers, have more classroom space, etc).


---
# Analyzing Nested Data

Several methods can be used to account for nesting or clustering of data so that we get more accurate estimates of associations (or effects) and error/significance. Some methods also allow for investigating relationships at multiple levels.


--
<span style = "color:green"> Adjusting standard errors: </span> The most straightforward approach is to adjust the model standard errors for clustering. This is based on the idea that a naive model (where the clusters are ignored) would consider every observation independent, in which case there would appear to be more information in the data than there really is (resulting in artificially narrow confidence intervals). 


--
<span style = "color:green"> Subgroup-level analyses: </span> With large enough sample sizes and appropriate adjustment for multiple comparisons, we could carry out our analyses within each subgroup or cluster.


--
<span style = "color:green"> Multilevel (or mixed-effects) models: </span> The most common analytic approach for nested data, which can address dependency among observations and can be used to examine the relationships between the outcome and (different) predictors at each level. 


--
<span style = "color:green"> Repeated-measures models: </span> With longitudinal data, there are several approaches that can be used to account for repeated observations from the same participant (e.g., growth curve modeling).


---
# Example data

The Stanford Education Data Archive (SEDA) was launched in 2016 to provide nationally comparable, publicly available test score data for U.S. public school districts. 


--
SEDA allows researchers to study relationships between educational conditions, contexts, and outcomes (e.g., student math achievement) at the district level.


--
We'll look at district-level data for 103 Oregon school districts from the 2017-18 academic year. 

- Each district has 4 observations (rows), one for each grade from 3-6.

- Observations with missing values on any of the key variables were deleted for simplification.


---
# Data import

Let's import and clean the data, including limiting the data to math students and creating a factor for grade: 

```{r}
seda <- read_csv(here::here("data", "seda_oregon.csv")) %>%
  filter(subject == "mth") %>% 
  mutate(grade = as_factor(recode(grade, '3' = "Grade 3", 
                                         '4' = "Grade 4", 
                                         '5' = "Grade 5", 
                                         '6' = "Grade 6")))
```


---
# View the data 

```{r}
seda %>% select(-subject) %>% head(seda, n = 5) %>% print_md()
```


--
.small[Notice the 4 observations for each grade in each district. Whenever you have multiple observations that share something in common (share the same district, group, person), this is an indication that you probably have nested data!]


---
# Understanding clustered data

Let's look at the average math achievement score (`achievement`) across all grades for each Oregon district.


--
```{r, echo = FALSE, fig.retina=3}
seda %>% 
  group_by(district) %>% 
  mutate(mean = mean(achievement)) %>%
  ungroup() %>% 
  select(district, mean) %>%
  ggplot(aes(district, mean)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```


--
<span style = "color:green"> What is the relationship between proportion of ELL students and math achievement in Oregon districts? </span>


---
# % ELL and achievement


--
We might consider averaging all 4 grades' math achievement scores in each district and fit a simple linear regression for the association between % ELL and math achievement:


--
```{r, echo = FALSE, fig.retina=3}
seda %>%
  group_by(district) %>% 
  mutate(achievement = mean(achievement)) %>% 
  ungroup() %>% 
  ggplot(aes(percent_ell, achievement)) +
  geom_smooth(method='lm', se = FALSE) +
  geom_point(alpha = 0.1) + 
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


--
But do we have more information if we didn't average the outcome over the 4 grades?

---
# Achievement for all grades

```{r, echo = FALSE}
seda %>% 
  group_by(district) %>% 
  mutate(mean = mean(achievement)) %>%
  ungroup() %>% 
  select(district, mean) %>%
  ggplot(aes(district, mean)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```

---
# Achievement for each grade

```{r, echo = FALSE}
seda %>% 
  group_by(district, grade) %>% 
  mutate(mean = mean(achievement)) %>% 
  ungroup() %>% 
  select(district, grade, mean) %>%
  ggplot(aes(district, mean, color = grade)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```


--
If we fit a model for each grade, what could differ about the regression lines for each grade?


---
# Relationship by grade

Fitting a simple linear regression model for each grade (i.e., 4 separate models):


--
```{r, echo = FALSE, fig.retina=3}
seda %>%
  group_by(district, grade) %>% 
  mutate(achievement = mean(achievement)) %>% 
  ungroup() %>% 
  ggplot(aes(percent_ell, achievement, color = grade)) +
  geom_smooth(method='lm', se = FALSE) +
  geom_point(alpha = 0.1) +
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


--
Now that you see regression lines for each grade, what model parameters (intercepts, slopes) did end up differing for each grade?

---
# Intraclass correlation coefficient

We can estimate the differences we've been discussing using the <span style = "color:green"> intraclass correlation coefficient (ICC) </span>. The ICC gives us an estimate of how much of the variability in our outcome is simply because of differences in the outcome between groups (in our example, grades). 


--
The ICC ranges from 0 to 1. The larger the ICC, the greater the proportion of variability in the outcome that is due to group differences. The ICC is one tool we can use to gauge whether clustering is important enough to warrant multilevel modeling.


--
<span style = "color:green"> The ICC for the math achievement outcome in our data is 0.645 or 64.5%. </span> How would you interpret this? 


--
- 64.5% of the variation in the math achievement outcome is attributable to differences in achievement between grades. 


--
- Because the ICC is telling us that more than two-thirds of the variability in the outcome is because of grade (and has nothing to do with ELL proportion), we know that our findings could mask important differences in grade-level achievement.


---
# Simple linear regression

```{r, echo=T}
m1 <- lm(achievement ~ percent_ell, data = seda)
coef(m1) # Extracting intercept and %ELL coefficient
```


--
This is called a <span style = "color:green"> fixed-effect model. </span> We're assuming that grades are sufficiently similar that we can average the slopes and intercepts for each grade, and this will give us an accurate enough picture of the relationship of interest. 


--
Another way of saying this is each grade's slope and intercept are set - "fixed" - to be the average slope and intercept across grades.


---
# Simple linear regression

When thinking about whether a fixed-effect approach makes sense, we can apply the same reasoning we use when thinking about any average. 


--
Any mean is a single value (say, 31.5) that represents a set of values:


--
- If that set is close to the average (say, 29 and 34), then the single value of 31.5 is probably a good representation of the underlying values (i.e., it's not very different from 29 or 34). 


--
- But the same mean of 31.5 could represent the values of 0 and 63.


--
0-63 is the range of the Beck Depression Index (BDI): Does a mean of 31.5 convey that one person has no depression symptoms (0) and a second person has severe depression (63)?


---
# Mixed-effects model

Instead of the simple linear regression (fixed-effect model), we can fit a mixed-effects model that allows intercepts, or both intercepts and slopes, to vary across grades. 


--
In other words, this approach allows us to examine the underlying values of the intercepts and/or slopes that are contributing to the fixed-effect model estimates (the average of each grade's intercept and slope). 


--
- Another term for this is *disaggregating*, in the same sense as we used when talking about aggregation bias. We're potentially reducing our risk of aggregation bias by disaggregating results by grade. 


--
For now, let's look at what happens if we let the intercept (the average math achievement level at 0% ELL) vary by grade. This is called a <span style = "color:green"> random-intercepts model </span> because the intercept is allowed to vary for each grade. 


---
# Random-intercepts model

For mixed-effects models, we'll use the `lmer()` function from the base R package *lme4*, which uses a function and formula style similar to the models we've been fitting:

--
```{r}
m2 <- lmer(achievement ~ percent_ell + (1 | grade), data = seda)
```


--
The only difference from the usual linear regression specification is the element in parentheses. This is the "random effect" part of the model, or what we want to allow to differ across groups/clusters. 


--
- What is after the `|` is the variable that indicates the group/cluster. What is before the `|` is what we're interested in allowing to vary for each group. 


--
- Like the basic regression model where 1 is used to fit a model that estimates only the intercept, here the `(1 | grade)` means "only the intercept (1) can vary by grade."


---
# Random-intercepts model

```{r}
coef(m2)$grade # Specify the grouping variable
```


--
What do you notice is different for each grade?



---
# ICC and variability explained

In practice, before fitting a mixed-effects model, we'd fit the equivalent of the null model we're used to and use it to calculate the ICC. 


--
```{r}
m0 <- lmer(achievement ~ 1 + (1 | grade), data = seda)
performance::icc(m0)
```


--
There's a relationship between the ICC and variability explained, or $R^2$:


--
.pull-left[
```{r}
performance::r2_nakagawa(m0)
```
]


--
.pull-right[
In essence, the mixed-effect model is resetting our baseline of "explainable" variation from all of the variation in achievement (100%) to only the portion that is not attributable to differences in achievement by grade (100% - 65% = **35%**). 
]

---
# Adding random slopes

So far we've considered one intercept and slope for all grades or examining individual intercepts for each grade.


--
It's fairly clear from the earlier plot that the random-intercepts model is probably sufficient for our data. But let's illustrate what happens if we examine intercepts *and* slopes for each grade (random-slopes-and-intercepts model).


--
<span style = "color:green"> Random-slopes-and-intercepts model: </span>

```{r}
m3 <- lmer(achievement ~ percent_ell + (percent_ell|grade), data = seda)
coef(m3)$grade
```


--
.small[Note that we can look at slopes for each grade with only one intercept across grades (random-slopes model), but in practice we wouldn't typically test this model. If the random-intercepts model is better fitting than the fixed-effect model, the next logical step in model fitting is to add random slopes to the random-intercepts model (i.e., the random-slopes-and-intercepts model).]

---
# Fixed effect vs random effects

```{r, echo = FALSE, fig.retina=3}
seda %>% 
  ggplot(aes(percent_ell, achievement)) +
  geom_smooth(method='lm', se = FALSE, color = "deeppink") +
  geom_point(alpha = 0.1) + 
  geom_smooth(aes(color = grade), method='lm', se = 0) +
  geom_point(aes(color = grade), alpha = 0.1) +
  scale_color_hue(l=80, c=30) +
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


---
# Bringing it together

--
.pull-left[
The <span style = "color:green"> random-slopes-and-intercepts model </span> gives us the slope and intercept for each cluster:

```{r}
coef(m3)$grade
```
]

--
.pull-right[
The <span style = "color:green"> fixed-effect model </span> gives us the average (or aggregation) of the different intercepts and slopes across clusters:
 
```{r}
coef(m1)
```
]


--
.pull-left[
In other words, the fixed-effect results are the average of the intercepts and slopes for each grade:
]


--
.pull-right[
```{r, echo=FALSE}
cbind(`(Intercept)` = mean(unlist(coef(m3)$grade[1])), 
      percent_ell = mean(unlist(coef(m3)$grade[2])))
```
]


--
So, back to averages: Does the average of the slope and the intercept give a good enough representation of the slopes and intercepts across grades? If not, which one do you think we need to examine by grade? *We'll explore how to answer this question next unit.*

---
# Back to variability explained


--
.pull-left[

<span style = "color:green"> Our understanding from the fixed-effect model: </span>All variation in math achievement could be explained, but %ELL explained very little of it (marginal $R^2$ from previous slide, or $R^2$ from simple linear regression). We might have concluded that %ELL was not an important predictor of math achievement. 
]


.pull-right[
```{r, echo = FALSE, fig.retina=3}
m1r2 <- summary(m1)$r.squared
m0icc <- performance::icc(m0)$ICC_unadjusted

piedata1 <- tibble(
  Variability=c("Explained by ELL%", "Unexplained"),
  value=c(m1r2, 1-m1r2)
)

ggplot(piedata1, aes(x="", y=value, fill=Variability)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.position="top", 
        legend.title = element_blank(),
        legend.text = element_text(size=18),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "pt"))
```
]


--
.pull-left[
<span style = "color:green"> Our understanding from the random-intercepts model: </span>Only one-third of variation in math achievement could be explained after accounting for differences in achievement by grade (65%). %ELL and grade differences explain 72% of variability in math achievement (conditional $R^2$). This could lead to a very different conclusion compared with the fixed-effect model. 
]


.pull-right[
```{r, echo = FALSE, fig.retina=3}
piedata2 <- tibble(
  Variability=c("Explained by ELL% + Grade", "Unexplained"),
  value=c(m1r2+m0icc, 1-(m1r2+m0icc))
)

ggplot(piedata2, aes(x="", y=value, fill=Variability)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.position="top", 
        legend.title = element_blank(),
        legend.text = element_text(size=18),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "pt"))
```
]


---
# To Dos

### Suggested Readings

- Little et al 2013 before next class 

### Quiz

- Unit 1 quiz available Friday 4/12 and due by 11:59 PM on Monday 4/15.


