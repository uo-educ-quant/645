---
title: "Outcome Types & Reliability"
subtitle: "EDUC 645 (Unit 1)"
#author:
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(NHANES, easystats, emmeans, DHARMa, ggeffects,
       here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, lfe, arsenal, ggpubr, stargazer, fixest, gtsummary, huxtable, aod, MBESS)

i_am("slides/EDUC645_1_Outcome_Types_Reliability.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

NHANES_raw <- NHANES::NHANES %>% 
  select(ID, SurveyYr, Gender, Age, AgeDecade, Race3, Education, Poverty, Diabetes, DaysMentHlthBad, Depressed, PhysActive, SleepTrouble, SleepHrsNight) %>% 
  mutate(Race_Eth = as_factor(case_when(Race3 == "Mexican" | Race3 == "Hispanic" ~ "Hispanic/Latino",
                                        TRUE ~ Race3)),
         PhysActive = as_factor(case_when(PhysActive == "No" ~ "Inactive",
                                          PhysActive == "Yes" ~ "Active")),
         PhysActive = factor(PhysActive, levels = c("Active", "Inactive")),
         DaysMHBad_most = as_factor(case_when(DaysMentHlthBad < 10 ~ "No",
                                              DaysMentHlthBad >= 10 ~ "Yes")),
         DaysMHBad_most = factor(DaysMHBad_most, levels = c("No", "Yes"))) %>% 
  rename(Depress_Freq = Depressed, Sex = Gender, DaysMHBad_count = DaysMentHlthBad, Sleep_Trouble = SleepTrouble) %>% 
  select(ID, SurveyYr, Age, Sex, Race_Eth, Diabetes, Depress_Freq, PhysActive, DaysMHBad_count, DaysMHBad_most, Sleep_Trouble, SleepHrsNight) 

ah_happiness <- rio::import(here::here("data", "ah_happiness.csv")) %>% 
  drop_na() %>% 
  select(q1:q8) %>% 
  mutate(q1 = recode (q1, "0"=3, "1"=2, "2"=1, "3"=0),
         q2 = recode (q2, "0"=3, "1"=2, "2"=1, "3"=0),
         q3 = recode (q3, "0"=3, "1"=2, "2"=1, "3"=0),
         q4 = recode (q4, "0"=3, "1"=2, "2"=1, "3"=0),
         q7 = recode (q7, "0"=3, "1"=2, "2"=1, "3"=0),
         q8 = recode (q8, "0"=3, "1"=2, "2"=1, "3"=0))
```

class: middle, inverse

# Dichotomous outcomes 


---
# What makes a dichotomous outcome dichotomous?

Dichotomous (or binary) outcomes are a broad group of outcomes that take on only two values. 


--
- In a dataset, these may be represented as 0 or 1, or with names that correspond to two categories. 


--
The fact that the outcome can only be 0 or 1 (or two mutually exclusive categories) implies certain things about the meaning of the outcome. **.blue[Any ideas?]**
  

--
One implication is that the outcome describes the presence or the absence of a characteristic, or two states of being. 


--
- More specifically, a person can't possess *some* or *part* of the characteristic. Or, there isn't some "in-between" state of being (like "semi-alive").


---
# Examples

Depression is often measured using a scale, like the Beck Depression Index (BDI), which results in a numerical score (ranges from 0-63). Often scales have a cutoff that puts an individual into the diagnostic range (29+ on the BDI = "Severe Depression").


--
A researcher might be interested in life experiences associated with severe depression, so they code responses to the scale as 0 = no or mild/moderate depression (score 0-28), or 1 = severe depression (score 29-63).
  

--
- **.blue[Can you think of any potential downsides of this approach?]**
  

--
Other times, an outcome will be measured dichotomously, but we wonder if that is a reasonable representation of the outcome construct.


--
- Students' affective attitude toward their principal ("The principal is nice to me" - Yes/No)


--
- **.blue[Could there be an intermediate between "not nice to me" and "nice to me"? Is it realistic to think that a principal would be "not nice" to a student? Is "niceness" the best construct?]**


---
# Example data: NHANES

NHANES (National Health and Nutrition Examination Study) is an annual US survey of about 5,000 children and adults. Participants answer an interview questionnaire and receive a physical examination, and data are collected on demographic characteristics, nutrition and physical activity, and physical and mental health. We'll look at a few variables initially:


--
`Diabetes` is whether or not (yes/no) a participant has been diagnosed with diabetes.

`PhysActive` represents whether the "participant does moderate or vigorous-intensity sports, fitness or recreational activities" (see [here](https://cran.r-project.org/web/packages/NHANES/NHANES.pdf)). Participants answered "yes" or "no".

`PhysActiveDays` is the number of days in a typical week that the participant does those activities. 

---
# NHANES

We'll use data from the 2011-12 survey year and limit the sample to adults and adolescents (13 or older). We'll also remove any participant with missing data (later in the term we'll discuss why this might be a bad idea!).


```{r}
NHANES_data1112 <- NHANES_raw %>% 
  filter(SurveyYr == "2011_12", Age > 12) %>% 
  drop_na()
```


--
```{r}
NHANES_data1112 %>% 
  select(ID:PhysActive) %>% 
  head(n = 3)
```

---
# Diabetes (No/Yes)

```{r, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(Diabetes)) +
  geom_bar(fill = c("yellow", "royalblue"), width = 0.5, alpha = 0.8) +
  labs(x = "Has Diabetes",
       y = "Number of Participants")
```


---
# PhysActive (Inactive/Active)

```{r, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(PhysActive)) +
  geom_bar(fill = c("yellow", "royalblue"), width = 0.5, alpha = 0.8) +
  labs(x = "Participates in Physical Activities",
       y = "Number of Participants")
```

---
class: middle, inverse

# Representing Dichotomous Outcomes

---
# Probability and odds

How do we represent dichotomous outcomes? We could calculate a <span style = "color:green"> simple proportion: </span>


--
$$\frac{N\ of\ Participants\ with\ Diabetes}{N\ of\ All\ Participants} = 0.102\ or\ 10.2\%$$

--
<span style = "color:green"> That's a probability. </span> The probability of having diabetes. Probabilities range from 0-1 or 0-100%. We can do the same for "Participants without Diabetes". Notice we'd get the same result if we subtracted our first probability from 1 (1 - 0.09).

$$\frac{Participants\ without\ Diabetes}{All\ Participants} = 0.898\ or\ 89.8\%$$


--
Dividing these two values gives us the raw *odds* of having diabetes in our data. Odds can have any positive value, and an odds of zero means the outcome didn't occur. 

$$\frac{Probability\ of\ Having\ Diabetes}{Probability\ of\ Not\ Having\ Diabetes} = \frac{0.102}{0.898} = 0.11$$


--
The odds of having diabetes are low. This makes sense: only 10% of participants had diabetes. When the probability of the outcome is low (i.e., the outcome is uncommon), probability and odds are similar. As the probability of increases, so do the odds, but odds increase *almost* exponentially compared with probability.


---
# Odds ratio


Maybe we're interested in whether there is a relationship between being physically active and having diabetes. 


--
- That means we're interested in comparing the odds of having diabetes among participants who are physically active to the odds of having diabetes among participants who are not physically active. 


--
We can do this by dividing these two odds; in other words, a ratio. This ratio tells us how much higher or lower the odds of the outcome are for one group compared with the other group. 


--
- We can calculate the odds just as we did before, but within the physically inactive group (`PhysActive` = Inactive) and the physically active group (`PhysActive` = Active): 



$$\ Inactive\ Participants: \frac{Probability\ of\ Having\ Diabetes}{Probability\ of\ Not\ Having\ Diabetes} = \frac{0.14}{0.86} = 0.163$$


$$\ Active\ Participants: \frac{Probability\ of\ Having\ Diabetes}{Probability\ of\ Not\ Having\ Diabetes} = \frac{0.07}{0.93} = 0.075$$

---
# Odds ratio

We divide these odds to find our odds ratio (OR):

$$\ OR = \frac{Odds\ of\ Inactive\ and\ Having\ Diabetes}{Odds\ of\ Active\ and\ Having\ Diabetes} = \frac{0.163}{0.075} = 2.17$$


--
An odds ratio of 1 means the odds of having diabetes are the same for participants who were active and those who were inactive. An odds ratio greater than 1 means inactive participants (the numerator) have greater odds of having diabetes than active participants. 


--
- So, compared with being active, being inactive is associated with higher odds of having diabetes.
  
---
# Odds ratio

We can also flip the odds ratio if our question is focused on being active:

$$\ OR = \frac{Odds\ of\ Active\ and\ Having\ Diabetes}{Odds\ of\ Inactive\ and\ Having\ Diabetes} = \frac{0.075}{0.163} = 0.46$$


--
So, compared with being inactive, being active is associated with lower odds of having diabetes. 


--
Both interpretations are mathematically equivalent, but one might be preferred based on the research question or setting: 

- If the interest is on protective factors for diabetes, then you might put the emphasis on being active ("Being active is associated with lower odds of diabetes") 
  
- If the focus is on risk factors for diabetes, then you might put the emphasis on being inactive ("Being inactive is associated with greater odds of diabetes") 

---
# Using R (step-by-step)

View a table with the number of participants with diabetes among inactive participants (bottom row) and active participants (top row)"

```{r, echo = TRUE}
table(NHANES_data1112$PhysActive, NHANES_data1112$Diabetes)
```


--
For example, the number of participants who are inactive and have diabetes is N = 211, and summing the row (211 + 1300) gives the total number of inactive participants (N = 1,511). 


---
# Using R (step-by-step)

Then we calculate the probability of having diabetes for inactive participants:

```{r, echo = TRUE}
round((211 / (211 + 1300)), digits = 3)
```


--
And the probability of NOT having diabetes for inactive participants:

```{r, echo = TRUE}
round((1300 / (211 + 1300)), digits = 3) # Equal to 1 - 0.14.
```


---
# Using R (step-by-step)


We then calculate the odds of having diabetes for inactive participants using those results:
```{r, echo = TRUE}
round((0.14 / 0.86), digits = 3)
# Or: round((0.14 / (1 - 0.14)), digits = 3)
```


--
And do the same for active participants (top row):

```{r, echo = TRUE}
round((127 / (127 + 1689)), digits = 3)
round((0.07 / (1 - 0.07)), digits = 3)
```


---
# Using R (step-by-step)

Finally, we divide the two odds to get the odds ratio:

```{r, echo = TRUE}
round((0.163 / 0.075), digits = 2)
```


---
# Interpretation

To summarize (OR):

$$\ OR = \frac{\frac{Probability\ of\ Being\ Inactive\ and\ Having\ Diabetes}{Probability\ of\ Being\ Inactive\ and\ Not\ Having\ Diabetes}}{\frac{Probability\ of\ Being\ Active\ and\ Having\ Diabetes}{Probability\ of\ Being\ Active\ and\ Not\ Having\ Diabetes}} = 2.17$$


--
Inactive participants had a higher odds of having diabetes than active participants. Participants who were inactive had 2.17 times the odds of diabetes than participants who were active.

Compared with being active, being inactive is associated with a greater odds of diabetes. Inactive participants had 117% greater odds of having diabetes than active participants. 


--
- When we're describing *more than* for a ratio, we have to subtract 1 from the ratio (because we're starting from 1, or no difference between groups, and saying how much greater the odds are than that). This is how we get 117% (2.17 - 1 = 1.17). For the flipped ratio with odds of diabetes in the active group in the numerator, the odds ratio was 0.46. Because this ratio is less than 1, here we subtract *from 1:* 1 - 0.46 = 0.54 ("Being active is associated with a 54% reduction in odds of diabetes compared with being inactive.")


---
# So, why use odds?

In the example we've just been discussing, we were interested in the relationship between one independent variable (whether or not a participant was physically active `PhysActive`) and the outcome, diabetes status. 


--
Because we have only have two variables, we can represent our data in a simple table known as a 2-by-2 table or contingency table:

|           | No Diabetes | Diabetes   |
|:----------|:------------|:-----------|
| Active    | 1692        | 127        |
| Inactive  | 1304        | 211        |


--
Both an odds ratio and risk ratio can be calculated directly from this table, and we can test whether the difference between groups that is quantified by the ratio is significant.


--
But, what if we want to look at the relationship between a *continuous* independent variable, like `Age`, and a dichotomous outcome like `Diabetes`? 

---
# So, why use odds?

**This implies that there is contingency table for each value of the independent value (i.e., for each year of age ranging from 13 to 80):**


--
**Age 61**

|           | No Diabetes | Diabetes   | 
|:----------|:------------|:-----------|
| Inactive  | 24          | 7          |
| Active    | 16          | 2          |


--
**Age 62**

|           | No Diabetes | Diabetes   |
|:----------|:------------|:-----------|
| Inactive  | 13          | 12         |
| Active    | 16          | 7          |


---
# So, why use odds?

Then, what happens when we want to include *multiple* independent variables? These could be continuous, dichotomous, categorical, or a mix.


--
- This is necessary for many common research questions. For example: What are the odds of diabetes for inactive participants compared with active participants, controlling for participant age?


--
Hopefully, it's becoming clear that looking at anything more than a basic relationship between two dichotomous variables will not result in easily interpretable results (i.e., a single estimate of the relationship). 


--
Instead we need an analysis method that - like the linear regression model we're already familiar with - provides us a single estimate of the relationship of interest, while also accommodating the unique properties of dichotomous outcomes. 


--
- There are a few options, but the most commonly used is <span style = "color:green"> logistic regression. </span>


---
class: middle, inverse

# Count Outcomes


---
# Normally distributed data

Let's first look at a more normally distributed variable: `SleepHrsNight`, the self-reported number of hours of sleep a participant usually gets.


--
```{r, warning=FALSE, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(SleepHrsNight)) +
  geom_histogram(binwidth = 0.5)
```


---
# Count data

`DaysMHBad_count`, the number of days per month a participant reports experiencing poor mental health.


--
```{r, warning=FALSE, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(DaysMHBad_count)) +
  geom_histogram(binwidth = 1)
```


---
# What makes a count outcome a count?

Counts indicate how many of an outcome are present, or often, how many of an outcome are present *in a specific timeframe* (aka a *rate*).


--
In addition, a distribution of counts is:


--
- Usually *skewed*, meaning that some values are much more common than others - often a lot, or none at all (zero).


--
- Often *overdispersed*, which means the counts vary more than would be expected.


--
Counts/rates require analyses that account for these properties. These methods are similar to those used for dichotomous outcomes. 


--
 - **.blue[Why do you think that is?]** *Hint: Think about why `SleepHrsNight` was normally distributed and `DaysMHBad_count` was distributed like a count.*


--
 - Another way of thinking of counts is that they measure a series of dichotomous outcomes.


---
class: middle, inverse

# Reliability


---
# Reliability 

Reliability is general term for consistency of measurement, but there are different forms of consistency that are important. 


--
* Scale items (questions) are **.blue[internally consistent]**: they are likely to be measuring the *same* underlying (aka latent) construct, like depression or happiness.


--
  - Coefficients alpha and omega estimate internal consistency


--
* Responses to the scale won't systematically differ just because measurement is conducted at different times, by different researchers, or with different forms of the scale.


--
  - *Test-retest reliability* tests stability over time


--
  - *Inter-rater reliability* tests equivalence among researchers


--
  - *Alternative forms* tests reliability of different item orders or wording 


--
* Individuals with the same true value of the construct will have the same score on the scale.


--
  - Best examined with latent variable methods like Item Response Theory (IRT).

---
# Internal consistency reliability

How well a set of items correlate with each other (“hang together”) is internal consistency reliability. If items are strongly correlated, this gives some evidence that they are measuring the same construct.


--
The most common internal consistency measure (coefficient) is Alpha; another option is coefficient Omega. Both range from 0-1, with estimates closer to 1 indicating greater internal consistency. 


--
* Omega uses more realistic assumptions that improve the accuracy of the estimate. 


--
* Internal consistency estimates can be impacted to missing data, the number of items, and whether items share the same direction. *Having one or more items coded in the opposite direction of other items is a common source of (artificially) low internal consistency.*


--
* Estimates are specific to the sample: A scale may have greater internal consistency in one dataset and less in another. 


--
* Estimates are accompanied by some degree of uncertainty, so it is important to also report a confidence interval.


---
# Data example - Add Health

Eight items from a rating scale intended to measure the construct of happiness.


--
"How often was each of the following things true during the past seven days?" 

  - q1: "You were bothered by things that usually don't bother you."
  - q2: "How often do you feel isolated from others?"
  - q3: "You had trouble keeping your mind on what you were doing."
  - q4: "You felt depressed."
  - q5: "You felt happy."
  - q6: "You enjoyed life."
  - q7: "You felt sad."
  - q8: "You felt that people disliked you."

(0 = never or rarely, 1 = sometimes, 2 = a lot of the time, 3 = most or all of the time)

---
# Example data: Add Health

```{r}
ah_happiness %>% 
  head()
```


--
We'll use the MBESS package to calculate coefficients with accompanying confidence intervals.

---
# Alpha

```{r} 
ah_happiness %>% 
  select(q1:q8) %>% 
  ci.reliability(type = "alpha", interval.type = "11")
```

---
# Omega

```{r}
ah_happiness %>% 
  select(q1:q8) %>% 
  ci.reliability(type = "omega", interval.type = "mlr")
```


---
# Standards for reliability

Most people consider an alpha (or omega) value of 0.8 or greater to be optimal, but what is acceptable depends on context and the implications for the measure's use.


--
Measures with significant impact will likely require higher reliability than less important measures:
- $\alpha = .75$ may be acceptable for an online personality quiz, whereas it may not be unacceptable for a medical test result


--
Some recommendations:

* Rely on literature and content expertise when interpreting reliability

* Use well-validated and widely used measures whenever possible

* Do not use listwise deletion to estimate internal consistency (MBESS functions will impute using FIML, if specified)

* Present estimate with confidence intervals

---
# To Dos

### Suggested Readings

- BMLR Ch. 8.1-8.5 (pp. 211-231) before next class 







