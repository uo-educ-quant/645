---
title: "Analyzing Count Outcomes"
subtitle: "EDUC 645 (Unit 3)"
#author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  editor_options: 
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(easystats, emmeans, DHARMa, ggeffects, haven,
       here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom.mixed, xaringanthemer, reshape2, DiagrammeR, lme4,
       performance, lmerTest, gtsummary, NHANES)

i_am("slides/EDUC645_3_PoissonNB.Rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

NHANES_data1112 <- NHANES::NHANES %>% 
  select(ID, SurveyYr, Gender, Age, AgeDecade, Race3, Education, Poverty, Diabetes, DaysMentHlthBad, Depressed, PhysActive, SleepTrouble, SleepHrsNight) %>% 
  mutate(Race_Eth = as_factor(case_when(Race3 == "Mexican" | Race3 == "Hispanic" ~ "Hispanic/Latino",
                                        TRUE ~ Race3)),
         PhysActive = as_factor(case_when(PhysActive == "No" ~ "Inactive",
                                          PhysActive == "Yes" ~ "Active")),
         PhysActive = factor(PhysActive, levels = c("Active", "Inactive")),
         DaysMHBad_most = as_factor(case_when(DaysMentHlthBad < 10 ~ "No",
                                              DaysMentHlthBad >= 10 ~ "Yes")),
         DaysMHBad_most = factor(DaysMHBad_most, levels = c("No", "Yes"))) %>% 
  rename(Depress_Freq = Depressed, Sex = Gender, DaysMHBad_count = DaysMentHlthBad, Sleep_Trouble = SleepTrouble) %>% 
  select(ID, SurveyYr, Age, Sex, Race_Eth, Diabetes, Depress_Freq, PhysActive, DaysMHBad_count, DaysMHBad_most, 
         Sleep_Trouble, SleepHrsNight) %>%   
  filter(SurveyYr == "2011_12", Age > 12) %>% 
  filter(if_all(everything(), ~!is.na(.))) %>% 
  select(-SurveyYr)

drvisits <- haven::read_dta(here::here("data", "drvisits.dta")) %>% 
  as_tibble() %>% 
  rename(educ_yrs = educ,
         income_log = loginc,
         reform_n = reform,
         visits = numvisit) %>% 
  mutate(health_status = case_when(badh == 0 ~ "Good",
                                   badh == 1 ~ "Poor"),
         health_status = factor(health_status, levels = c("Good", "Poor")),
         marital_status = case_when(married == 0 ~ "Married",
                                    married == 1 ~ "Unmarried"),
         marital_status = factor(marital_status, levels = c("Married", "Unmarried")),
         intervention = case_when(reform_n == 0 ~ "Pre",
                            reform_n == 1 ~ "Post"),
         intervention = factor(intervention, levels = c("Pre", "Post")),
         age_c = datawizard::standardize(age, center = TRUE, scale = FALSE)) %>% 
  select(id, educ_yrs, income_log, health_status, marital_status, age, age_c, intervention, visits)
```


# Recap: What makes a count outcome a count?

Counts indicate how many of an outcome are present, or often, how many of an outcome are present *in a specific timeframe* (aka a *rate*).


--
In addition, a distribution of counts is:


--
- Usually *skewed*, meaning that some values are much more common than others - often a lot, or none at all (zero).


--
- Often *overdispersed*, which means the counts vary more than would be expected.


--
Counts/rates require analyses that account for these properties. These methods are similar to those used for dichotomous outcomes (remember that another way of thinking of counts is that they measure a series of dichotomous outcomes).


---
# Normally distributed data

We previously looked at a more normally distributed variable from NHANES: `SleepHrsNight`, the self-reported number of hours of sleep a participant usually gets.


--
```{r, warning=FALSE, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(SleepHrsNight)) +
  geom_histogram(binwidth = 0.5)
```


---
# Count data

`DaysMHBad_count`, the number of days per month an NHANES participant reports experiencing poor mental health.


--
```{r, warning=FALSE, fig.retina=3}
NHANES_data1112 %>% 
  ggplot(aes(DaysMHBad_count)) +
  geom_histogram(binwidth = 1)
```


---
class: middle, inverse

# Analyzing count data

---
# Options for analysing count data


--
<span style = "color:green"> Linear regression: </span>Count outcome variables are sometimes log-transformed and analyzed using OLS regression. 


--
- Many issues in addition to violating model assumptions, including biased estimates and loss of data because it is necessary to take the log of zero (which is undefined).


--
<span style = "color:green"> Poisson regression: </span>Often used for modeling count data. 


--
<span style = "color:green"> Negative binomial regression: </span>Similar to Poisson regression, but relaxes the restrictive assumption of the Poisson model that the mean of the count outcome equals the variance of the outcome.


--
<span style = "color:green"> Zero-inflated regression model: </span>Used to account for excess zeros produced by a different process from "true" zeros. 


---
# Assessing variability

We can get a preliminary sense of whether overdispersion is present by comparing the mean and variance of the `DaysMHBad_count` variable we looked at earlier. Here we'll limit the data to participants 16 or older because the independent variable we'll look at shortly was not measured in participants under 16.


--
```{r}
NHANES_data1112 %>% 
  filter(Age >= 16) %>% 
  summarize(mean = mean(DaysMHBad_count),
            variance = var(DaysMHBad_count))
```


--
The variance of the counts far exceeds the mean, suggesting overdispersion. We'll formally assess overdispersion of model *residuals* once we've fitted our models. 


--
This said, these values are a good indication that we should go with negative binomial regression, but we'll compare it against a Poisson regression model to confirm. 


---
# Fitting a model

Here, we'll look at whether there is a relationship between participants' self-reported sleep quality (`Sleep_Trouble`) and the number of days per month they have poor mental health (`DaysMHBad_count`). To fit a negative binomial model we'll use the `glm.nb` function from the MASS package (included with R).

--
```{r}
library(MASS)
mod_1 <- glm.nb(DaysMHBad_count ~ Sleep_Trouble, 
                data = NHANES_data1112)
```


--
Fitting a comparable Poisson model for a comparison of fit. 

```{r}
mod_1b <- glm(DaysMHBad_count ~ Sleep_Trouble, family = poisson(),
              data = NHANES_data1112)
```

---
# Fitting a model

--
.pull-left[
**Deviance:**

```{r}
mod_1b$deviance # Poisson
mod_1$deviance # Neg Binom
```
]


--
.pull-right[
**AIC:**

```{r}
mod_1b$aic # Poisson
mod_1$aic # Neg Binom
```
]


--
And most critically, we can see that the Poisson gave us much smaller standard errors, which would produce artificially narrow confidence intervals. 

```{r}
summary(mod_1b)$coefficients[2, "Std. Error"] # Poisson
summary(mod_1)$coefficients[2, "Std. Error"] # Neg Binom
```


---
# Fitting a model

We can also look at whether the negative binomial model better accounts for overdispersion. We'll use the DHARMa package again. 


--
```{r}
mod_1_residuals <- simulateResiduals(mod_1)
testDispersion(mod_1_residuals, 
               alternative = "greater", plot = F)
```


--
Compared with the Poisson:

```{r, echo=FALSE, include=FALSE}
mod_1b_residuals <- simulateResiduals(mod_1b)
mod_1b_residuals_p <- testDispersion(mod_1b_residuals, 
                                     alternative = "greater", plot = F)
```


--
.pull-left[
```{r}
mod_1b_residuals_p$statistic
```
]


--
.pull-right[
```{r}
mod_1b_residuals_p$p.value
```
]

---
# Basic interpretaton 

Like logistic regression, we have to exponentiate the model coefficients to interpret the results. With counts, the exponentiated coefficient is a **rate ratio**.


--
```{r}
mod_1 %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) 
```


--
- Compared with participants who do not have trouble sleeping, participants who have trouble sleeping on average experience 2.13 times as many days per month with poor mental health (95% CI [1.78, 2.57]). 


--
- On average, participants who have trouble sleeping experience 113% more poor mental health days each month (95% CI [1.78, 2.57]) than participants who do not have trouble sleeping.


---
# drvisits data

Let's look at data from a study that uses a common design in the social sciences: a single-group pre-post study, where one group is exposed to an intervention and the difference in the outcome measure before and after the intervention is compared.


--
The `drvisits` dataset includes the following variables:

* 

---
# Number of visits 

```{r, warning=FALSE, fig.retina=3}
drvisits %>% 
  ggplot(aes(visits)) +
  geom_histogram(binwidth = 1)
```

---
# Initial model

First we'll fit our initial model, which will be a Poisson model and include the number of visits as the outcome and time point of the intervention (before/after) as the IV. We'll include a random intercept for patients, because the data the observations are clustered within patients. 
 
```{r}
drvisits_m1 <- glmer(visits ~ intervention + (1 | id),
                     family = poisson(), data = drvisits)
```

---
# Examine overdispersion

Next, we'll look at the results of the dispersion test from DHARMa to decide if we should use a negative-binomial model instead of Poisson. 

```{r}
drvisits_m1_residuals <- simulateResiduals(drvisits_m1 , n = 1000) 
testDispersion(drvisits_m1_residuals, alternative = "greater", plot = F)
```


--
This suggests we don't have concerning overdispersion in the model residuals, so we'll stick to the more parsimonious Poisson. 


---
# Adding covariates

In many research areas, there can often be a relatively standard set of sociodemographic characteristics that are  controlled for. We'll imagine this is the case here, and add a set of control variables next. 


--
```{r}
drvisits_m2 <- glmer(visits ~ intervention + educ_yrs + income_log + 
                       age_c + health_status + (1 | id), 
                     family = poisson(),
                     data = drvisits,
                     control = glmerControl(optimizer = "bobyqa",
                               optCtrl = list(maxfun = 2e5)))

anova(drvisits_m1, drvisits_m2, test ="Chisq")
```


---
# Adding covariates

```{r, warning=FALSE, message = FALSE}
performance::compare_performance(drvisits_m1, drvisits_m2, 
                                 metrics = "common") %>% 
  print_md()
```


---
# Assumptions and diagnostics

Inspect residuals:

```{r}
drvisits_m2_residuals <- simulateResiduals(drvisits_m2 , n = 1000) 
plot(drvisits_m2_residuals)
```


                                     
---
# Assumptions and diagnostics

We can also formally test for zero inflation, again using DHARMa:

```{r}
testZeroInflation(drvisits_m2, plot = FALSE)
```


--
Even if the test had been significant, is our outcome one where there could be plausible mechanisms for excess zeros?

---
# Assumptions and diagnostics

Checking multicollinearity visually using the `check_model()` function from the *performance* package.

```{r, warning=FALSE, fig.retina=3}
performance::check_model(drvisits_m2, check = "vif")
```


---
# Examining moderation

```{r}
drvisits_m2_int <- glmer(visits ~ intervention + educ_yrs + 
                           income_log + health_status*intervention + 
                           (1 | id), 
                         family = poisson(),
                         data = drvisits,
                         control = glmerControl(optimizer = "bobyqa",
                                       optCtrl = list(maxfun = 2e5)))
```

---
# Examining moderation

```{r}
drvisits_m2_int %>% 
  broom.mixed::tidy(exponentiate = TRUE, conf.int = TRUE, 
                    effects = "fixed")
  print_md()
```


--
Even though the interaction is nonsignificant, it might be informative to look at the predicted counts broken out by health status since it is a strong main effect. 


---
# Examining moderation

```{r}
# using the ggeffects package
drvisits_m2_int_probs <- predict_response(drvisits_m2_int,
                                    type = "fe",
                                    terms = c("health_status", "intervention"),
                                    margin = "marginalmeans")
```


---
# Examining moderation

```{r}
print(drvisits_m2_int_probs)
```

---
# Examining moderation

```{r, fig.retina=3}
plot(drvisits_m2_int_probs)
```

---
# Ignoring dependency

What if we used a simple fixed-effect model instead of accounting for the dependency between patients' observations before and after the intervention?

```{r, eval = F}
drvisits_m1 <- glmer(visits ~ intervention + (1 | id),
                     family = poisson(), data = drvisits)
```

*Versus*

```{r}
drvisits_fixed <- glm(visits ~ intervention,
                      family = poisson(), data = drvisits)
```

---
# Ignoring dependency

```{r}
drvisits_m1 %>% 
  broom.mixed::tidy(exponentiate = TRUE, conf.int = TRUE, effects = "fixed") %>% 
  print_md()
```

*Versus*

```{r}
drvisits_fixed %>% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) %>% 
  print_md()
```

