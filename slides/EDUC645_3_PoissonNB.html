<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Analyzing Count Outcomes</title>
    <meta charset="utf-8" />
    <script src="EDUC645_3_PoissonNB_files/header-attrs-2.21/header-attrs.js"></script>
    <link href="EDUC645_3_PoissonNB_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="EDUC645_3_PoissonNB_files/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="EDUC645_3_PoissonNB_files/remark-css-0.0.1/ki-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_custom.css" type="text/css" />
    <link rel="stylesheet" href="xaringanthemer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Analyzing Count Outcomes
]
.subtitle[
## EDUC 645 (Unit 3)
]

---





# Recap: What makes a count outcome a count?

Counts indicate how many of an outcome are present, or often, how many of an outcome are present *in a specific timeframe* (aka a *rate*).


--
In addition, a distribution of counts is:


--
- Usually *skewed*, meaning that some values are much more common than others - often a lot, or none at all (zero).


--
- Often *overdispersed*, which means the counts vary more than would be expected.


--
Counts/rates require analyses that account for these properties. These methods are similar to those used for dichotomous outcomes (remember that another way of thinking of counts is that they measure a series of dichotomous outcomes).


---
# Normally distributed data

We previously looked at a more normally distributed variable from NHANES: `SleepHrsNight`, the self-reported number of hours of sleep a participant usually gets.


--

```r
NHANES_data1112 %&gt;% 
  ggplot(aes(SleepHrsNight)) +
  geom_histogram(binwidth = 0.5)
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-1-1.png" width="576" style="display: block; margin: auto;" /&gt;


---
# Count data

`DaysMHBad_count`, the number of days per month an NHANES participant reports experiencing poor mental health.


--

```r
NHANES_data1112 %&gt;% 
  ggplot(aes(DaysMHBad_count)) +
  geom_histogram(binwidth = 1)
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /&gt;


---
class: middle, inverse

# Analyzing count data

---
# Options for analysing count data


--
&lt;span style = "color:green"&gt; Linear regression: &lt;/span&gt;Count outcome variables are sometimes log-transformed and analyzed using OLS regression. 


--
- Many issues in addition to violating model assumptions, including biased estimates and loss of data because it is necessary to take the log of zero (which is undefined).


--
&lt;span style = "color:green"&gt; Poisson regression: &lt;/span&gt;Often used for modeling count data. 


--
&lt;span style = "color:green"&gt; Negative binomial regression: &lt;/span&gt;Similar to Poisson regression, but relaxes the restrictive assumption of the Poisson model that the mean of the count outcome equals the variance of the outcome.


--
&lt;span style = "color:green"&gt; Zero-inflated regression model: &lt;/span&gt;Used to account for excess zeros produced by a different process from "true" zeros. 


---
# Assessing variability

We can get a preliminary sense of whether overdispersion is present by comparing the mean and variance of the `DaysMHBad_count` variable we looked at earlier. Here we'll limit the data to participants 16 or older because the independent variable we'll look at shortly was not measured in participants under 16.


--

```r
NHANES_data1112 %&gt;% 
  filter(Age &gt;= 16) %&gt;% 
  summarize(mean = mean(DaysMHBad_count),
            variance = var(DaysMHBad_count)) %&gt;% 
  print_md()
```



|mean | variance|
|:----|--------:|
|3.88 |    59.50|


--
The variance of the counts far exceeds the mean, suggesting overdispersion. We'll formally assess overdispersion of model *residuals* once we've fitted our models. 


--
This said, these values are a good indication that we should go with negative binomial regression, but we'll compare it against a Poisson regression model to confirm. 


---
# Fitting a model

Here, we'll look at whether there is a relationship between participants' self-reported sleep quality (`Sleep_Trouble`) and the number of days per month they have poor mental health (`DaysMHBad_count`). To fit a negative binomial model we'll use the `glm.nb` function from the MASS package.

--

```r
mod_1 &lt;- MASS::glm.nb(DaysMHBad_count ~ Sleep_Trouble,
                       data = NHANES_data1112)
```


--
Fitting a comparable Poisson model for a comparison of fit. 


```r
mod_1b &lt;- glm(DaysMHBad_count ~ Sleep_Trouble, family = poisson(),
              data = NHANES_data1112)
```

---
# Fitting a model

--
.pull-left[
**Deviance:**


```r
mod_1b$deviance # Poisson
```

```
#&gt; [1] 33116.23
```

```r
mod_1$deviance # Neg Binom
```

```
#&gt; [1] 2652.866
```
]


--
.pull-right[
**AIC:**


```r
mod_1b$aic # Poisson
```

```
#&gt; [1] 38025.25
```

```r
mod_1$aic # Neg Binom
```

```
#&gt; [1] 13187.78
```
]


--
And most critically, we can see that the Poisson gave us much smaller standard errors, which would produce artificially narrow confidence intervals. 


```r
summary(mod_1b)$coefficients[2, "Std. Error"] # Poisson
```

```
#&gt; [1] 0.01774997
```

```r
summary(mod_1)$coefficients[2, "Std. Error"] # Neg Binom
```

```
#&gt; [1] 0.09383608
```


---
# Fitting a model

We can also look at whether the negative binomial model better accounts for overdispersion. We'll use the DHARMa package again. 


--

```r
mod_1_residuals &lt;- simulateResiduals(mod_1)
testDispersion(mod_1_residuals, 
               alternative = "greater", plot = F)
```

```
#&gt; 
#&gt; 	DHARMa nonparametric dispersion test via sd of residuals fitted vs.
#&gt; 	simulated
#&gt; 
#&gt; data:  simulationOutput
#&gt; dispersion = 0.58444, p-value = 1
#&gt; alternative hypothesis: greater
```


--
Compared with the Poisson:




--
.pull-left[

```r
mod_1b_residuals_p$statistic
```

```
#&gt; dispersion 
#&gt;   14.74392
```
]


--
.pull-right[

```r
mod_1b_residuals_p$p.value
```

```
#&gt; [1] 0
```
]



---
# Basic interpretation 

Like logistic regression, we have to exponentiate the model coefficients to interpret the results. With counts, the exponentiated coefficient is a **rate ratio**.


--

```r
mod_1 %&gt;% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) 
```


|term             | estimate| std.error| statistic|  p.value | conf.low| conf.high|
|:----------------|--------:|---------:|---------:|:---------|--------:|---------:|
|(Intercept)      |     3.00|      0.05|     22.31|3.28e-110 |     2.72|      3.30|
|Sleep_TroubleYes |     2.10|      0.09|      7.91| 2.64e-15 |     1.75|      2.53|

--
- Compared with participants who do not have trouble sleeping, participants who have trouble sleeping on average experience 2.13 times as many days per month with poor mental health (95% CI [1.78, 2.57]). 


--
- On average, participants who have trouble sleeping experience 113% more poor mental health days each month (95% CI [1.78, 2.57]) than participants who do not have trouble sleeping.


---
# drvisits data

Let's look at data from a study that uses a common design in the social sciences: a *single-group pre-post* study, where one group is exposed to an intervention and the difference in the outcome measure before and after the intervention is compared.


--
The `drvisits` dataset is from a pre-post study of a policy intervention intended to reduce the number of doctor visits patients required. The dataset is limited to female participants and includes the following variables:

* `id`: Patient ID
* `age`: Patient age in years
* `age_c`: Patient age in years (centered)
* `educ_yrs`: Number of years of education
* `marital_status`: Married or unmarried
* `income_log`: Annual income (logarithm)
* `health_status`: Self-reported overall health (Good or Poor)
* `intervention`: Timing of observation relative to intervention (Pre or Post)
* `visits`: Number of doctor visits during the 3 months preceding assessment

---
# Number of visits 


```r
drvisits %&gt;% 
  ggplot(aes(visits)) +
  geom_histogram(binwidth = 1)
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-16-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Initial model

First we'll fit our initial model, which will be a Poisson model and include the number of visits as the outcome and time point of the intervention (before/after) as the IV. We'll include a random intercept for patients, because the data the observations are clustered within patients. 
 

```r
drvisits_m1 &lt;- glmer(visits ~ intervention + (1 | id),
                     family = poisson(), data = drvisits)
```

---
# Examine overdispersion

Next, we'll look at the results of the dispersion test from DHARMa to decide if we should use a negative-binomial model instead of Poisson. 


```r
drvisits_m1_residuals &lt;- simulateResiduals(drvisits_m1 , n = 1000) 
testDispersion(drvisits_m1_residuals, alternative = "greater", plot = F)
```

```
#&gt; 
#&gt; 	DHARMa nonparametric dispersion test via sd of residuals fitted vs.
#&gt; 	simulated
#&gt; 
#&gt; data:  simulationOutput
#&gt; dispersion = 1.1013, p-value = 0.232
#&gt; alternative hypothesis: greater
```


--
This suggests we don't have concerning overdispersion in the model residuals, so we'll stick to the more parsimonious Poisson. 


---
# Adding covariates

In many research areas, there can often be a relatively standard set of sociodemographic characteristics that are controlled for. We'll imagine this is the case here, and add a set of control variables next. 


--

```r
drvisits_m2 &lt;- glmer(visits ~ intervention + educ_yrs + income_log + 
                       age_c + health_status + (1 | id), 
                     family = poisson(),
                     data = drvisits,
                     control = glmerControl(optimizer = "bobyqa",
                               optCtrl = list(maxfun = 2e5)))
```


--

```r
performance::test_likelihoodratio(drvisits_m1, drvisits_m2)
```


|Name        |   Model | df| df_diff|   Chi2|        p|
|:-----------|:--------|--:|-------:|------:|--------:|
|drvisits_m1 |glmerMod |  3|        |       |         |
|drvisits_m2 |glmerMod |  7|       4| 236.67| 4.83e-50|

---
# Adding covariates


```r
performance::compare_performance(drvisits_m1, drvisits_m2, 
                                 metrics = c("common") 
```



Table: Comparison of Model Performance Indices

|Name        |    Model |  AIC (weights) |  BIC (weights) | R2 (cond.) | R2 (marg.) |  ICC | RMSE |
|:-----------|:--------:|:--------------:|:--------------:|:----------:|:----------:|:----:|:----:|
|drvisits_m1 | glmerMod | 9536.2 (&lt;.001) | 9553.4 (&lt;.001) |       0.67 |   1.85e-04 | 0.67 | 1.60 |
|drvisits_m2 | glmerMod | 9307.6 (&gt;.999) | 9347.5 (&gt;.999) |       0.65 |       0.07 | 0.62 | 1.62 |

---
# Assumptions and diagnostics

Inspect residuals:


```r
drvisits_m2_residuals &lt;- simulateResiduals(drvisits_m2 , n = 1000) 
plot(drvisits_m2_residuals)
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-24-1.png" style="display: block; margin: auto;" /&gt;


                                     
---
# Assumptions and diagnostics

We can also formally test for zero inflation, again using DHARMa:


```r
testZeroInflation(drvisits_m2, plot = FALSE)
```

```
#&gt; 
#&gt; 	DHARMa zero-inflation test via comparison to expected zeros with
#&gt; 	simulation under H0 = fitted model
#&gt; 
#&gt; data:  simulationOutput
#&gt; ratioObsSim = 1.1448, p-value &lt; 2.2e-16
#&gt; alternative hypothesis: two.sided
```


--
It looks like we have significant zero inflation, so we could consider a zero-inflated Poisson (see last slide).

---
# Assumptions and diagnostics

Checking multicollinearity visually using the `check_model()` function from the *performance* package.


```r
performance::check_model(drvisits_m2, check = "vif")
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-26-1.png" width="576" style="display: block; margin: auto;" /&gt;


---
# Examining moderation


```r
drvisits_m2_int &lt;- glmer(visits ~ intervention + educ_yrs + 
                           income_log + health_status*intervention + 
                           (1 | id), 
                         family = poisson(),
                         data = drvisits,
                         control = glmerControl(optimizer = "bobyqa",
                                       optCtrl = list(maxfun = 2e5)))
```

---
# Examining moderation


```r
drvisits_m2_int %&gt;% 
  broom.mixed::tidy(exponentiate = TRUE, conf.int = TRUE, 
                    effects = "fixed")
```


|term                               | estimate| std.error| statistic|  p.value| conf.low| conf.high|
|:----------------------------------|--------:|---------:|---------:|--------:|--------:|---------:|
|(Intercept)                        |     0.59|      0.30|     -1.04|     0.30|     0.22|      1.59|
|interventionPost                   |     0.94|      0.03|     -1.67|     0.09|     0.88|      1.01|
|educ_yrs                           |     1.00|      0.01|      0.14|     0.89|     0.98|      1.03|
|income_log                         |     1.12|      0.08|      1.75|     0.08|     0.99|      1.28|
|health_statusPoor                  |     2.40|      0.18|     11.84| 2.37e-32|     2.08|      2.78|
|interventionPost:health_statusPoor |     1.13|      0.10|      1.36|     0.17|     0.95|      1.35|

--
Even though the interaction is nonsignificant, it might be informative to look at the predicted counts broken out by health status since it is a strong main effect. 


---
# Examining moderation


```r
# using the ggeffects package
drvisits_m2_int_probs &lt;- predict_response(drvisits_m2_int,
                                    type = "fe",
                                    terms = c("health_status", 
                                              "intervention"),
                                    margin = "marginalmeans")
```


---
# Examining moderation


```r
print(drvisits_m2_int_probs)
```

```
#&gt; # Predicted counts of number of doctor visits in the last 3 months before interview
#&gt; 
#&gt; intervention: Pre
#&gt; 
#&gt; health_status | Predicted |     95% CI
#&gt; --------------------------------------
#&gt; Good          |      1.49 | 1.38, 1.60
#&gt; Poor          |      3.58 | 3.12, 4.12
#&gt; 
#&gt; intervention: Post
#&gt; 
#&gt; health_status | Predicted |     95% CI
#&gt; --------------------------------------
#&gt; Good          |      1.40 | 1.30, 1.51
#&gt; Poor          |      3.81 | 3.30, 4.40
#&gt; 
#&gt; Adjusted for:
#&gt; *   educ_yrs = 11.51
#&gt; * income_log =  7.71
```

---
# Examining moderation


```r
plot(drvisits_m2_int_probs)
```

&lt;img src="EDUC645_3_PoissonNB_files/figure-html/unnamed-chunk-32-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Ignoring dependency

What if we used a simple fixed-effect model instead of accounting for the dependency between patients' observations before and after the intervention?


--
In other words: 


```r
drvisits_m1 &lt;- glmer(visits ~ intervention + (1 | id),
                     family = poisson(), data = drvisits)
```

**.blue[*Versus*]**


```r
drvisits_fixed &lt;- glm(visits ~ intervention,
                      family = poisson(), data = drvisits)
```

---
# Ignoring dependency


```r
drvisits_m1 %&gt;% 
  broom.mixed::tidy(exponentiate = TRUE, conf.int = TRUE, effects = "fixed")
```


|term             | estimate| std.error| statistic| p.value | conf.low| conf.high|
|:----------------|--------:|---------:|---------:|:--------|--------:|---------:|
|(Intercept)      |     1.61|      0.06|     12.57|3.17e-36 |     1.49|      1.73|
|interventionPost |     0.97|      0.03|     -1.04|    0.30 |     0.91|      1.03|

**.blue[*Versus*]**


```r
drvisits_fixed %&gt;% 
  broom::tidy(exponentiate = TRUE, conf.int = TRUE) 
```


|term             | estimate| std.error| statistic|  p.value| conf.low| conf.high|
|:----------------|--------:|---------:|---------:|--------:|--------:|---------:|
|(Intercept)      |     2.83|      0.02|     58.07|     0.00|     2.73|      2.93|
|interventionPost |     0.83|      0.03|     -7.00| 2.65e-12|     0.79|      0.88|

---
# Zero-inflated Poisson

Because the zero inflation test was significant, we could fit a zero-inflated (ZI) Poisson model then compare fit. 


--
We'll use the `mixed_model` function from the `GLMMadaptive` package for this model, and we'll first refit our model with demographic covariates in this package for better comparability. 


--

```r
drvisits_m2a &lt;- mixed_model(visits ~ intervention + educ_yrs + 
                             income_log + age_c + health_status,
                            random = ~ 1 | id, 
                            family = poisson(),
                            data = drvisits)
```

Note: If we had earlier found that we had important overdispersion and that a negative binomial model was better fitting, we'd use the same syntax as above but with *family* as `negative.binomial()` for the ZI models.

---
# Zero-inflated Poisson

Next, we fit a ZI model where every observation has an equal likelihood of being a "true zero" or an "excess zero" (ie, a zero caused by a different mechanism). 


```r
drvisits_m2b &lt;- mixed_model(visits ~ intervention + educ_yrs + 
                             income_log + age_c + health_status,
                            random = ~ 1 | id, 
                            family = zi.poisson(),
                            zi_fixed = ~ 1,
                            data = drvisits)
```


---
# Zero-inflated Poisson

Finally, let's fit a model that specifies some mechanism of excess zeros based on existing theory/evidence. A theoretical justification might be something like this: 


--
- The intervention is intended to reduce the number of doctor's visits patients need; this implies the intervention is intended to improve health, which in turn means patients need less health care. In this scenario, then, the "true" zero mechanism is being well (so health care is unneeded).


--
- What might be an alternative zero mechanism? Perhaps patients with low income are unwell and *need* to see a doctor, but can't afford it. 


--

```r
drvisits_m2c &lt;- mixed_model(visits ~ intervention + educ_yrs + 
                             income_log + age_c + health_status,
                            random = ~ 1 | id, 
                            family = zi.poisson(),
                            zi_fixed = ~ income_log,
                            data = drvisits)
```


---
# Zero-inflated Poisson

We can then compare model fit:


|Name         |    AIC |   Chi2|        p|
|:------------|:-------|------:|--------:|
|drvisits_m2a |9307.63 |       |         |
|drvisits_m2b |9151.03 | 158.60| 2.29e-36|
|drvisits_m2c |9151.63 |   1.40|     0.24|


--
This suggests we should go with a ZI model over a standard Poisson, but also that we should stick with the simpler ZI model without a specific zero mechanism specified. 


--
The DHARMa author gives good advice for this scenario, which also applies to model selection more generally:


--
- **.blue[Zero-inflation tests are often not a reliable guide to decide whether to add a ZI term or not. In general, model structures should be decided on a priori, and if that is not possible, via model selection techniques (AIC, BIC, etc). A zero-inflation test should only be run after that decision, and to validate the decision that was taken.]**
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
