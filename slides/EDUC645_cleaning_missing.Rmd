---
title: "Cleaning Data & Handling Missingness"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, rio, xaringan, dplyr, knitr, xaringanthemer, kableExtra, lfe, arsenal, foreign, reshape2, ggpubr, broom, stargazer, fixest, gtsummary, huxtable, mice, miceadds, VIM, magrittr, DiagrammeR)

theme_set(theme_classic())

i_am("slides/EDUC645_cleaning_missing.rmd")

red_pink <- "#e64173"
turquoise = "#20B2AA"
orange = "#FFA500"
red = "#fb6107"
blue = "#3b3b9a"
green = "#8bb174"
grey_light = "#B3B3B3"
grey_mid = "#7F7F7F"
grey_dark = "grey20"
purple = "#6A5ACD"
slate = "#314f4f"
extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "red_pink"),
  ".gray" = list(color= "#B3B3B3"),
  ".purple" = list(color = "purple"),
  ".small" = list("font-size" = "90%"),
  ".large" = list("font-size" = "120%"),
  ".tiny" = list("font-size" = "70%"),
  ".tiny2" = list("font-size" = "50%"))
write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)

```

# Roadmap

---
# Goals for the unit
* Identify Forms of Missingness
* Learn about Techniques for handling Missing Data Handling
  - Traditional methods
  - Modern approaches (mainly multiple imputation)
* Apply best practices to treating missing data
 - Conducting Imputation
 - Reporting on handling missing data

---

class: middle, inverse

# Missing Data Motivating example

---
# Dataset: galo.csv** 

GALO data is originally described by Peschar (1975) and analyzed by Dronkers and Schijf (1994)
1959 cohort: 
- 1270 sixth grade students
- 37 elementary schools in Groningen, Netherlands

Key variables.
DV: score, student's achievement score on GALO test

---
# Missing Data Diagnostics

Inspect missing and imputed data before and after imputation:

*Before imputation*
  1. Calculate missingness in the data (total n, percent/ proportion)
  2. Examine missingness using plots (e.g., margin plots)
    * Show the observed and missing values across pairs of variables

--

*After imputation*
  3. Inspect strip plots
  * Identify inconsistent patterns of missingness among imputed data by displaying original data alongside imputed data
  4. View kernel density plots
  * Review distributions of observed and imputed data

---
# Missing on each variable

```{r, echo = F}
galo <- rio::import(here::here("data", "galo.csv"))

#I understand we don't want introduce students to the functional programming world
# I'm in searching for an easier way to report missing values that can be more accessible to students
missing <- function(data){
  na_percentage <- function(x){
    y = sum(is.na(x))/length(x)
    paste(round(100*y, 2), "%", sep="")
  }
  apply(data, 2, na_percentage)
}

missing(galo)
```

---
# Missing data pattern

There is no set threshold for the percentage of missingness but researchers commonly agree that if the dataset is large and the missingness is below 5%, the incomplete observations can be ignored and the analysis can be conducted based on the rest of the data. 

For secondary data like we are using throughout this course, we don't know whether the missing values are missing completely at random (MCAR), missing at random (MAR), or not missing at random (NMAR). 

```{r, echo = F}
galo %>% select(advice, medu, fedu, focc) %>% 
  md.pattern(rotate.names = TRUE)
```

---
# How the values are missing in pairs:

```{r}
galo %>% select(advice, medu, fedu, focc) %>%
  md.pairs()
```

---
# Further inspect missingness

Missingness/margin plot
```{r, echo = F}
marginplot(galo[, c("advice", "medu")], col = mdc(1:2), cex = 1.2, cex.lab = 1.2, cex.numbers = 1.3, pch = 19)
# Red dots are missing values on the variable on the alternate axis

```

---
# Dealing with missing data

RQ: what is the relationship between teacher's recommendation on secondary school choice and student's GALO score?
Note that the regression functions (e.g., lm()) in R typically only report results based on complete observations.

???
To illustrate how different ways to deal with missing data might affect our analysis, let's start with a research question and an OLS regression to answer the question.

---

# Problems with missing data (missingness)

Power: Missingness often reduces the number of useful observations, and therefore reduces power

--

Bias: Missingness induces bias as incomplete data…
- May not be representative of the intended population
- May not be balanced across observed and unobserved confounders
- Are likely biased 

--

Recoverability: Traditional approaches to missingness have limited our ability to recover statistical power and true variability

---

# Missing data mechanisms

Missing Completely at Random (MCAR): Missingness is unrelated to variables's values

--

Missing at Random (MAR): Missingness is related to values in the data, but can be explained by other values in the data.

--

Missing Not at Random (MNAR): Missing data are caused by an individual’s value of the variable

*All forms can be present within the same data*

---
# Traditional methods for treating missingness

### Listwise or case deletion
- Delete observations with missing data
- Reduces power and may introduce bias
- Useful for multivariate analysis
  - Missingness across many variables, even if missing data are few, can lead to large amounts of deleted data

--

### Dummy variable adjustment
- Data are missing on X because that variable doesn’t apply or has no meaning for some subset of the sample
- Missing data on baseline covariates in randomized controlled trials.

---

# Single Imputation Methods
- Mean substitution
  - Replace missing values on a variable with average of that variable
  - May lower variance, reduce power, and distort covariances and intercorrelations among variables
- Averaging over variables with missing data 
  - May inflate average variances
- Hot-decking
  - Replaces missing values with random observed values
- Last observation carried forward (LOCF)
  - Reduces variability

---
# Dummy Value Adjusted Method

When to use: You want to run a linear regression model using two predictor variables, $B_1$ and $B_2$, and $B_1$ has a high amount of missingness:

Step 1. Create a dummy (indicator) variable $B_3$ that equals 1 if $B_1$ is not missing and 0 if $B_1$ is missing
Step 2. For the missing values of $B_1$, you substitute a constant value (e.g., 2)
Step 3. Include all three variables in the model, $B_1$, $B_2$, and $B_3$

**Optimal use (even over FIML and MI)**
- Data are missing because that variable doesn’t apply or has no meaning for some subset of the sample
- Missing data on baseline covariates in randomized controlled trials

---
# Imputation

- Uses the underlying structure of the data to replace missing values with new values that could have been  observed, had they not been missing

- Aims to re-establish the relationship between data and the sample population, without creating overly precise data

---
# Two modern imputation approaches
### Different theoretical orientations to missingness

* Full-Information Maximum-Likelihood (FIML) imputation
  - Estimate model parameters close to their values as estimated from complete data 
  - Model-specific and limited to models that are estimated using maximum-likelihood estimation

--

* Multiple Imputation (MI) 
  - Aims to estimate complete data 
  - Not model-specific and therefore more versatile application to data analysis

---
# Assumptions of MI and FIML

{Note the strong assumptions that MI and FIML make on observables vs. unobservables}

---
# MI by Chained Equations (MICE)

* Chained equations: 
  - Focuses the algorithm on one variable at a time
  - Once variable is imputed, the algorithm moves to the next variable, skipping over complete variables, until all missing data is imputed
  - Process is repeated to create each imputed data set
* Numerous algorithms exist
  - Selection on variable being imputed (e.g., continuous, ordered, nominal/categorical)

---
# Multiple Imputation Process

Repeating the process to average estimates of the missing data:
* Produces a more accurate values
* More precisely defines uncertainty in the data
* Accounts for measurement error in original data

```{r, echo=F}
m <- DiagrammeR::mermaid("
                         graph LR
                        A((d))-->B((m<sub>1</sub>))
                       A((d))-->C((m<sub>2</sub>))
                      A((d))-->D((m<sub>3</sub>))
                         B((m<sub>1</sub>))-->E((a<sub>1</sub>)) 
                        C((m<sub>2</sub>))-->F((a<sub>2</sub>)) 
                         D((m<sub>3</sub>))-->G((a<sub>3</sub>))
                         E((a<sub>1</sub>))-->H((r))
                         F((a<sub>2</sub>))-->H((r)) 
                         G((a<sub>3</sub>))-->H((r))
                       ")

widgetframe::frameableWidget(m)
```

---
# Casewise deletion (CD)

```{r, echo = F}
m1 <- lm(advice ~ score, galo)
m2 <- lm(advice ~ score + female + medu + fedu + focc, galo)

list("CD" = m1,
     "CD (covariates)" = m2) %>% 
  huxreg()
```

---
# Mean imputation

```{r, echo = F}
galo_meani <- galo %>% 
  drop_na(advice) %>% 
  group_by(schid) %>% 
  mutate(medumean = mean(medu, na.rm = TRUE),
         fedumean = mean(fedu, na.rm = TRUE),
         foccmean = mean(focc, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(medu = ifelse(is.na(medu), medumean, medu),
         fedu = ifelse(is.na(fedu), fedumean, fedu),
         focc = ifelse(is.na(focc), foccmean, focc))

# Rerun the analysis:

m3 <- lm(advice ~ score, galo_meani)

m4 <- lm(advice ~ score + female + medu + fedu + focc, galo_meani)

list("CD" = m1,
     "CD (covariates)" = m2,
     "MeanI" = m3,
     "MeanI (covariates)" = m4) %>% 
  huxreg()
```

 - Due to the fact that students are nested in schools, the best practice is to compute mean values WITHIN EACH SCHOOL rather than across all observations.
 
 - be extra careful when dealing with missingness on predictor and outcome variables. 

Our outcome variable, advice, has a very small missing value (0.45%), so we should drop all incomplete observations rather than replacing them with variable means

???
Simply imputing all missing values using mean values, but there's good practice and bad practice. 

---
# Multiple imputation (MI)

```{r, echo = F}
galo_multi <- mice(galo, seed = 12345) 
class(galo_multi) # should be "mids"
summary(galo_multi)
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(2, 1))
```

---
# MI specifications

Specifying imputation algorithms:
- number of imputed data sets (`m`: 5 is default, >= 50 is better)
- number of iterations (`maxit`: 5 is common, 10 is better). 

```{r}
galo_multi <- mice(galo, m = 50, maxit = 10, seed = 12345)
```


```{r, echo = F, echo = F}
summary(galo_multi)
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(2, 1))
```

---
# Look at the first dataset

```{r, echo = F}
galo_multi1 <- complete(galo_multi)
head(galo_multi1)
```


---
# Examine strip plot 

```{r, echo = F}
stripplot(galo_multi, pch = 5, cex = 1.2)
# Red dots are imputed values, blue are observed values
```

---
# Examine kernel density plot

```{r, echo = F}

densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(3, 1))
# Red lines are imputed value distributions, blue are observed value distributions

```


```{r, echo = F}
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(3, 1))

```

---
# Building a model using a 'tidyverse' approach
 
```{r}
# Alternate scripting using "tidy" approach
est1 <- galo %>%
  mice(m = 50, seed = 12345) %>%
  with(lm(advice ~ score)) %>%
  pool()
summary(est1)
```

---
# Building a model using a 'tidyverse' approach
 
```{r}
# Alternate scripting using "tidy" approach
est2 <- galo %>%
  mice(m = 50, seed = 12345) %>%
  with(lm(advice ~ score + female + medu + fedu + focc)) %>%
  pool()
summary(est2)
```

---

```{r, echo = F}
list("CD" = m1,
     "CD (covariates)" = m2,
     "MeanI" = m3,
     "MeanI (covariates)" = m4,
     "MultI" = est1,
     "MultI (covariates)" = est2) %>% 
  huxreg()
```


---
# Best Reporting Practices

* Report the percent/proportion of data missing on variables in analyses
* Indicate the primary reason you believe data are missing and briefly justify your assumption
* Report the imputation method. If using MI, report the number of imputed data sets and mention algorithm used
* Cite the software/package used to impute data
* If there is substantial missingness, conduct and mention a sensitivity analysis of primary analyses using the imputed data and complete case analyses

---
# Key Takeaways


---
class: middle, inverse
# Synthesis and wrap-up

---
# Unit Goals

---
# To Dos

### Reading
-

### Assignments
- Assignment #X Due XX at 11:59PM
- Quiz #X Due on X XX at 5PM