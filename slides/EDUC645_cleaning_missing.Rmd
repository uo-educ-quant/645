---
title: "Cleaning Data & Handling Missingness"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, rio, xaringan, dplyr, knitr, xaringanthemer, kableExtra, lfe, arsenal, foreign, reshape2, ggpubr, broom, stargazer, fixest, gtsummary, huxtable, mice, miceadds, VIM, magrittr, DiagrammeR)

theme_set(theme_classic())

i_am("slides/EDUC645_cleaning_missing.rmd")

red_pink <- "#e64173"
turquoise = "#20B2AA"
orange = "#FFA500"
red = "#fb6107"
blue = "#3b3b9a"
green = "#8bb174"
grey_light = "#B3B3B3"
grey_mid = "#7F7F7F"
grey_dark = "grey20"
purple = "#6A5ACD"
slate = "#314f4f"
extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".red-pink" = list(color= "red_pink"),
  ".gray" = list(color= "#B3B3B3"),
  ".purple" = list(color = "purple"),
  ".small" = list("font-size" = "90%"),
  ".large" = list("font-size" = "120%"),
  ".tiny" = list("font-size" = "70%"),
  ".tiny2" = list("font-size" = "50%"))
write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)

```

# Roadmap

---
# Goals for the unit
* Identify Forms of Missingness
* Learn about Techniques for handling Missing Data Handling
  - Traditional methods
  - Modern approaches (mainly multiple imputation)
* Apply best practices to treating missing data
 - Conducting Imputation
 - Reporting on handling missing data

---

class: middle, inverse

# Missing Data Motivating example

---
# Dataset: galo.csv** 

GALO data is originally described by Peschar (1975) and analyzed by Dronkers and Schijf (1994)
1959 cohort: 
- 1270 sixth grade students
- 37 elementary schools in Groningen, Netherlands

Key variables.
DV: score, student's achievement score on GALO test

---
# Missing Data Diagnostics
## Inspect missing and imputed data before and after imputation

*Before imputation*
  1. Calculate overall amount of missingness in the data (n, percent/ proportion)
  2. Examine missingness using plots (e.g., margin plots)
    * Show the observed and missing values across pairs of variables
    * Note: Reasons for missing data can differ between variables

--

*After imputation*
  3. Inspect strip plots
  * Identify inconsistent patterns of missingness among imputed data by displaying observed (original data) and imputed data together
  4. View kernel density plots
  * Review distributions of observed and imputed data rather than individual values as points (similar purpose to strip plots)

---
# Missing on each variable

```{r}
galo <- rio::import(here::here("data", "galo.csv"))

#I understand we don't want introduce students to the functional programming world
# I'm in searching for an easier way to report missing values that can be more accessible to students
missing <- function(data){
  na_percentage <- function(x){
    y = sum(is.na(x))/length(x)
    paste(round(100*y, 2), "%", sep="")
  }
  apply(data, 2, na_percentage)
}

missing(galo)
```

---
# Missing data pattern

There is no set threshold for the percentage of missingness but researchers commonly agree that if the dataset is large and the missingness is below 5%, the incomplete observations can be ignored and the analysis can be conducted based on the rest of the data. 

For secondary data like we are using throughout this course, we don't know whether the missing values are missing completely at random (MCAR), missing at random (MAR), or not missing at random (NMAR). 

```{r}
galo %>% select(advice, medu, fedu, focc) %>% 
  md.pattern(rotate.names = TRUE)
```

---
# How the values are missing in pairs:

```{r}
galo %>% select(advice, medu, fedu, focc) %>%
  md.pairs()
```

---
# Create a missingness or margin plot to further inspect missingness

```{r}
marginplot(galo[, c("advice", "medu")], col = mdc(1:2), cex = 1.2, cex.lab = 1.2, cex.numbers = 1.3, pch = 19)
# Red dots are missing values on the variable on the alternate axis

```

---
# Dealing with missing data

RQ: what is the relationship between teacher's recommendation on secondary school choice and student's GALO score?
Note that the regression functions (e.g., lm()) in R typically only report results based on complete observations.

???
To illustrate how different ways to deal with missing data might affect our analysis, let's start with a research question and an OLS regression to answer the question.

---

# Problems with missing data (missingness)

Power: Missingness often reduces the number of useful observations, and therefore reduces power

--

Bias: Missingness induces bias as incomplete data…
- May not be representative of the intended population
- May not be balanced across observed and/or unobserved confounders as originally planned
- Are likely to have biased characteristics

--

Recoverability: Traditional approaches to missingness have limited our ability to recover statistical power and true variability

---

# Missing data mechanisms

Missing Completely at Random (MCAR): Missingness is unrelated to the values of variables in the data
Example: Missingness on Depression Scores (DV) because a school event attended by younger students prevented data collection from those students

--

Missing at Random (MAR): Missingness is related to values in the data, but can be explained by other (observed) values in the data. Generally (assumed to be) the most common form of missingness
Example: Missingness on Depression Scores (DV) due to perceived stigma of mental health concerns, but Depression Score may be predicted by age, victimization level, etc.

--

Missing Not at Random (MNAR): Missing data are caused by an individual’s value of the variable
Example: Missingness on BDI because of depression level (IV)

--

*All forms of missingness can co-occur among same data*

---
# Traditional methods for treating missingness
## and their shortcomings

### Listwise or case deletion
- Delete observations with missingness
- Under MCAR, reduces power; under MAR and MNAR, reduces power and induced bias
- Especially useful for multivariate analysis
  - limited but widely dispersed missingness can lead to extensive case deletion

--

### Dummy variable adjustment
- Data are missing on X because that variable doesn’t apply or has no meaning for some subset of the sample
- Missing data on baseline covariates in randomized controlled trials.

--

### Single Imputation Methods
- Mean substitution (never use): Replace missing values on a variable with average of that variable (artificially lowers variance, reduces power, and distorts covariances and intercorrelations among variables)
  The following can perform well, but modern methods perform better
- Averaging over missing items (artificially inflates average value variances)
- Hot-decking: Replaces missing values with random observed values
- Last-observation-carried-forward 9 (reduces variability)

---
# Dummy Value Adjusted Method
Context: You want to do a linear regression of Y on two variables, X and Z, with a substantial number of cases that have missing values on X. 
1. Create a dummy (indicator) variable D that has a value of 1 if X is observed and 0 if X is missing
2. For the missing values of X, you substitute some constant value c (e.g., 2)
3. All three variables, X, Z, and D, then go into the regression as predictors

https://statisticalhorizons.com/is-dummy-variable-adjustment-ever-good-for-missing-data/ 

Optimal use (even over FIML and MI)
- Data are missing on X because that variable doesn’t apply or has no meaning for some subset of the sample
- Missing data on baseline covariates in randomized controlled trials

---
# Imputation

- Uses the underlying structure of the data to replace missing values with new values that could have been reasonably obtained, had they not been missing

- Aims to re-establish the relationship of the data with the population it was sampled from, without creating artificially precise data for the analysis

---
# Two modern imputation approaches
## Different theoretical orientations to missingness

* Full-Information Maximum-Likelihood (FIML) imputation
  - Estimate model parameters close to their values as estimated from complete data 
  - Model-specific and limited to models that are estimated using maximum-likelihood estimation (e.g., SEM, mixed/multilevel models, mixture models)

--

* Multiple Imputation (MI) 
  - Aims to estimate complete data 
  - Not model-specific and therefore more versatile application to data analysis

---
# Assumptions of MI and FIML

{Note the strong assumptions that MI and FIML make on observables vs. unobservables}

---
# MI by Chained Equations (MICE)

* Chained equations: 
  - Focuses the algorithm on one variable at a time
  - Once variable is imputed, the algorithm moves to the next variable, skipping over complete variables, until all missing data is imputed
  - Process is repeated to create each imputed data set
* Numerous algorithms exist
  - Selection on variable being imputed (e.g., continuous, ordered, nominal/categorical)

Recommended R package: mice (https://amices.org/mice/)

???
- Chained equations approach focuses the algorithm on one variable at a time
- Once that variable is imputed, the algorithm moves to the next variable, skipping over complete variables, until all missing data is imputed – this process is then repeated many times to create each imputed data set
- Numerous algorithms exist, and selection depends mainly on the type of variable being imputed (e.g., continuous, ordered or un-ordered nominal/categorical, etc.)

---
# Multiple Imputation Process

Benefits of repeating the process, merging (averaging) each estimate of the missing data:
* Produces a more accurate (and plausible) values
* More precisely defines uncertainty in the data
* Accounts for uncertainty from original data generation (e.g., measurement error) and of the imputation process

```{r, echo=F}
m <- DiagrammeR::mermaid("
                         graph LR
                        A((d))-->B((m<sub>1</sub>))
                       A((d))-->C((m<sub>2</sub>))
                      A((d))-->D((m<sub>3</sub>))
                         B((m<sub>1</sub>))-->E((a<sub>1</sub>)) 
                        C((m<sub>2</sub>))-->F((a<sub>2</sub>)) 
                         D((m<sub>3</sub>))-->G((a<sub>3</sub>))
                         E((a<sub>1</sub>))-->H((r))
                         F((a<sub>2</sub>))-->H((r)) 
                         G((a<sub>3</sub>))-->H((r))
                       ")

widgetframe::frameableWidget(m)
```

---

#### Casewise deletion (CD)

```{r}
m1 <- lm(advice ~ score, galo)
m2 <- lm(advice ~ score + female + medu + fedu + focc, galo)

list("CD" = m1,
     "CD (covariates)" = m2) %>% 
  huxreg()
```

---
# Mean imputation

```{r}
galo_meani <- galo %>% 
  drop_na(advice) %>% 
  group_by(schid) %>% 
  mutate(medumean = mean(medu, na.rm = TRUE),
         fedumean = mean(fedu, na.rm = TRUE),
         foccmean = mean(focc, na.rm = TRUE)) %>% 
  ungroup() %>% 
  mutate(medu = ifelse(is.na(medu), medumean, medu),
         fedu = ifelse(is.na(fedu), fedumean, fedu),
         focc = ifelse(is.na(focc), foccmean, focc))

# Rerun the analysis:

m3 <- lm(advice ~ score, galo_meani)

m4 <- lm(advice ~ score + female + medu + fedu + focc, galo_meani)

list("CD" = m1,
     "CD (covariates)" = m2,
     "MeanI" = m3,
     "MeanI (covariates)" = m4) %>% 
  huxreg()
```

 - Due to the fact that students are nested in schools, the best practice is to compute mean values WITHIN EACH SCHOOL rather than across all observations.
 
 - be extra careful when dealing with missingness on predictor and outcome variables. 

Our outcome variable, advice, has a very small missing value (0.45%), so we should drop all incomplete observations rather than replacing them with variable means

???
Simply imputing all missing values using mean values, but there's good practice and bad practice. 

---
# Multiple imputation (MI)

There are several ways to conduct multiple imputation - here we use the package {mice}.

```{r}
galo_multi <- mice(galo, seed = 12345) 
class(galo_multi) # should be "mids"
summary(galo_multi)
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(2, 1))
```

---
# MI specifications

Specifying imputation algorithms:
- number of imputed data sets (`m`: 5 is default, >= 50 is better)
- number of iterations (`maxit`: 5 is common, 10 is better). 

```{r}
galo_multi <- mice(galo, m = 50, maxit = 10, seed = 12345) 
summary(galo_multi)
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(2, 1))
```

--

Look at the first dataset

```{r}
galo_multi1 <- complete(galo_multi)
head(galo_multi1)
```

???

MICE stands for Multivariate Imputations via Chained Equations and its idea is, if any variable contains missing values, the package regresses it over the other variables and predicts the missng value. For this strategy to work, it requires the missing at random (MAR) assumption. In other words, if we have some reason to believe that missing on any of the variables we're computing is systematic, this approach is not appropriate.
 
---
# Examine strip plot 

```{r}
stripplot(galo_multi, pch = 5, cex = 1.2)
# Red dots are imputed values, blue are observed values
```

---
# Examine kernel density plot

```{r}

densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(3, 1))
# Red lines are imputed value distributions, blue are observed value distributions

```


```{r}
densityplot(galo_multi, scales = list(x = list(relation = "free")), layout = c(3, 1))

```

---
# using this dataset to rerun the analysis
 
```{r}
# Alternate scripting using "tidy" approach
est1 <- galo %>%
  mice(m = 50, seed = 12345) %>%
  with(lm(advice ~ score)) %>%
  pool()
summary(est1)
```

---

```{r}
est2 <- galo %>%
  mice(m = 50, seed = 12345) %>%
  with(lm(advice ~ score + female + medu + fedu + focc)) %>%
  pool()
summary(est2)
```

---

```{r}
list("CD" = m1,
     "CD (covariates)" = m2,
     "MeanI" = m3,
     "MeanI (covariates)" = m4,
     "MultI" = est1,
     "MultI (covariates)" = est2) %>% 
  huxreg()
```


---

{Emphasize the importance of different approaches being most appropriate to particular situations}


---
# Best Reporting Practices

* Report the percent/proportion of data missing on variables in analyses
* Indicate the primary reason you believe data are missing and briefly justify your assumption
* Report the imputation method. If using MI, report the number of imputed data sets and mention algorithm used
* Cite the software/package used to impute data
* If there is substantial missingness, conduct and mention a sensitivity analysis of primary analyses using the imputed data and complete case analyses

---

[insert example write up re data run]

---
# Key Takeaways


---
class: middle, inverse
# Synthesis and wrap-up

---
# Unit Goals

---
# To Dos

### Reading
-

### Assignments
- Assignment #X Due XX at 11:59PM
- Quiz #X Due on X XX at 5PM