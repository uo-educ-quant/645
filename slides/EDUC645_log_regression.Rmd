---
title: "Logistic Regression"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, DiagrammeR)

i_am("slides/EDUC645_log_regression.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")


```

# Roadmap

---
# Goals for the unit
	• linear probability model and discuss the limitations
	• Understand the different model assumptions of logistic regression vs. linear regression.
	• concept of probability, odds, and logit (Distinguish between odds and probability.)
	• logistic function and the logistic regression model 
		○ continuous predictors
			§ how to interpret the model parameters, 
			§ inference for model parameters
		○ categorical predictors 
			§ how to interpret the model parameters, 
			§ inference for model parameters
		○ interaction between a continuous and categorical predictor, 
			§ how to interpret the model parameters, 
			§ inference for model parameters
	• glm function in R to fit the logistic regression model
• Discuss the predictive power, model fit, and diagnostics for a logistic regression model
	• Evaluate models based on Area Under ROC Curve method.
• probit and differentiate from logit

---
# Binary Data

- You've learned to model continuous outcomes with linear regression models BUT there are many instances when we are interested in the probability of an outcome. How do we handle these sort of data?

--

- What about when you are interested in probability, an outcome related to binary data? (e.g., yes/no)  
    • Ex. probability of group given individual traits, probability of needs, etc.
    
--

- How are they different? Why can't we apply the same method?

---
# Binary Outcomes
- Outcomes with a yes/no or 0/1 response

--

- Examples:  
  - Depression or not?  
  - Vaccinated or not?  

--
	
- Proportion vs. Odds

---
# Proportion
- Description 1:

---
# Odds

Odds of an event: the probability of 'yes' divided by the probability of 'no'
$$Odds = \frac{p}{1(1-p)}$$

--

What are the odds of getting Tails when tossing a fair coin?

--

What are the odds of getting a 6 when tossing a fair dice?

---
# Odds and Probability

As the probability of an event increases, so do the odds

[insert data visualization comparing trajectories]

???
- As the probability of an event increases, so do the odds
- Odds increase *almost* exponentially as the probability increases

---
# Binary Outcome and Binary Predictor
- Measures of Association
	• Difference in probabilities = p1-p2
			- If difference in probabilities of the outcome across the categories is NOT 0, then predictor is associated with the outcome

--

	• Odds ratio (OR) = odds on category of predictor/odds other category of predictor
			- If OR is NOT 1, then predictor is associated with the outcome

???
- If the difference in probabilities of the outcome across the categories is NOT 0, there is an association between the outcome and the predictor

- If the OR is NOT 1, then there is an association between the predictor and the outcome

---
# Odds

- One way to quantify a XX's performance. 
Example 

__odds ratio__ \index{odds ratio} (OR): 

which tells us that the *odds* of a successful penalty kick are 3.05 times higher when the shooter's team is leading.

In general, we now have several ways of finding the odds of success under certain circumstances:

$$Odds = \frac{\# \textrm{successes}}{\# \textrm{failures}}=
\frac{\# \textrm{successes}/n}{\# \textrm{failures}/n}=
\frac{p}{1-p}$$

???

In our example, it is also possible to estimate the probability of a goal, $p$, for either circumstance. When the goalkeeper's team is behind, the probability of a successful penalty kick is $p$ = 22/24 or 0.833. We can see that the ratio of the probability of a goal scored divided by the probability of no goal is $(22/24)/(2/24)=22/2$ or 11, the odds we had calculated above. The same calculation can be made when the goalkeeper's team is not behind.
---

class: middle, inverse

# GLM and Logistic Regression

---
# Generalized linear models (GLMs)
- are a way in which to model a variety of different types of responses.
- we apply the general results of GLMs to the specific application of binomial responses. 

$Y$ = the number scored out of $n$ penalty kicks. 
The parameter, $p$, is the probability of a score on a single penalty kick. 

Recall that the theory of GLMs is based on the unifying notion of the one-parameter exponential family form:


$$f(y;\theta)=e^{[a(y)b(\theta)+c(\theta)+d(y)]}$$
---
# Generalized linear models (GLMs)

To see that we can apply the general approach of GLMs \index{generalized linear models (GLMs)} to binomial responses, we first write an expression for the probability of a binomial response and then use a little algebra to rewrite it until we can demonstrate that it, too, can be written in one-parameter exponential family form with $\theta = p$. This will provide a way in which to specify the canonical link and the form for the model. Additional theory allows us to deduce the mean, standard deviation, and more from this form.

If $Y$ follows a binomial distribution with $n$ trials and probability of success $p$, we can write:


P(Y=y)&= \binom{n}{y}p^y(1-p)^{(n-y)} \\
      &=e^{y\log(p) + (n-y)\log(1-p) + \log\binom{n}{y}}
\end{align*}

*However*, this probability mass function is not quite in one-parameter exponential family form. Note that there are two terms in the exponent which consist of a product of functions of $y$ and $p$. So more simplification is in order:

$$P(Y=y) = e^{y\log\left(\frac{p}{1-p}\right) + n\log(1-p)+ \log\binom{n}{y}}$$

The one-parameter exponential family form for binomial responses shows that the  canonical link is $\log\left(\frac{p}{1-p}\right)$. Thus, GLM theory suggests that constructing a model using the logit, the log odds of a score, as a linear function of covariates is a reasonable approach.   

---
# Logistic Regression

Binary (0 or 1) response (Y) and a single continuous predictor (X).

### Model for logistic regression with binary or binomial responses:

$$\log(\frac{p_i}{1-p_i})=\beta_0+\beta_1 x_i$$

where the observed values $Y_i \sim$ binomial with $p=p_i$ for a given $x_i$ and $n=1$ for binary responses.

???

The solid line is a linear regression fit with least squares to model the probability of a success (Y=1) for a given value of X.  With a binary response, the line doesn't fit the data well, and it produces predicted probabilities below 0 and above 1.  On the other hand, the logistic regression fit (dashed curve) with its typical "S" shape follows the data closely and always produces predicted probabilities between 0 and 1.  For these and several other reasons detailed in this chapter, we will focus on the following model for logistic regression with binary or binomial responses:

---
# Assumptions
- Outcome (Y) is discrete binomial variable (0/1 variable)
- Must be enough responses in every category
- Linearity in the logit scale 
  - X's must be linearly related to logit(Y)
- Absence of multicollinearity
- No outliers
- Independence of (X,Y)'s
  - No clustering of data into groups/contexts that would provide information about the values of other errors

---
# Interpretation of Model Parameters

$$\ln(\frac{p_i}{1-p_i})=\beta_0+\beta_1 X_i$$

$\beta_1$: Change in log odds for 1 unit change in $X_i$

[insert example]

---
# Interpretation of Model Parameters
To get the odds ratio, we need to back transform and exponentiate the coefficient, $\beta_i$:

$$\beta_1 = ln(\frac{odds|X=1}{odds|X=0})$$
   $$= ln(OR_{X=1:X=0})$$

$$OR = e^{ln(OR_{X=1:X=0})}$$
  $$= e^{\beta_1}$$

---
# [insert example walk-through with code]


---
# Tests for Significance of Model Coefficients

---
# Confidence Intervals for Model Coefficients

---
# Testing for Goodness-of-Fit

---
# Assess Confounding
1. Compare the unadjusted regression coefficients to the adjusted regression coefficients (not the ORs) 
- Calculating change in coefficient:
$$100*\frac{\beta_{adjusted} - \beta_{unadjusted}} {\beta_{unadjusted}}$$
2. If the change in coefficients > 10%, there is evidence for confounding by the variable in the adjusted model (aka, 10% confounding rule)

---
class: middle, inverse

# Interactions

---
# Assess Interactions

- Assess effect modification by adding interaction term to the model

Model *with* interaction term:

$$ln({Y_{odds}}) - {\beta_0} + {\beta_1}X_1 + {\beta_2}X_2$$
Model *without* interaction term:
$$ln({Y_{odds}}) = {\beta_0} + {\beta_1}X_1 + {\beta_2}X_2 + {\beta_3}(X_1*X_2)$$
---
# Interaction - categorical X categorical

---
# Interaction - continous X categorical

[Assess Interactions - example walkthrough]

---
# Interactions - centering variable

---
# Interaction - continous X continuous
[Assess Interactions - example walkthrough]

[interpretation of output, example write up]

---
# Influential Observations

- Are there observations that greatly change the estimated regression coefficients if they are removed from the dataset?
- Same concept as with DFBetas from multiple linear regression

---
# Influential Observations

[insert Leverage plot]

---

# Predicted Values: Probabilities
- Coefficients tell us the *average* predicted probability for a person, but what if we want to know about more specific groups or contexts?

--

- Obtain predicted probabilities for each person depending on their characteristics

$$p = \frac{e^{(\beta_0 + \beta_1X)}}{1 + e^{{(\beta_0 + \beta_1X)}}}$$
---
class: middle, inverse

# Assessing Predicted Values
 
---

# Assessing Predicted Values

* In MLR, $R^2$ indicates good prediction

--

* In logistic regression, use the *predicted probabilities* 
  - *predicted probabilities* classify people according to one of the two groups of the predictor variable.

---

# Classification Rule
* Need a threshold for defining probability for belonging in one or two of the groups

* But how do we determine an appropriate threshold?

--

* Calculate a cut-off by examining the *sensitivity* and *specificity* of our model

---
# Types of Classification

|             |         |Predicted|       |
|-------------|---------|---------|-------|
|             |         |*Y = 1*  |*Y = 0*|
|**Observed** | *Y = 1* | True +  | False-|
|             | *Y = 0* | False + | True- |

### Sensitivity
* What is the ratio of correct positive predictions (Y = 1) to total positive predictions?
_Positive predictive value (PPV)_ = $\frac{number of True +} {Total_{observed} Y = 1}$

### Specificity
* What is the ratio of correct negative predictions (Y = 0) to total negative predictions?
* _Negative predictive value (NPV)_ = $\frac{number of True -} {Total_{observed} Y = 0}$

---
# Specificity vs. Sensitivity
[insert hypothetical figure 
"Specificity and Sensitivity Change as the Probability Cutoff Changes"]

---
# Receiver Operating Characteristic (ROC) Curve


---
# Receiver Operating Characteristic (ROC) Curve
* Plot of true positive rate against the false positive rate for the different possible cutoff points of a diagnostic test, for example
* Illustrates the tradeoff between *sensitivity* and *specificity*
  - Increase in *sensitivity* will result in decrease in *specificity*, and vice versa
* More accurate tests, the curve is closer to the left-hand border and the top border of the ROC
* The __area under the curve (AUC)__ measures *accuracy*

---
# Area Under the ROC Curve (AUC)

```{r}
simple_roc <- function(labels, scores){
  labels <- labels[order(scores, decreasing=TRUE)]
  data.frame(TPR=cumsum(labels)/sum(labels), FPR=cumsum(!labels)/sum(!labels), labels)
}

set.seed(1)
sim_d <- function(N, noise=100){
  x <- runif(N, min=0, max=100)
  y <- 122 - x/2 + rnorm(N, sd=noise)
  dv <- factor(y > 100)
  data.frame(x, y, dv)
}
dv_d <- sim_d(500, 10)

test_set_idx <- sample(1:nrow(dv_d), size=floor(nrow(dv_d)/4))

test_set <- dv_d[test_set_idx,]
training_set <- dv_d[-test_set_idx,]
```


```{r}
library(ggplot2)
library(dplyr)
test_set %>% 
  ggplot(aes(x=x, y=y, col=dv)) + 
  scale_color_manual(values=c("blue", "orange")) + 
  geom_point() + 
  ggtitle("DV to X")
```

---
# Examining the ROC curve

* Two classifiers that put the labels in the same order will have exactly the same ROC curve regardless of the absolute values of the scores.

* This is shown by comparing the ROC curve you get using either the ‘response’ or the ‘link’ predictions from a logistic regression model. 

* The ‘response’ scores have been mapped into the range between 0 and 1 by a sigmoid function and the ‘link’ scores have not.

```{r, echo = T}
fit_glm <- glm(dv ~ x, training_set, family=binomial(link="logit"))

glm_link_scores <- predict(fit_glm, test_set, type="link")

glm_response_scores <- predict(fit_glm, test_set, type="response")

score_data <- data.frame(link=glm_link_scores, 
                         response=glm_response_scores,
                         dv=test_set$dv,
                         stringsAsFactors=FALSE)
```

---
# Examining the ROC Curve

.pull-left[
```{r plot-first, echo = FALSE, fig.cap="add caption"}
score_data %>% 
  ggplot(aes(x=link, y=response, col=dv)) + 
  scale_color_manual(values=c("black", "red")) + 
  geom_point() + 
  geom_rug() + 
  ggtitle("Both link and response scores put cases in the same order")
```
]
.pull-right[
```{r ref.label = 'plot-first', fig.show = 'hide'}
```
]
---
# Plotting the ROC Curve

.pull-left[
```{r plot-second, echo = FALSE}
library(pROC)
plot(roc(test_set$dv, glm_response_scores, direction="<"),
     col="yellow", lwd=3, main="The turtle finds its way")
## 
## Call:
## roc.default(response = test_set$bad_widget, predictor = glm_response_scores,     direction = "<")
## 
## Data: glm_response_scores in 59 controls (test_set$bad_widget FALSE) < 66 cases (test_set$bad_widget TRUE)
## Area under the curve: 0.9037
glm_simple_roc <- simple_roc(test_set$dv=="TRUE", glm_link_scores)
with(glm_simple_roc, points(1 - FPR, TPR, col=1 + labels))
```

]
.pull-right[
```{r ref.label = 'plot-second', fig.show = 'hide'}
```
]

---
# Obtaining the AUC

.pull-left[
```{r plot-third, echo = FALSE}
set.seed(1)
N <- 2000
P <- 0.01
rare_success <- sample(c(TRUE, FALSE), N, replace=TRUE, prob=c(P, 1-P))
guess_not <- rep(0, N)
plot(roc(rare_success, guess_not), print.auc=TRUE)

## Call:
## roc.default(response = rare_success, predictor = guess_not)
## 
## Data: guess_not in 1978 controls (rare_success FALSE) < 22 cases (rare_success TRUE).
## Area under the curve: 0.5

simp_roc <- simple_roc(rare_success, guess_not)
with(simp_roc, lines(1 - FPR, TPR, col="blue", lty=2))
```
]

.pull-right[
```{r ref.label = 'plot-third', fig.show = 'hide'}
```
]

---

### AUC Guidelines:
* Will vary depending on the outcome variable, but here are some general guidelines:
  - AUC = 0.5 = Poor (equal to random chance)
  - 0.7 < AUC < 0.8 = Acceptable
  - 0.8 < AUC < 0.9 = Excellent
  - AUC > .9 = Outstanding

---

[shiny app for playing around with ROC and AUC:
https://kennis-research.shinyapps.io/ROC-Curves/]

---
# Example -- Unit 1 Logistic Regression

This dataset was drawn from Meta Krüger's (1994) study investigating gender differences in school leadership in Netherlands. The study implemented a matching procedure to generate pairs of comparable schools (N=98) with the only difference being that one school principal was female and the other male, then surveyed the school principals, teachers, and students. Due to deletion of incomplete observations, our dataset is a bit smaller than the one used in the article, it contains 94 school principals and 800 students.

```{r}
principal <- rio::import(here::here("data", "principal.csv")) %>%
  mutate(attention = (q1+q2+q3)/3,
         affective = ifelse(q4 >= 3, 1, 0)) %>% 
  select(schid, stuid, stufemale, female, match, attention, affective) %>% 
  group_by(schid) %>% 
  mutate(attention = (attention-mean(attention))/sd(attention)) %>% 
  ungroup() 

head(principal)
```

---
# The Variables
 - schid, school identification number
 - stuid, student identification number within each school
 - female, coded one for female principals and zero for male principals
 - stufemale, coded one for female students and zero for male students
 - match, coded one if student and principal are the same gender
 - q1, students' rating on a survey item, "Sometimes the principal talks to me", on a 4-point likert scale, 1=low, 4=high
 - q2, students' rating on a survey item, "I think the principal knows who I am", on a 4-point likert scale, 1=low, 4=high
 - q3, students' rating on a survey item, "Principal knows how well I am doing", on a 4-point likert scale, 1=low, 4=high
 - q4, students' rating on a survey item, "The principal is nice", on a 4-point likert scale, 1=low, 4=high

---

For starters, some data managment; we're interested in the association between student-perceived attention from principal and affective attitude toward principal, therefore we compute two variables of interest:

 - attention: take the average score of attention items, q1, q2, and q3, then standardize it at school level to measure student-perceived attention
 - affective: recode q4 to be a binary variable coded one for students who rated 3 or 4 on this question ("the school principal is nice")
 
---
# Visualizing the relationship
 
Plot outcome variable, affective, on predictor variable, attention:

```{r}
principal %>% 
  mutate(affective = recode(affective, '1' = "Yes", '0' = "No")) %>% 
  ggplot(aes(attention, affective, color = affective)) +
  geom_boxplot(size = 0.5) +
  geom_jitter(alpha = 0.2,
              size = 0.5) + 
  scale_x_continuous() + 
  labs(y = "Affective attitude towards principal",
       x = "Self-perceived attention from principal") +
  theme(legend.position = "none") 
```

What patterns do you notice from the plots above?

---

# 1.1.3 Modeling the relationship

How to model these relationships in our analysis?
Our inquiry: The associations between student self-perceived attention from principal and affective attitude towards principal.

What models should we use to fit the data when our outcome variable is a dichotomous variable?

---

# Linear probability model (LPM)

```{r}
m1 <- lm(affective ~ attention, principal)
m2 <- lm(affective ~ attention + match, principal)
summary(m1)
```

Does the regression coefficient of attention confirm your previous impression with the plot?

---

Visualize the coefficient/slope of the fitted line (basically, we force the relationship to be linear):

```{r}
principal %>% 
  ggplot(aes(attention, affective)) +
  geom_point(alpha = 0.01) +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") + 
  labs(y = "Affective attitude towards principal",
       x = "Self-perceived attention from principal")
```


Interpret the results (report regression coefficients **on a probability scale**) and the figure. 

Will this be the model where you stop at?

#### Binomial logistic regression

```{r}
m3 <- glm(affective ~ attention, principal, 
          family = binomial(link = "logit"))
m4 <- glm(affective ~ attention + match, principal,
          family = binomial(link = "logit"))

summary(m3)
```

---

Visualize the coefficient on a log-odds scale (basically we model the change in log-odds of affective in a linear function of attention):

```{r}
tibble(attention = (-3):3) %>% 
  mutate(pred = predict(m3, newdata = .)) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "royalblue") +
  labs(y = "Affective attitude towards principal (LOG-ODDS)",
       x = "Self-perceived attention from principal")
```

---

Visualize the coefficient on a probability scale (mapping the parameters to probability nonlinearly):

Coder note: in the data, "attention" ranges from -2.42 to 2.61, the plot is therefore limited to a fraction when the probability of "affective" approaching one but still below 0.5. Because we cannot easily guess the missing plot fraction (as appose to when the line is linear, we can tell where the line goes simply by looking at the slope), this plot is not telling us enough information about the population of interest:

```{r}
tibble(attention = (-3):3) %>% 
  mutate(pred = predict(m3, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "royalblue") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Self-perceived attention from principal")
```

---

I manipulate the X axis to range from -3 to 15 to show the nonlinear relationship in its full image:

```{r}
tibble(attention = (-3):15) %>% 
  mutate(pred = predict(m3, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "royalblue") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Self-perceived attention from principal")
```

Interpret the results (report regression coefficients **on a log-odds scale**) and the two figures.


---
# Binomial and Beyond
- Probit regression
- Multinomial logistic regression
- Ordinal regression
- Count data
- Hazard ratio
- Survival analysis

---
class: middle, inverse
# Synthesis and wrap-up

---
# Logistic vs. OLS regression
|                 | Linear Least Squares | Binomial Regression |
|-----------------|----------------------|---------------------|
|Response variable| Normal distribution  | Number of successes in $n$ trials |
|Variance         | equal for each level of X | np(1−p) for each level of X |
|Model fitting    | μ=β0+β1x using Least Squares | log(p1−p)=β0+β1x using Maximum Likelihood |
|EDA              | plot X vs. Y; add line | find log⁡(odds) for several subgroups; plot vs. X|
|Comparing models | extra sum of squares F-tests; AIC/BIC | drop-in-deviance tests; AIC/BIC |
|Interpreting coef| β1= change in mean response for unit change in X | eβ1= percent change in odds for unit change in X|