---
title: "Logistic Regression"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, lfe, arsenal, ggpubr, stargazer, fixest, gtsummary, huxtable, aod)

i_am("slides/EDUC645_log_regression.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

```

# Roadmap

---
# Goals for the unit
- Differentiate between the linear probability model and the logistic regression model  
- Describe the concept of probability, odds, and logit function  
- Estimated a fitted regression line using Logistic regression  
- Interpret the model parameters for logistic function and logistic regression model using continuous   predictors, categorical predictors, interaction between a continuous and categorical predictor  
- Discuss the predictive power, model fit, and diagnostics for a logistic regression model  

---
# A Motivating Question

Study investigating gender differences in school leadership in Netherlands (Krüger, 1994). The study matched comparable schools (N=98) then surveyed the school principals, teachers, and students. 

We're interested in the association between school-level student-perceived *attention from principal* and *affective attitude toward principal*(i.e., "the school principal is nice").

---

# Reviewing the data

```{r, echo = F}
principal <- rio::import(here::here("data", "principal.csv")) %>%
  select(schid, stuid, pfemale, sfemale, match, attention, affective, page) %>% 
  group_by(schid) %>% 
  mutate(attention = (attention-mean(attention))/sd(attention)) %>% 
  ungroup() 

head(principal)
```


---
# Describe dichotomous outcome variable

```{r, echo = F}
principal %>% 
  mutate(affective = recode(affective, '1' = "Yes", '0' = "No")) %>%
  ggplot(aes(affective)) +
  geom_bar(fill = "royalblue", width = 0.5, alpha = 0.8) +
  labs(x = "Affective attitude towards principal",
       y = "Number of Students")
```

---
# Describe affective on attention

```{r, echo = F}
principal %>% 
  ggplot(aes(attention, affective)) +
  geom_point(color = "royalblue") +
  labs(y = "Affective attitude towards principal",
       x = "Attention from principal")+
  geom_smooth(method = "loess", color = "red") 
```

---
# Modeling the relationship

Previously you have practiced modeling outcome variables with continuous distributions using various kinds of predictor variables. 

Our inquiry: The association between students among school's attention from principal and attitude towards principal.

With the case of Krüger's data, we need to model the relationship between a dichotomous outcome variable in our analysis.

---
# Binary Outcomes
- Outcomes with a yes/no or 0/1 response

--

- Examples:  
  - Depression or not?  
  - Vaccinated or not?  

--
	
- Proportion vs. Odds

---
# Odds

Odds of an event: the probability of 'yes' divided by the probability of 'no'
$$Odds = \frac{p}{1(1-p)}$$

- As the probability of an event increases, so do the odds

- Odds increase *almost* exponentially as the probability increases

---

class: middle, inverse

# GLM and Modeling Dichotomous Data

---
# Revisiting the GLM

Recall that the theory of GLMs is based on the unifying notion of the one-parameter exponential family form:

Our full population regression model:

$$Y = \color{blue}{\beta_{0} + \beta_{1} X} + \color{green}{\varepsilon}$$

$Y$: our outcome
$\color{blue}{\beta_{0}}$ and $\color{blue}{\beta_{1}}$: our population parameters and regression coefficients to be estimated
$\color{green}{\varepsilon}$: our error/residual ( $\varepsilon$ is a fancy way of writing the Greek letter "epsilon")

--

Also written as:

$$Y_{i} = \beta_{0} + \beta_{1} X_{i} + \varepsilon_{i}$$
where we use the subscript $i$ to emphasize that the model estimates the outcome for each of the $i$ units (students, schools, patients, etc.).

.footnote[[1] Sometimes also called deterministic and stochastic components.]

---

# Linear Probability Model (LPM)

Back to our example:

One viable way to model our data is to treat it as though it *is* linear.

Visualize the coefficient/slope of the fitted line (basically, we force the relationship to be linear):

```{r, echo = F}
principal %>% 
  ggplot(aes(attention, affective)) +
  geom_point(alpha = 0.05) +
  geom_smooth(method = "lm", se = FALSE, color = "royalblue") + 
  labs(y = "Affective attitude towards principal",
       x = "Self-perceived attention from principal")
```
---

# LPM model coefficient interpretation

```{r}
m1 <- lm(affective ~ attention, principal)
summary(m1)
```

Interpret the results (report regression coefficients **on a probability scale**) and the figure. 

---
# Probability scale 
Coefficient on a probability scale: Mapping the parameters to probability nonlinearly

```{r, echo = F}
tibble(attention = (-3):3) %>% 
  mutate(pred = predict(m1, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "royalblue") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Self-perceived attention from principal")
```

???

Note: in the data, "attention" ranges from -2.42 to 2.61, the plot is therefore limited to a fraction when the probability of "affective" approaching one but still below 0.5. Because we cannot easily guess the missing plot fraction (as appose to when the line is linear, we can tell where the line goes simply by looking at the slope), this plot is not telling us enough information about the population of interest:

---

Manipulate the X axis to show the nonlinear relationship in its full image:

```{r, echo = F}
tibble(attention = (-15):20) %>% 
  mutate(pred = predict(m1, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "royalblue") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Self-perceived attention from principal")
```

Interpret the results (report regression coefficients **on a log-odds scale**) and the two figures.

---
# **Log-odds** scale 
### Visualizing the coefficient on a **log-odds** scale 
Models the change in log-odds of affective in a linear function of attention
Manipulated X axis to show the nonlinear relationship in its full image:

```{r, echo = F}
tibble(attention = (-3):15) %>% 
  mutate(pred = predict(m1, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "brown") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Attention from principal")
```

???

Coder note: in the data, "attention" ranges from -2.42 to 2.61, the plot is therefore limited to a fraction when the probability of "affective" approaching one but still below 0.5. 

Because we cannot easily guess the missing plot fraction (as appose to when the line is linear, we can tell where the line goes simply by looking at the slope), this plot is not telling us enough information about the population of interest:

---

```{r}

plot(m1, 1)

```

---
# Interpret the results

Report regression coefficients **on a log-odds scale**

```{r, echo = F}
tibble(attention = (-15):20) %>% 
  mutate(pred = predict(m1, newdata = ., type = "response")) %>% 
  ggplot(aes(attention, pred)) +
  geom_line(color = "brown") +
  labs(y = "Affective attitude towards principal (PROBABILITY)",
       x = "Attention from principal")
```

---
# Another Way to Model the Data

There are other options to modeling dichotomous data.

--

Logistic regression is another option.

--

Logistic regression has become the default approach to modeling binomial distributions, but there are cases in which LPM might be preferred.

---
# Binomial distribution and the Generalized Linear Model (GLM)

How can we apply the general approach of GLMs to binomial responses
- We first write an expression for the probability of a binomial response 
  - Then Provide a way in which to specify the (canonical) link and the form for the model

--

If $Y$ follows a binomial distribution with $n$ trials and probability of event $p$:

$$P(Y=y) = \binom{n}{y}p^y(1-p)^{(n-y)} \\
      =e^{y\log(p) + (n-y)\log(1-p) + \log\binom{n}{y}}$$

---
# Binomial distribution and the GLM
### The missing link
GLM theory suggests that constructing a model using the logit(the log odds of a score) as a linear function of covariates.

The logit function allows us to connect GLM linear regression with logistic regression.

This 'link' function transforms the dependent binomial variable using a one-parameter exponential formula:
$$\log(\frac{p}{1-p})$$

$p$: Probability of a given event

---
# Using Logit Link in Logistic Regression

Where the observed values $Y_i \sim$ binomial with $p=p_i$ for a given $x_i$ and $n=1$ for binary responses, we can insert the logit link function into our GLM model as follows:

$$\color{green}{Y_i}=\beta_0 + \beta_1 x_i + \varepsilon_{i}$$

--

$$\color{green}{Y_i} = \color{blue}{\log(\frac{p_i}{1-p_i})}$$

Becomes...
--

$$\color{blue}{\log(\frac{p_i}{1-p_i})}=\beta_0 + \beta_1 x_i +  \varepsilon_i$$

$Y_i$ is the number events of $n$ observations. 
$p$ is the probability of an event on a single observation. 

---

# Likelihood Models and Estimation

Fitting simple linear models using OLS assumes that the mean value of a response, $Y$, is linearly related to some variable, X. 

However, often responses are not normally distributed. 

*binary response linearly represented*
```{r, echo= F}
principal %>% 
  ggplot(aes(attention, affective)) +
  geom_point(color = "royalblue") +
  labs(y = "Affective attitude towards principal",
       x = "Attention from principal") 
```


---
# Likelihood methods

### Fitting logistic regression requires estimation using likelihood methods:

*Likelihood* function tells us how likely we are to observe our data for a given parameter value, $p{\beta}$. 
- come into play is when data is produced from a complex structure that could imply correlation between outcomes (e.g., students' test scores taught by the same teacher)
- provide flexibility in the types of models we can fit
- provide ways in which to compare models

---
# Distribution of likelihood function $p{\beta}$
```{r, echo= F}
set.seed(110951)
x <- rgamma(1000, 6, 0.5)
library(MASS)
truehist(x, xlim = c(0, 35))
xpts <- seq(from = 0, to = 35, length = 1000)
lines(xpts, dgamma(xpts, 6, 0.5))
```

Note: we will not discuss MLE in the general form. Instead, we will consider a simple case of MLE that is relevant to the logistic regression.

---
# Maximum Likelihood Estimation

*Maximum likelihood estimation (MLE)* is a general class of method in statistics that is used to estimate the model parameters

---
# Maximum Likelihood Estimation

MLE: is the optimal estimate of pB, the value where we are most likely to see our data from all possible values between 0 and 1.
-  iterates over local maxima find the value of pB where the likelihood or log likelihood is a maximum.

*Contour plot*
```{r, echo = F}
# Reference: https://rpubs.com/gill1109/contour

NxPts <- 100
NyPts <- 100

xGridPts <- seq(from = 5, to = 7, length = NxPts)
yGridPts <- seq(from = 0.4, to = 0.6, length = NyPts)

xypairs <- expand.grid(xGridPts, yGridPts) 

LogLikFun <- function(shape, rate) sum(dgamma(x, shape = shape, rate = rate, log = TRUE))

LogLikOnGrid <- mapply(LogLikFun, xypairs[ , 1], xypairs[ ,2]) 


dim(LogLikOnGrid) <- c(NxPts, NyPts) # convert to matrix


MaxLogLik <- max(LogLikOnGrid)

LogLikOnGrid <- matrix(LogLikOnGrid, NxPts, NyPts) - max(LogLikOnGrid)

contour(x = xGridPts, y = yGridPts, z = LogLikOnGrid)

contour(x = xGridPts, y = yGridPts, z = LogLikOnGrid, 
        levels = c( (0:-8)*10), col = "blue", add = TRUE)

contour(x = xGridPts, y = yGridPts, z = LogLikOnGrid, 
        levels = - qchisq(0.95, 2)/2, col = "red", add = TRUE)
```

---
# Same idea, different plot

*Wire-frame plot*
```{r, echo = F}
persp(x = xGridPts, y = yGridPts, z = LogLikOnGrid)
```

---
# MLE limitations

- MLE is primarily a single point estimate with the information contained in a single value
- Likelihoods will not necessarily be symmetrically dispersed around the point of maximum likelihood. 
- We may be interested in the full distribution of credible parameter values, so that we can perform sensitivity analyses and understand the possible outcomes or optimal decisions associated with particular credible intervals. 

Bayesian methods can be applied to circumvent these limitations.


---
# Interpretation of Model Parameters

$$\ln(\frac{p_i}{1-p_i})=\beta_0+\beta_1 X_i$$
$\beta_1$: Change in log odds for 1 unit change in $X_i$

# Regression equation components

$$Affective = \color{orange}{(\beta_0)} + \color{green}{(\beta_1)}(Attention)$$

<span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0.

--

<span style = "color:green"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X.

---

# Logistic regression model estimates

`family` refers to the type of distribution, where we have defined `binomial`, and the `link` asks for the link function that we want to use, which is the `logit` for a logistic regression

```{r}
m3 <- glm(affective ~ attention, principal, 
          family = binomial(link = "logit"))
summary(m3)
```

---

# Logistic regression model estimates

Adding a categorical variable
```{r}
m4 <- glm(affective ~ match, principal,
          family = binomial(link = "logit"))
summary(m4)
```


---
# Categorical/Nominal Predictor Variables

We can test for an overall effect of rank using the Wald test. 

The order in which the coefficients are given in the table of coefficients is the same as the order of the terms in the model.
Wald test refers to the coefficients by their order in the model 
- b supplies the coefficients
- Sigma supplies the variance covariance matrix of the error terms

Terms tells R which terms in the model are to be tested, in this case, terms 1 and 2, are the three terms for the levels of rank.

```{r}

wald.test(b = coef(m4), Sigma = vcov(m4), Terms = 1)

```

---

# Binary Outcome and Binary Predictor

- Measures of Association
	• Difference in probabilities = p1-p2
			- If difference in probabilities of the outcome across the categories is NOT 0, then predictor is associated with the outcome

---
# Odds

- One way to quantify a predictor's effect.

$$Odds = \frac{\# \textrm{successes}}{\# \textrm{failures}}=
\frac{\# \textrm{successes}/n}{\# \textrm{failures}/n}=
\frac{p}{1-p}$$

$p$ represents the probability of an event occuring

---
#Odds Ratio

We can take our interpretation further by producing an odds ratio (OR)

	• Odds ratio (OR) = odds on category of predictor/odds other category of predictor
			- If OR is NOT 1, then predictor is associated with the outcome

???
- If the difference in probabilities of the outcome across the categories is NOT 0, there is an association between the outcome and the predictor

- If the OR is NOT 1, then there is an association between the predictor and the outcome

---
# Practice calculating OR

To get the odds ratio, we need to back transform and exponentiate the coefficient, $\beta_i$:

$$\beta_1 = ln(\frac{odds|X=1}{odds|X=0})$$
   $$= ln(OR_{X=1:X=0})$$

$$OR = e^{ln(OR_{X=1:X=0})}$$
  $$= e^{\beta_1}$$

---

```{r, echo = F}
probabilities <- predict(m3, type = "response")

principal <- principal %>%
  mutate(logit_affect = log((probabilities/(1-probabilities)))) 

principal%>% 
      ggplot(aes(attention, logit_affect)) + 
          geom_point(alpha = 0.5) +
          geom_smooth(method = "loess", color = "red") 

```

---
# Confidence Intervals

We can also obtain confidence intervals (CIs)for the coefficient estimates. 

For logistic models, CIs are based on the profiled log-likelihood function. 

```{r}
## CIs using profiled log-likelihood
confint(m3)
```

We can also get CIs based on just the standard errors by using the default method.

```{r}
## CIs using standard errors
confint.default(m3)
```

---
# Odds Ratios

To obtain ORs, we exponentiate the coefficients. 

We can use the same logic to get odds ratios and their confidence intervals, by exponentiating the confidence intervals from before. 

```{r}
exp(coef(m3))
```

For a one unit increase in attention, the odds of being admitted to graduate school (versus not being admitted) increase by a factor of 1.07.

```{r}
## odds ratios and 95% CI
exp(cbind(OR = coef(m3), confint(m3)))
```
To put it all in one table, we use cbind to bind the coefficients and confidence intervals column-wise.

Note: While R produces the odds ratio for the intercept, this OR is not generally interpreted.

---
class: middle, inverse

# Interactions

---
# Assess Interactions

Assess effect modification by adding interaction term to the model

Model *without* interaction term:

$$ln({Y_{odds}}) = {\beta_0} + {\beta_1}X_1 + {\beta_2}X_2$$
Model *with* interaction term:
$$ln({Y_{odds}}) = {\beta_0} + {\beta_1}X_1 + {\beta_2}X_2 + {\beta_3}(X_1*X_2)$$
---
# Interaction - categorical X categorical

```{r}
summary(glm(affective ~ pfemale*sfemale
             principal,
          family = binomial(link = "logit")))
```

---
# Interaction - continous X categorical

```{r}
summary(glm(affective ~ attention*match, principal,
          family = binomial(link = "logit")))
```

---
# Interaction - continous X continuous

```{r}
summary(glm(affective ~ attention*page, principal,
          family = binomial(link = "logit")))
```

---
# Tests for Significance of Model Coefficients

---
# Confidence Intervals for Model Coefficients

---

class: middle, inverse

# Assumptions and Diagnostics

---
# Assumptions
- Outcome (Y) is discrete binomial variable (0/1 variable)
- Must be enough responses in every category
- Linearity in the logit scale 
  - X's must be linearly related to logit(Y)
- Absence of multicollinearity
- No outliers
- Independence of (X,Y)'s
  - No clustering of data into groups/contexts that would provide information about the values of other errors

---
# Logistic Regression Assumptions

1. Linearity assumption: the linear relationship between continuous predictor variable and the logit of the outcome variable

2. Binomial logistic regression (logistic regression/logit model)

```{r}
m2 <- lm(affective ~ attention + match, principal)
```

Check for critical ASSUMPTIONS that unique to logistic regression:

 - linearity assumption/the linear relationship between continuous predictor variable and the logit of the outcome variable

---
# Outliers: Cook's distance plot 
 
```{r, echo = F}

plot(m3, 4, id.n = 5)

```

---
# Outliers: Residual Errors 

Check standardized residual errors to identify outliers 
Rule of thumb: data points with an absolute error above 3

```{r}
principal_checked <- broom::augment(m3) %>% 
  mutate(index = 1:n())

principal_checked %>% top_n(3, .cooksd)

principal_checked %>% 
  mutate(affective = as.factor(affective)) %>% 
  ggplot(aes(index, .std.resid)) +
  geom_point(aes(color = affective))
```

---

# Influential Observations

- Are there observations that greatly change the estimated regression coefficients if they are removed from the dataset?
- Same concept as with DFBetas from multiple linear regression

```{r, echo = F}

plot(m3, 5, id.n = 5)

```

---
# Multicollinearity  

Can add another predictor to see if there is evidence of collinearity (i.e., if variance inflation factor < 5)

```{r}
car::vif(m2)
```

---
# Multicollinearity  

```{r}
summary(m3)
summary(m4)
```

---

# Comparing models 

Putting models together and report them in a table:

```{r, echo = F}
huxtable::huxreg(
  "LPM" = m1, 
  "LPM" = m2,
  "Logistic" = m3,
  "Logistic" = m4,
  coefs = c("Attention" = "attention",
            "Matched gender" = "match")
)
```

---
# Assess Confounding
1. Compare the unadjusted regression coefficients to the adjusted regression coefficients (not the ORs) 
- Calculating change in coefficient:
$$100*\frac{\beta_{adjusted} - \beta_{unadjusted}} {\beta_{unadjusted}}$$
---
# Predicted Values: Probabilities
- Coefficients tell us the *average* predicted probability for a person, but what if we want to know about more specific groups or contexts?

You can also use predicted probabilities to help you understand the model. Predicted probabilities can be computed for both categorical and continuous predictor variables. In order to create predicted probabilities we first need to create a new data frame with the values we want the independent variables to take on to create our predictions.

We will start by calculating the predicted probability of admission at each value of rank, holding gre and gpa at their means. First we create and view the data frame.

--

- Obtain predicted probabilities for each person depending on their characteristics

$$p = \frac{e^{(\beta_0 + \beta_1X)}}{1 + e^{{(\beta_0 + \beta_1X)}}}$$
---

# Assessing Predicted Values

* In MLR, $R^2$ can be an indicator of good prediction for a model

--

* In logistic regression, the *predicted probabilities* can be referred to for good predictive power for a model
  - *predicted probabilities* classify people according to one of the two groups of the predictor variable.

---

# Classification Rule
* Need a threshold for defining probability for belonging in one or two of the groups

* But how do we determine an appropriate threshold?

--

* Calculate a cut-off by examining the *sensitivity* and *specificity* of our model

---
# Types of Classification

|             |         |Predicted|       |
|-------------|---------|---------|-------|
|             |         |*Y = 1*  |*Y = 0*|
|**Observed** | *Y = 1* | True +  | False-|
|             | *Y = 0* | False + | True- |

### Sensitivity
* What is the ratio of correct positive predictions (Y = 1) to total positive predictions?
_Positive predictive value (PPV)_ = $\frac{number of True +} {Total_{observed} Y = 1}$

### Specificity
* What is the ratio of correct negative predictions (Y = 0) to total negative predictions?
* _Negative predictive value (NPV)_ = $\frac{number of True -} {Total_{observed} Y = 0}$

---

# Logistic vs. OLS regression
|                 | Linear Least Squares | Binomial Regression |
|-----------------|----------------------|---------------------|
|Response variable| Normal distribution  | Number of successes in $n$ trials |
|Variance         | equal for each level of X | np(1−p) for each level of X |
|Model fitting    | μ=β0+β1x using Least Squares | log(p1−p)=β0+β1x using Maximum Likelihood |
|EDA              | plot X vs. Y; add line | find log⁡(odds) for several subgroups; plot vs. X|
|Comparing models | extra sum of squares F-tests; AIC/BIC | drop-in-deviance tests; AIC/BIC |
|Interpreting coef| β1= change in mean response for unit change in X | eβ1= percent change in odds for unit change in X|

---
# Key Takeaways


---
class: middle, inverse
# Synthesis and wrap-up

---
# Unit Goals

---
# To Dos

### Reading
-

### Assignments
- Assignment #X Due XX at 11:59PM
- Quiz #X Due on X XX at 5PM
