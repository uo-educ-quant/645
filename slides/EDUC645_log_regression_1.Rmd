---
title: "Logistic Regression Part 1: Probability and Odds"
subtitle: "EDUC 645"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(NHANES, pander, here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, lfe, arsenal, ggpubr, stargazer, fixest, gtsummary, huxtable, aod)

i_am("slides/EDUC645_log_regression_1.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

NHANES_data <- NHANES::NHANES %>%
  select(ID, SurveyYr, Gender, Age, AgeDecade, Race3, Education, Poverty, Diabetes, DaysMentHlthBad, Depressed, PhysActive, PhysActiveDays) %>% 
  filter(SurveyYr == "2011_12", Age > 12) %>% 
  mutate(Race_Eth = as_factor(case_when(Race3 == "Mexican" | Race3 == "Hispanic" ~ "Hispanic/Latino",
                                    TRUE ~ Race3))) %>% 
  rename(Depress_Freq = Depressed, Sex = Gender, DaysMHBad = DaysMentHlthBad) %>% 
  select(ID, Age, Sex, Race_Eth, Diabetes, Depress_Freq, PhysActive, PhysActiveDays)

```


# An alternative to odds ratio?

--
For an odds ratio, we calculated odds of having diabetes among inactive and active participants by dividing the probability of having (and not having) diabetes in each activity group:

$$\text{Inactive Participants: } \frac{Probability\ of\ Having\ Diabetes}{Probability\ of\ Not\ Having\ Diabetes} = \frac{0.13}{0.87} = 0.15$$

$$\text{Active Participants: } \frac{Probability\ of\ Having\ Diabetes}{Probability\ of\ Not\ Having\ Diabetes} = \frac{0.06}{0.94} = 0.06$$


--
Then we divided these to get the odds ratio:

$$\ OR = \frac{Odds\ of\ Being\ Inactive\ and\ Having\ Diabetes}{Odds\ of\ Being\ Active\ and\ Having\ Diabetes} = \frac{0.15}{0.06} = 2.5$$




---
# An alternative to odds ratio?


For an odds ratio, we calculated odds of having diabetes among inactive and active participants by dividing the probability of having (and not having) diabetes in each activity group:

$$\text{Inactive Participants: } \frac{\color{Green}{Probability\ of\ Having\ Diabetes}}{Probability\ of\ Not\ Having\ Diabetes} = \frac{\color{Green}{0.13}}{0.87} = 0.15$$

$$\text{Active Participants: } \frac{\color{Green}{Probability\ of\ Having\ Diabetes}}{Probability\ of\ Not\ Having\ Diabetes} = \frac{\color{Green}{0.06}}{0.94} = 0.06$$



Then we divided these to get the odds ratio:

$$\ OR = \frac{Odds\ of\ Being\ Inactive\ and\ Having\ Diabetes}{Odds\ of\ Being\ Active\ and\ Having\ Diabetes} = \frac{0.15}{0.06} = 2.5$$



Instead of doing this, let's just divide the *probability* of having diabetes in each activity group. This will give us a $\color{blue}{\text{risk ratio}}$ or $\color{blue}{\text{relative risk}}$ (RR). It can be similar in magnitude to an odds ratio, especially when the probability of the outcome is small (just like how odds and probability can be similar). But, the RR has a somewhat different interpretation.

$$\ RR = \frac{Probability\ of\ Being\ Inactive\ and\ Having\ Diabetes}{Probability\ of\ Being\ Active\ and\ Having\ Diabetes} = \frac{0.13}{0.06} = 2.17$$

---
# An alternative to odds ratio?

To summarize:

$$\ OR = \frac{\frac{Probability\ of\ Being\ Inactive\ and\ Having\ Diabetes}{Probability\ of\ Being\ Inactive\ and\ Not\ Having\ Diabetes}}{\frac{Probability\ of\ Being\ Active\ and\ Having\ Diabetes}{Probability\ of\ Being\ Active\ and\ Not\ Having\ Diabetes}} = 2.5$$

- Inactive participants had a higher odds of having diabetes than active participants. Participants who were inactive had 2.5 times the odds of diabetes than participants who were active.

- Compared with being active, being inactive is associated with a greater odds of diabetes. Inactive participants had 150% greater odds of having diabetes than active participants. 


--
  + If we're communicating *more than* or *less than*, and a ratio of 1 means "no difference", then here the amount of "more than" is 2.5 - 1, which is 1.5 or 150%. "2.5 times" is equivalent, because 1 x 1 = 1.   


--
- If we flipped the ratio so that odds of diabetes in the active group were the numerator, the odds ratio would be O.40 (0.06/0.15). This could be interpreted as "Being active is associated with a 60% reduction in odds of diabetes compared with being inactive."

---
# An alternative to odds ratio?

To summarize:

$$\ RR = \frac{\frac{Probability\ of\ Being\ Inactive\ and\ Having\ Diabetes}{\color{gray}{Probability\ of\ Being\ Inactive\ and\ Not\ Having\ Diabetes}}}{\frac{Probability\ of\ Being\ Active\ and\ Having\ Diabetes}{\color{gray}{Probability\ of\ Being\ Active\ and\ Not\ Having\ Diabetes}}} = 2.17$$

- Inactive participants had a higher risk of having diabetes than active participants. Participants who were inactive were 2.17 times more likely to have diabetes than participants who were active.

- Compared with being active, being inactive is associated with greater risk of diabetes. Inactive participants had 117% greater risk of having diabetes than active participants. 


--
So, if probability (i.e., likelihood, risk) is more interpretable, why do we use odds? 


--
In the example we've just been discussing, we were interested in the relationship between one independent variable (whether or not a participant was physically active `PhysActive`) and the outcome, diabetes status. 


--
Because we have only have two variables, we can represent our data in a simple table...


---
# So, why use odds?

|           | No Diabetes | Diabetes   |
|:----------|:------------|:-----------|
| Inactive  | 1536        | 230        |
| Active    | 2174        | 140        |


--
Both an odds ratio and risk ratio can be calculated directly from this table. We can test the significance of the difference between groups using a chi square test:


--
```{r 2by2test, echo=TRUE}
Table_PhysAct_Diabetes <- table(NHANES_data$PhysActive, 
                                NHANES_data$Diabetes)
chisq.test(Table_PhysAct_Diabetes, correct = FALSE)$p.value
```


--
- R uses scientific notation for very small values, so 1.522e-14 = 0.00000000000001522.


--
So, active and inactive participants significantly differ in the proportion of participants with diabetes. 


--
  + The ratio(s) is also needed, because it tells us the *size* (magnitude) of this difference and its direction (higher odds or risk for inactive participants).


---
# So, why use odds?

This seems simple enough, but what happens if we want to look at the relationship between a *continuous* independent variable, like `age`, and a dichotomous outcome like `diabetes`. 

--
  + **This implies that there is contingency table for each value of the independent value (i.e., for each year of age ranging from 13 to 80):**


--
**Age 61**

|           | No Diabetes | Diabetes   | 
|:----------|:------------|:-----------|
| Inactive  | 24          | 7          |
| Active    | 16          | 2          |


--
**Age 62**

|           | No Diabetes | Diabetes   |
|:----------|:------------|:-----------|
| Inactive  | 13          | 12         |
| Active    | 16          | 7          |

---
# So, why use odds?

Then, what happens when we want to include *multiple* independent variables? These could be continuous, dichotomous, categorical, or a mix.

--
  + This is necessary for many common research questions. For example: What are the odds of diabetes for inactive participants compared with active participants, controlling for participant age?


--
Hopefully, it's becoming clear that looking at anything more than a basic relationship between two dichotomous variables will not result in easily interpretable results (i.e., a single estimate of the relationship).


--
  + We need an analysis method that gives us interpretable results. There are a few options, but the most commonly used is logistic regression. 


--
$\color{blue}{\text{Logistic regression}}$ is an extension of the linear regression model you're familiar. Both are part of the family of generalized linear models (GLMs).


--
  + If different models can be made to work in similar ways, it becomes easier to learn, use, and improve these models. 


---
# So, why use odds?

- Remember that in linear regression, the (continuous) outcome variable responds in a linear way. That is, it behaves like a number line (O in the center with negative numbers extending to the left and positive to the right).


--
  + For example, if an increase in the independent variable is associated with an increase in the outcome, each unit increase in the independent variable results in an increase in the outcome by some amount. *Another* increase in the independent variable results in *another* increase in the outcome *by the same amount*.


--
  + The same thing happens if the association is negative, except each unit increase in the independent variable results in a *decrease* in the outcome by some amount. We tell the difference between these situations by whether the model coefficient is positive or negative. If there were no relationship, the coefficient would be...0. 


--
- Notice the two key features: Values can be positive or negative, which is needed to indicate the direction of effect, and change in the outcome is by a consistent magnitude, for every unit change in the independent variable. 


--
- **Dichotomous outcomes don't naturally behave this way.**

---
# So, why use odds?

We talked earlier about how the most intuitive way to describe a relationship between two dichotomous variables is a proportion (= probability). This was because it doesn't make sense to say that being more active means that a person has "less" diabetes.

--
  + In the NHANES data, 13% of inactive participants had diabetes, compared with 6% of active participants. 


--
Despite having a more straightforward interpretation, probability does not behavior linearly (like a number line): It ranges only from 0 to 1. And, changes in the independent variable don't necessarily result in consistent changes in probability. 

--
  + Odds  have the same behavior, and they also don't range in value like a number line (remember, odds can't be less than 0).


--
So, if we want to be able to apply the same basic linear regression approach to a dichotomous outcome, we have to find some way to transform the outcome so that it behaves more like what the linear model is expecting.


--
Here is where *odds* (and therefore the odds ratio) becomes important. It turns out they have a very handy feature: Taking the $\color{blue}{\text{logarithm}}$ of odds *makes them behave linearly*

???
Sometimes a standard linear regression model is used for dichotomous outcomes. This is called a linear probability model (LPM), and some argue it has a more straightforward interpretation than logistic regression. However, modeling dichotomous outcomes as a continuous range from 0 to 1 is conceptually problematic and violates several key assumptions of linear regression.

---
# Relationship between probability, odds, and log odds

```{r odds table, echo=FALSE}
odds_table <- tibble(Log_odds = seq(from = -5, to = 5, by = 0.25),
                     Odds = round(exp(Log_odds), 3),
                     Odds_diff = Odds - lag(Odds),
                     Prob1 = round(Odds/(1 + Odds), 3),
                     Prob1_diff = Prob1 - lag(Prob1)) %>% 
  select(Prob1, Prob1_diff, Odds, Odds_diff, Log_odds) %>% 
  slice(17:25) %>% 
  mutate(Odds_diff = case_when(Odds_diff < 0.09 ~ NA_real_,
                          TRUE ~ Odds_diff),
         Prob1_diff = case_when(Prob1_diff < 0.05 ~ NA_real_,
                          TRUE ~ Prob1_diff))
  
odds_table
```


---

class: middle, inverse

# GLM and Modeling Dichotomous Data

---
# Revisiting the GLM

Recall that our generalized linear regression model is written as:

$$Y_{i} = \color{blue}{\beta_{0} + \beta_{1} X} + \color{green}{\varepsilon_{i}}$$

 + $Y_{i}$: Our outcome, with the subscript $i$ to emphasize that the model estimates the outcome for each of the $i$ units (students, schools, patients, etc.)

 + $\color{blue}{\beta_{0}}$ and $\color{blue}{\beta_{1}}$: Our population parameters and regression coefficients to be estimated

 + $\color{green}{\varepsilon_{i}}$: Our error/residual


--
The $\color{blue}{\text{logit link}}$ allows us to connect GLM linear regression with logistic regression. 

$$\log(\frac{p}{1-p})$$

 + $p$: Probability of a given event

---
# Using Logit Link in Logistic Regression

When we have an outcome variable that follows the binomial distribution, as does a dichotomous variable, we can insert the logit link function into our GLM model:


--
$$\color{green}{Y_i}=\beta_0 + \beta_1 x_i + \varepsilon_{i}$$


--
$$\color{green}{Y_i} = \color{blue}{\log(\frac{p_i}{1-p_i})}$$

Becomes...

--
$$\color{blue}{\log(\frac{p_i}{1-p_i})}=\beta_0 + \beta_1 x_i +  \varepsilon_i$$

 + $Y_i$ is the number events of $n$ observations. 
 + $p_i$ is the probability of an event on a single observation. 

---
# Likelihood methods

Fitting logistic regression requires estimation using likelihood methods:

- *Likelihood* function tells us how likely we are to observe our data for a given parameter value, $p{\beta}$. 

- Come into play is when data is produced from a complex structure that could imply correlation between outcomes (e.g., students' test scores taught by the same teacher)

- Provide flexibility in the types of models we can fit

- Provide ways in which to compare models

- *Maximum likelihood estimation (MLE)* is a general class of method in statistics that is used to estimate the model parameters


---
# To Dos

### Assignments
- Assignment 1 is due April 25 at 11:59PM

  + Available on Canvas on Monday April 17
  
- Quiz 1 is due April 18 at 11:59PM

  + Available on Canvas on Friday April 14
  
  
---
<!-- # Regression equation components -->

<!-- $$Affective = \color{orange}{(\beta_0)} + \color{green}{(\beta_1)}(Attention)$$ -->

<!-- $\beta_1$: Change in log odds for 1 unit change in $X_i$ -->

<!-- <span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0. -->

<!-- -- -->

<!-- <span style = "color:green"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X. -->

<!-- --- -->
<!-- # Logistic regression model estimates -->

<!-- `family` refers to the type of distribution, where we have defined `binomial`, and the `link` asks for the link function that we want to use, which is the `logit` for a logistic regression -->

<!-- <!-- ```{r} --> -->
<!-- <!-- m3 <- glm(affective ~ attention, principal,  --> -->
<!-- <!--           family = binomial(link = "logit")) --> -->
<!-- <!-- summary(m3) --> -->
<!-- <!-- ``` --> -->

<!-- --- -->
<!-- # Logistic regression model estimates -->

<!-- <!-- Adding a categorical variable --> -->
<!-- <!-- ```{r} --> -->
<!-- <!-- m4 <- glm(affective ~ match, principal, --> -->
<!-- <!--           family = binomial(link = "logit")) --> -->
<!-- <!-- summary(m4) --> -->
<!-- <!-- ``` --> -->

