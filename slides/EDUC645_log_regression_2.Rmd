---
title: "Logistic Regression Part 2: Model Fitting / Diagnostics"
subtitle: "EDUC 645"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(NHANES, performance, DHARMa, pander, here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, lfe, arsenal, ggpubr, stargazer, fixest, gtsummary, huxtable, aod)

i_am("slides/EDUC645_log_regression_2.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

NHANES_data <- NHANES::NHANES %>%
  select(ID, SurveyYr, Gender, Age, AgeDecade, Race3, Education, Poverty, Diabetes, DaysMentHlthBad, Depressed, PhysActive, PhysActiveDays) %>% 
  filter(SurveyYr == "2011_12", Age > 12) %>% 
  mutate(Race_Eth = as_factor(case_when(Race3 == "Mexican" | Race3 == "Hispanic" ~ "Hispanic/Latino",
                                    TRUE ~ Race3))) %>% 
  rename(Depress_Freq = Depressed, Sex = Gender, DaysMHBad = DaysMentHlthBad) %>% 
  select(ID, Age, Sex, Race_Eth, Diabetes, Depress_Freq, PhysActive, PhysActiveDays)

```


---
# Revisiting the GLM

Recall that our generalized linear regression model is written as:

$$Y_{i} = \color{blue}{\beta_{0} + \beta_{1} X} + \color{green}{\varepsilon_{i}}$$

 + $Y_{i}$: Our outcome, with the subscript $i$ to emphasize that the model estimates the outcome for each of the $i$ units (students, schools, patients, etc.)

 + $\color{blue}{\beta_{0}}$ and $\color{blue}{\beta_{1}}$: Our population parameters and regression coefficients to be estimated

 + $\color{green}{\varepsilon_{i}}$: Our error/residual


--
The $\color{blue}{\text{logit link}}$ allows us to connect GLM linear regression with logistic regression. 

$$\log(\frac{p}{1-p})$$

 + $p$: Probability of a given event

---
# Using Logit Link in Logistic Regression

When we have an outcome variable that follows the binomial distribution, as does a dichotomous variable, we can insert the logit link function into our GLM model:


--
$$\color{green}{Y_i}=\beta_0 + \beta_1 x_i + \varepsilon_{i}$$


--
$$\color{green}{Y_i} = \color{blue}{\log(\frac{p_i}{1-p_i})}$$

Becomes...

--
$$\color{blue}{\log(\frac{p_i}{1-p_i})}=\beta_0 + \beta_1 x_i +  \varepsilon_i$$

 + $Y_i$ is the number events of $n$ observations. 
 + $p_i$ is the probability of an event on a single observation. 

---
# Likelihood methods

Fitting logistic regression requires estimation using likelihood methods:

- *Likelihood* function tells us how likely we are to observe our data for a given parameter value, $p{\beta}$. 

- Come into play is when data is produced from a complex structure that could imply correlation between outcomes (e.g., students' test scores taught by the same teacher)

- Provide flexibility in the types of models we can fit

- Provide ways in which to compare models

- *Maximum likelihood estimation (MLE)* is a general class of method in statistics that is used to estimate the model parameters

---

class: middle, inverse

# Setting Up the Model

---

# Data check

Before setting up the model, we should check that our variables are the correct types. Dichotomous variables should be *factors*, and continuous variables should be *numeric* (num) or *integers* (int).

```{r}
str(NHANES_data)
```

- Also check the *order* of factor levels (e.g., "No", "Yes"):
  + For factors to be used as **independent variables**, the second level should be the desired "target" level (the state we want to focus our interpretation on, e.g., "Yes" indicating being physically active). The first level will be the reference level. 
  + For **dependent variables**, the second level should be the outcome of interest (e.g., "Yes" indicating having diabetes). 

---
# Regression equation components

$$Diabetes = \color{orange}{(\beta_0)} + \color{green}{(\beta_1)}(Age)$$

$\beta_1$: Change in *log odds* for 1 unit change in $X_i$

<span style = "color:orange"> Intercept $(\beta_0)$: </span> Predicted outcome when X is equal to 0.

<span style = "color:green"> Slope $(\beta_1)$: </span> Predicted increase in the outcome for every one unit increase in X.

---
# Logistic regression model estimates

`family` refers to the type of distribution, where we have defined `binomial`, and the `link` asks for the link function that we want to use, which is the `logit` for a logistic regression.

We'll start with a continuous independent variable `Age`. Example question: Is increased age associated with greater odds of diabetes? 

---
# Logistic regression model estimates

```{r}
mod_1 <- glm(Diabetes ~ Age, data = NHANES_data,
                 family = binomial(link = "logit"))
summary(mod_1)
```

---
# Logistic regression model estimates

Or, our question could be: Do inactive participants have greater odds of diabetes than active participants, controlling for age? 

We can add our categorical activity variable `PhysActive`:

---
# Logistic regression model estimates

```{r}
mod_2 <- glm(Diabetes ~ PhysActive + Age, data = NHANES_data,
                 family = binomial(link = "logit"))
summary(mod_2)
```

---
# Output

- Remember that the quantities the model is working in are *log odds*, but we want to be working in *odds* and *odds ratios*. To do this, we need to exponentiate our model parameters.

  + *Note:* An odds ratio from a multiple logistic regression model is referred to as an *adjusted odds ratio* (aOR).


--
- The default output also doesn't provide us confidence intervals, which we need for interpreting how precise are results are. 


--
  + For logistic models, we use what are called *profiled* confidence intervals. The standard CI you learned about last term (the Wald CI) is strongly dependent on the assumption of normality. The profiled CI is less sensitive to this assumption, and takes advantage of the same likelihood methods we use to estimate the model. 


---
# Output

```{r}
mod_2 %>% 
      broom::tidy(exponentiate = TRUE, conf.int = TRUE)
```

---
# Recoding levels of independent variable

We can make `PhysActiveYes` the reference level, so that our interpretation focuses on inactive participants ("Compared with active participants, inactive participants had...).


```{r}
NHANES_data <- NHANES_data %>% 
  mutate(PhysActive = factor(PhysActive, levels = c("Yes", "No")))

mod_3 <- glm(Diabetes ~ PhysActive + Age, data = NHANES_data,
             family = binomial(link = "logit"))

mod_3 %>% 
      broom::tidy(exponentiate = TRUE, conf.int = TRUE)
```


---
# Model Comparison

- We can use a few approaches to identify a comparatively well-fitting model. As with linear regression, we consider a null model (a model with no independent variables) to be the simplest possible model. 


--
- Ideally, the independent variables to be added to the model should be selected with theoretical justification. After demonstrating improved fit with the added variables, it can be appropriate to modify the model to improve fit further (e.g., through including interactions or transformations of variables like squaring or cubing)


--
- Often adding more variables improves the fit of a model, so we must be cautious about overcomplicating the model or improving fit just for fit's sake. 


--
- In linear regression, you learned about $R^2$ as one tool for model comparison. A similar concept in logistic regression is *deviance*, which is a measure of the goodness of model fit. By comparing deviance across models, we can gauge whether fit is improving with additional independent variables or other changes. 
  + Among models compared, the one with the lowest deviance is likely to be the best model for the given dataset. We can test the significance of the difference in deviance across models, so long as the models use exactly the same data.
  + A conceptually similar metric is AIC: Among models compared, the one with the lowest AIC is likely to be the best model for the given dataset.

---
# Model Comparison

**Before comparing models, ensure there is no missingness on all independent variables.**

Comparing deviance between null model and model with just `PhysActive`:

```{r}
mod_1b <- glm(Diabetes ~ PhysActive, data = NHANES_data,
             family = binomial(link = "logit"))

print(anova(mod_1b, test ="Chisq"))
```

---
# Model Comparison

Comparing deviance between model with just `PhysActive` and model with both `PhysActive` and `Age`:

```{r}
print(anova(mod_1b, mod_2, test ="Chisq"))
```

---
# Model Comparison

We can make similar comparisons with AIC:

```{r}
mod_0 <- glm(Diabetes ~ 1, data = NHANES_data,
             family = binomial(link = "logit"))
mod_0$aic 
```

```{r}
mod_1b$aic 
```


```{r}
mod_2$aic 
```

**It's best to corroborate model fit using several approaches!**

---
# Assumptions and Diagnostics

After selecting a best-fitting model, then we assess whether there are major violations of logistic regression assumptions. 

- Outcome (Y) is discrete binomial variable (0/1 variable)

- Must be enough responses in every category

- Linearity in the logit scale 
  - X's must be linearly related to logit(Y)
  
- Absence of multicollinearity

- No outliers

- Independence of (X,Y)'s 
  + No clustering of data into groups/contexts that would provide information about the values of other errors

---
# Assumptions and Diagnostics

We can check outliers and multicollinearity visually using the `check_model()` function from the *performance* package.

```{r}
check_model(mod_3, check = c("outliers", "vif"))
```

---
# Assumptions and Diagnostics

In generalized linear models like logistic regression, checking the normality of residuals can be challenging.

  + In linear regression, we anticipate that residuals will follow the normal distribution. In logistic regression, however, remember that we are dealing with probability (the probability of having the outcome (1) or not (0)). 
  
  + Probability doesn't follow a normal distribution, so residuals from logistic and similar models may appear non-normal even when the model is actually well-fitting. 


--
One solution is to "standardize" the residuals so that they behave like residuals from a typical linear regression model. We can do this using the *DHARMa* package. 

```{r}
# After loading the DHARMa package, 
# create an object containing the standardized residuals:
mod_3_residuals <- simulateResiduals(mod_3)
```

---
# Assumptions and Diagnostics

```{r}
# Then plot the residuals:
plot(mod_3_residuals)
```


---
# To Dos

### Assignments
- Assignment 1 is due April 26 at 11:59PM

  + Available on the course website today.
  
  