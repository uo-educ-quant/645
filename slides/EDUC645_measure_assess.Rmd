---
title: "Measurement & Assessment"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2)

i_am("slides/EDUC645_measure_assess.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 6.75,
  fig.width = 10.5,
  warning = F,
  message = F
)
opts_chunk$set(dev = "svg")
options(device = function(file, width, height) {
  svg(tempfile(), width = width, height = height)
})
options(knitr.table.format = "html")

```

# Roadmap

---
# Goals for the unit
Measurement

Reliability

Validity

---
class: center, inverse

# Measurement

---
# Measurement

Our ability to link theoretical concepts and observable responses.

Good measurement is indispensable.
Allow for the testing of a theory or its parts
Ensure inferences are accurate and correct
---
# Classical Measurement Theory

X = T + E

X is the observed score
T is the True Score
E is the Error associated with Measurement 
---
# Measurement Error
Random error
Chance factors confounding a measure’s true score
Random error is inversely related to a measure’s reliability

Non-random error
Systematic bias in measurement
Non-random error influences the validity of a measure

---
# Properties of Measurement
Reliability
Internal reliability
Temporal reliability

Validity
Face validity
Criterion-related validity
Concurrent validity
Predictive validity
Content validity
Construct validity

---
class: center, inverse

# Reliability

---
# Reliability
Consistency of measurement
Computed by taking the ratio of the true score variance and the observed variance:
[ insert formula ]

As VAR(T) increases, so does Reliability.

---
# Measuring Reliability
* Test-Retest method: tests stability

* Inter-rater method: test equivalence

* Alternative form method 

* Split-halves method: tests internal consistency

* Coefficient alpha: Internal consistency method

---
# Test-retest
Examines temporal stability of a measure. 
- May be influenced by the time between assessments. 
- Always provide details regarding the time between assessments in a study. 
- Practice effect bias.

Individuals should not have “changed” in ways that would bias the test-retest assessment. 
- Any difference in the assessment could be non-random error and reduce the reliability.

Only appropriate when the individual has not changed as a result of the first test, and/or the length of time between assessments has diminished the likelihood of practice effect bias.

---
# Alternative Form
Developer must create two separate tests examining the same domain, yet use different content.  

Method provides a test of equivalences.
If possible, both versions are administered as closely in time as possible (e.g., same day). 
Avoid order effects by randomizing sequence among participants.

Care must be taken. Tests will never be perfectly equivalent, particularly when the measure is assessing abstract elements.

---
# Split Halves

Offer the assessment yet divide the items at random, hence “split-half method”:
  - Split in two comparable sets, you can compare the reliability of one set to the other.  
  - Splitting the measure 'artificially' lowers reliability
    - Adjust the reliability estimate using the Spearman-Brown adjustment:
  
  $$r_{adjusted} = \frac{N*r_{sh}}{1 + (N-1)r_{sh}}$$

---
# Internal Consistency
Assessment of how well a set of variables correlate with each other (“hang together”).

Internal consistency estimates are sensitive to:
- Missing data
- Unidirection of variables
- Number of items

---
# Cronbach's Alpha

$$\alpha_{c} = \frac{k}{(k-1)[1-\Sigma(\frac{s^{2}_{i}}{s^{2}})]}$$

$k$ = number of items included in scale analysis
$s^{2}_{i}$ = variance of scores for each item
$s^{2}$ = variance of all scores in the scale
Range = 0-1

---
# Data example

Add Health public use data.

In wave IV inhome survey, there were eight variables collected  on a Likert-type ordinal scale that seem to measure a latent construct - happiness. See details as well as key variables information below. 

Measured on a four-point likert-type scale (0=never or rarely, 1=sometimes, 2=a lot of the time, 3=most of the time or all of the time).
*q1* to *q8*, the individual's answers to eight survey questions
The questions are as following:
   - "Think about the past seven days. How often was each of the following things true during the past seven days?"
      - q1, "You were bothered by things that usually don't bother you."
      - q2, "How often do you feel isolated from others?"
      - q3, "You had trouble keeping your mind on what you were doing."
      - q4, "You felt depressed."
      - q5, "You felt happy."
      - q6, "You enjoyed life."
      - q7, "You felt sad."
      - q8, "You felt that people disliked you."
      
```{r, echo = F}
happy0 <- rio::import(here::here("data", "ah_happiness.csv")) %>% 
  drop_na()

head(happy0)
```

### Check for reverse coded items 

Among eight items, q5 and q6 seem to measure happiness but other six items are reverse coded. 
Recode them to be consistent with q5 and q6.

```{r}
happy <- happy0 %>% 
  mutate(q1 = recode (q1, "0"=3, "1"=2, "2"=1, "3"=0),
         q2 = recode (q2, "0"=3, "1"=2, "2"=1, "3"=0),
         q3 = recode (q3, "0"=3, "1"=2, "2"=1, "3"=0),
         q4 = recode (q4, "0"=3, "1"=2, "2"=1, "3"=0),
         q7 = recode (q7, "0"=3, "1"=2, "2"=1, "3"=0),
         q8 = recode (q8, "0"=3, "1"=2, "2"=1, "3"=0))
```

### Look into the eight items

#### Summary statistics 

```{r}
items <- happy %>% 
  select(q1:q8) 

summary(items)
```

#### Response frequencies

```{r}
items %>%  
  response.frequencies() %>% 
  round(2)
```

#### Internal consistency 

Chronbach’s alpha:

```{r}
alpha(items[,1:8])
```

---
# Interpreting Cronbach's Alpha and C.I. for Cronbach's alpha


---
# Standards for reliability
Cronbach's alpha 
       α ≥ .9 Excellent 
.9 > α ≥ .8 Good 
.8 > α ≥ .7 Acceptable 
.7 > α ≥ .6 Questionable 
.6 > α ≥ .5 Poor 
.5 > α Unacceptable

---
# Reliability: KR-20

Items are dichotomous (0 or 1)
Kuder-Richardson formula number 20

$$KR20 = \frac{k}{(k-1)[1-\Sigma\frac{p_{i}q{i}}{s^2}]}$$

k = number of items included in scale analysis
pi = proportion responding “positively”
qi  = 1 – pi
s2 = variance of all scores in the scale

---
# Attentuation Correction
Estimates the correlation between X & Y, assuming perfect measurement.

$${\rho}x_{T}y_{T} = \frac{{\rho}x_{i}y_{j}}{\sqrt{p_{xx'}p_{yy'}}}$$
Observed correlation over the SQRT of the (Reliability of X*Reliability of Y). 

---
class: center, inverse

# Validity

---
# Validity
* Face validity: Measure seems to assess domain

* Criterion-related validity
  - Concurrent validity: Measure is correlated with behavior
  - Predictive validity: Measure is predictive of behavior

* Content validity
  - Measurement includes holistic elements of domain

* Construct validity
  - Measurement relates to other theoretically-derived measures

---

# Above and beyond: Factor analysis

Statistical method to uncover clusters (factors) of interrelated variables.

Each factor represents items that are highly correlated with each other.

The higher the inter-correlation of the item, the higher the factor loading of the item on a given factor. 

*Learn more in SEM courses*

---
class: middle, inverse
# Synthesis and wrap-up

