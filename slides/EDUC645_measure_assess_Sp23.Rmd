---
title: "Reliability in Measurement"
subtitle: "EDUC 645 (Spring 2023)"
#author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, lfe, ggpubr, fixest, gtsummary, huxtable, psych, arsenal, stargazer, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, MBESS)

i_am("slides/EDUC645_measure_assess_Sp23.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

happy0 <- rio::import(here::here("data", "ah_happiness.csv")) %>% 
  drop_na()

happy <- happy0 %>% 
  select(q1:q8) %>% 
  mutate(q1 = recode (q1, "0"=3, "1"=2, "2"=1, "3"=0),
         q2 = recode (q2, "0"=3, "1"=2, "2"=1, "3"=0),
         q3 = recode (q3, "0"=3, "1"=2, "2"=1, "3"=0),
         q4 = recode (q4, "0"=3, "1"=2, "2"=1, "3"=0),
         q7 = recode (q7, "0"=3, "1"=2, "2"=1, "3"=0),
         q8 = recode (q8, "0"=3, "1"=2, "2"=1, "3"=0))
```


# What do we mean by measure?


--
Most of what we study in the social sciences - whether as predictors or outcomes - cannot be directly observed. We've talked about many examples already:


--
- Depression, anxiety, math ability, well-being, suicidality, self-efficacy


--
Measuring describes our ability to assess the presence and intensity/severity of the construct of interest accurately (validity) and consistently (reliability).


--
- In other words, when we measure something, we are trying to link individuals' responses to an underlying construct.


--
- Validity is best investigated using more advanced methods like factor analysis (in structural equation modeling) and item response theory (IRT).


--
Measurement involves methods to create measurement tools, assess their properties, and gauge whether we're making valid inference from their results.

---
# Measurement methods

The most common measurement methodology is called classical test theory (CTT):


--
$X$ = $T$ + $e$

* $X$ is the observed score
* $T$ is the true score
* $e$ is the error associated with measurement


--
Measurement error has two components:


--
- Random error: Chance factors that obscure an individual’s true score (T)


--
- Non-random error: Systematic bias in measurement that influences the validity of a measure


--
There will always be some amount of error involved in measurement. Our goal is to minimize this error as much as possible by developing or using measures that correspond as closely as possibly to the underlying construct. 

---
# Reliability 

Consistency of measurement:

* Individuals with the same true value of the construct have similar responses to the measure.

* Responses to the measure won't systematically differ just because of being administered the measure at different times or by different researchers.

* Measure questions/items measure the same construct (internal consistency)


---
# Reliability Methods

* Test-retest method: tests stability

* Inter-rater method: tests equivalence

* Alternative form method: tests whether different forms of the measure function similarly

* Split-halves method: tests internal consistency

* Coefficients alpha and omega: tests internal consistency


---
# Internal Consistency

How well a set of variables correlate with each other (“hang together”) is internal consistency reliability. If measures are strongly correlated, this gives some evidence that they are measuring the same construct across participants.

* Internal consistency estimates are sensitive to missing data, whether items share the same direction, and the number of items.

* Estimates are also specific to the sample: Determining that a scale is "reliable" in one dataset doesn't mean it will be in others. 


--
Most common metric is Cronbach's Alpha (coefficient alpha); another option is coefficient omega. Both range from 0-1, with estimates closer to 1 indicating greater internal consistency. 


--
Alpha has limitations:

* It relies on unrealistic assumptions that can result in under- or overestimating reliability.
* As an estimate there is some degree of uncertainty associated with its estimation, but it is routinely presented without a confidence interval.

---
# Data example - Add Health

Eight items from a rating scale intended to measure the (latent) construct of happiness.


--
"How often was each of the following things true during the past seven days?" 

  - q1: "You were bothered by things that usually don't bother you."
  - q2: "How often do you feel isolated from others?"
  - q3: "You had trouble keeping your mind on what you were doing."
  - q4: "You felt depressed."
  - q5: "You felt happy."
  - q6: "You enjoyed life."
  - q7: "You felt sad."
  - q8: "You felt that people disliked you."

(0 = never or rarely, 1 = sometimes, 2 = a lot of the time, 3 = most or all of the time)

---
# Data example - Add Health

```{r}
head(happy)
```


--
We'll use the MBESS package to calculate coefficients with accompanying confidence intervals.

---
# Alpha

```{r} 
happy %>% 
  select(q1:q8) %>% 
  ci.reliability(type = "alpha", interval.type = "11")
```

---
# Omega

```{r}
happy %>% 
  select(q1:q8) %>% 
  ci.reliability(type = "omega", interval.type = "mlr")
```


---
# Standards for reliability

Most people consider an alpha (or omega) value of 0.8 or greater to be optimal, but what is acceptable depends on context and the implications for the measure's use.


--
Measures with significant impact will likely require higher reliability than less important measures:
- $\alpha = .75$ may be acceptable for an online personality quiz, whereas it may not be unacceptable for a medical test result


--
Some recommendations:

* Rely on literature and content expertise when interpreting reliability

* Use well-validated and widely used measures whenever possible

* Do not use listwise deletion to estimate internal consistency (MBESS functions will impute using FIML, if specified)

* Present estimate with confidence intervals



