---
title: "Introduction and nested data"
subtitle: "EDUC 645: General Linear Model II"
author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{R, setup, include = F}
library(pacman)
p_load(here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, DiagrammeR, lme4, equatiomatic, performance, lmerTest, gtsummary)

i_am("slides/EDUC645_nested_data.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

```

# Roadmap

---
# Goals for the unit
- Introduce nested data structure (two-level, e.g., repeated measures within individuals, students within schools)
- Understand nesting/clustering means
- Demonstrate varying intercepts and slopes across clusters
- Practice fitting the same regression model independently for each cluster and create a table of intercepts and slopes across clusters
- Understand what it means for intercepts and slopes to vary across clusters
- Discuss the notion that regression coefficients may vary across different clusters
- Develop the concept of:
  - Fixed-effects as the aggregation of the different intercepts and slopes by taking an average of varying intercept and slopes across clusters
	- Random effects as the difference between cluster-specific intercept and slope and their averages across clusters
	- Covariance matrix of random-effects
- Discuss the implications of ignoring the variance of random-effects in terms of the inference for fixed-effects and statistical power

- Introduce 
	- Random intercept-only model estimation through the R lmer package, 
		- interpret the model parameters and align them with the earlier discussion of fixed-effects and random-effects.
	- Intra-class correlation as a measure of within-cluster dependency and how to use it to decide whether one needs to use multilevel modeling

---
# A Motivating Question

The Data: Stanford Education Data Archive (SEDA), launched in 2016 to provide nationally comparable, publicly available test score data for U.S. public school districts, allowing scientific inquiries on the relationships between educational conditions, contexts, and outcomes (especially student math/ELA achievements) at the district-level across the nation.

Data set:
- district-level data for 103 Oregon school districts, year 2017-18 data
- Observations with missing values on any of the key variables were deleted for simplification

From now on, we pull out the cluster of subject, and focus on math achievement only.
**Overarching inquiry: What is the relationship between percentage of ELL students and math achievement in Oregon districts?**

---
class: middle, inverse

# Multilevel Models

---
# Multilevel Models (MLM)

* Units of analysis: Individual and Group
* Examine between-individual and between-group variability
* The intercept and slopes may be different for individuals within and across groups
* These differences may be explained by cross-level interactions (differential effects due to individual and group-level characteristics)
* Allows for the inclusion of important individual and group variables
* No residual correlation

---
# Sampling and Design Considerations

### Formulating Research Questions

#### Several questions may be proposed and result in different multilevel equations:
- What is the association between UVI and number of partners among women in PR, after adjusting for clustering effects?
- Is there an association between UVI and living in a city where female condoms (FCs) are free and accessible?
- Do characteristics pertaining to a city modify the association between UVI and number of partners?

---
# Design Considerations

#### What constitutes a group-level variable?
- Characteristic of
  - individual (provider, partner)
  - a setting (number of beds in hospital, allocated budget for HIV prevention in an agency)
  - catchment areas (Census, DHS)
- Policies (% of budget allocated to abstinence-only)

--

#### How are group-level variables defined?
- Aggregating individual-level characteristics into groups (school district SES)
- Developing group-level measures (mental illness stigma in a school district)

---
# Sampling Considerations

#### Calculating Statistical Power is similar to “traditional” analyses
- Formulate the hypotheses to be tested
- Compute or Estimate the ICC
- Determine the variable and outcome of interest
- Determine the sample frame based on the I.V.(s)

--

#### Sampling clusters or sampling individuals?
- Determine the number of parameters in the model
- Calculate the number of observations needed for each cluster

--

#### Law of Diminishing Returns

---
class: middle, inverse

# Nested and Crossed Random Effects

---
# Modeling the relationship between two variables
**For this introductory course, we only focus on two-level clustered data.**

**Overarching inquiry: What is the relationship between percentage of ELL students and math achievement in Oregon districts?**

What model/models can we fit to answer the question? 

# Descriptives
```{r, echo = FALSE}
seda %>%
  select(-district) %>% 
  tbl_summary(
    statistic = list(all_continuous() ~ "{mean} ({sd})",
                               all_categorical() ~ "{n} / {N} ({p}%)"),
    label = list(subject ~ "Subject",
                 grade ~ "Grade",
                 achievement ~ "Achievement",
                 gap_gender ~ "Achievement Gender Gap",
                 percent_ell ~ "ELL Percentage",
                 percent_sped ~ "SPED Percentage",
                 percent_frl ~ "FRL Percentage")) %>%
  modify_header(label ~ "**Key Variables**")
```


```{r, echo = FALSE}
district <- seda %>%
  filter(subject == "Math") %>% 
  select(-subject) %>% 
  select(math = achievement, everything()) %>% 
  mutate(grade = recode(grade, '3' = "G3", '4' = "G4", '5' = "G5", '6' = "G6"))
```

---
class: middle, inverse

# Fixed Effects & Random Effects

---

# Fixed effects model

* Assume a common intercept for all grades. 

--

```{r, echo = FALSE, fig.cap="add caption"}
district %>% 
  ggplot(aes(percent_ell, math)) +
  geom_smooth(method='lm', se = 0) +
  geom_point(alpha = 0.1) +
  labs(x = "ELL Percentage",
       y = "Math")
```

---

# Fixed effects model

```{r, echo=T}
m1 <- lm(math ~ percent_ell, district)
summary(m1)
```

---
# Fixed effects model

```{r, echo=T}
coef(m1)
```

---
# Random effects model

Allowing the intercept to differ across grades:

```{r, echo = F}
district %>% 
  ggplot(aes(percent_ell, math, color = grade)) +
  geom_smooth(method='lm', se = 0) +
  geom_point(alpha = 0.1) + 
  labs(x = "ELL Percentage",
       y = "Math")
```

-- 
Note:
 - Intercept: each grade has a unique intercept
 - Slope: slope changes accordingly

**mixed-effects models can do more!** 

???

Another way to answer the question is allowing the intercept to differ across grades - a unique intercept for each grade.

---
# Model 1. 
## Simple linear regression model for comparison

```{r}
m1 <- lm(math ~ percent_ell, district)
coef(m1)
```

---
# Model 2. 
## Multilevel model with *random __intercepts__*
    - What parameter differs across grades? what parameter remains the same?

```{r}
m2 <- lme4::lmer(math ~ percent_ell + (1|grade), district)
summary(m2)
coef(m2)$grade
```

---

The notion being used through this unit:
```{r}
equatiomatic::extract_eq(m2)
```

---
# Model 3. 
## Multilevel model with *random __slopes__* 

* **Note**: this comparison is primarily for instructive purposes
* How do you interpret the parameters now?
 
```{r}
m3 <- lmer(math ~ 1 + (0 + percent_ell|grade), district)
summary(m3)
coef(m3)$grade
# extract_eq(m3) doesn't work here
```

---
# Model 4. 
## Multilevel model with *random __intercepts__ and __slopes__*

* How about the parameters for this model specification?
* Specify the relationship between percentage of ELL and math achievement for grades 3, 4 & 6

```{r}
m4 <- lmer(math ~ percent_ell + (1 + percent_ell|grade), district)

summary(m4)
```

```{r, echo=TRUE}
coef(m4)$grade
```

```{r, echo=TRUE}
extract_eq(m4)
```

---
# Fixed effects vs random effects

If we put the two previous plots together:
* what does the bright red line represent? 
* what do the other four lines represent?

```{r, echo = FALSE}
district %>% 
  ggplot(aes(percent_ell, math)) +
  geom_smooth(method='lm', se = 0, color = "deeppink") +
  geom_point(alpha = 0.1) + 
  geom_smooth(aes(color = grade), method='lm', se = 0) +
  geom_point(aes(color = grade), alpha = 0.1) +
  scale_color_hue(l=80, c=30) +
  labs(x = "ELL Percentage",
       y = "Math")
```

---
# Fixed effect (model 1) vs. random effects (model 4) parameters

* **Fixed effect** represents the aggregation of the different intercepts and slopes by taking an average of varying intercept and slopes across clusters
 
* **Random effects** represent the difference between cluster-specific intercept and slope and their averages across clusters

```{r}
coef(m1)[1]
mean(unlist(coef(m4)$grade[1]))
```

```{r}
coef(m1)[2]
mean(unlist(coef(m4)$grade[2]))
```


---
class: middle, inverse

# Interdependence and the Intraclass Correlation Coefficient (ICC)

---
# What does “independence” mean?
#### The observation of one participant does not depend on the response of other participants

#### What happens to independence when:
- We collect data across multiple sites?
- Collect observations from the same individual over time?

---
# Intraclass Correlation

**Intraclass correlation coefficient**: Proportion of total variability in the outcome that occurs between groups.

i.e., the ICC is calculated as a ratio

ICC = $(variance of interest) / (total variance) = (variance of interest) / (variance of interest + unwanted variance)

--

* Larger ICC = More inequality between groups

???
The greater the ICC, the greater the reflection of inequality between groups.

---

# Intraclass Correlation
### Common methods to address the ICC “issue”
- Ecologic Regression
- Stratified Regressions
- Group fixed-effects Model
- Contextual Model
- Hierarchical Linear Models

--

### Group-level modeling

--

### Repeated-measured modeling (“Growth curves”)

---
# Manually Calculate ICC

[insert example]

---
# When modeling random effects becomes necessary?
## Unconditional model and ICC

We estimate a baseline model ($model_0$) to see how much variance in math is attributable to between-grade variation

```{r}
m0 <- lmer(math ~ 1 + (1 | grade), district)
extract_eq(m0)
summary(m0)
```

---
#Interpreting the ICC

*R notes: starting the {performance} package usage*

```{r}
performance::icc(m0)
```

Interpretation: Approximately 64.5% of the variance in math achievement lies between grades. 

Conclusion: The fixed effects model masks important information regarding differences in grade-level achievement.

???

Specifically, about 64.5% of the variance in math achievement lies between grades. 

At this point, the fixed effects model aggregating grade-level achievement to a grand mean is masking important information regarding grade-level achievement.
 
allow you to investigate changes over time

---
class: middle, inverse

# Assessing Model Fit

---
# Choosing the preferred model 
We have several models: 
* Fixed effects model (simple linear regression, model 1) 
* Four random effects (i.e., mixed-effects) models
  - Unconditional model (model 0)
  - Random intercepts (model 2)
  - Random slopes (model 3)
  - Random intercepts and slopes (model 4)

How do we know which one is preferred?

--
Indices and Methods to assess Model Fit:
* Root Mean Square Estimate (RMSE)
* Chi-squared significance test of the change in the model deviance
* Information criteria (i.e., AIC/BIC)
* Cross validation procedures

???
So far, we have one fixed effects model, but
how do we know which one is preferred?

---
# Compare model 1 and model 4:

```{r}
performance::compare_performance(m1, m4) %>% 
  print_md()
```

```{r}
test_likelihoodratio(m1, m4) %>% 
  print_md
```

Compare model 2 and model 4:

```{r}
performance::compare_performance(m2, m4) %>% 
  print_md()
```

```{r}
test_likelihoodratio(m2, m4) %>% 
  print_md
```

--

**Interpretation**: Model 4 performs best

_**HOWEVER**_ model 4 doesn't perform much better than model 2 

**Conclusion:** Choose model 2 for *parsimony*

???

Model 4 doesn't perform much better than model 2 (identical RMSE;
- non-significant chi squared test of the change in the model deviance; 
- also visually confirmed in the previous random-intercept random-slope plot that the slope of the fitted line didn't vary too much across four grades), so for this little exercise, I would choose model 2 for parsimonious reason.

---

# Model Building and Parsimony


---

# Summary
### Multilevel Modeling allows us to answer complex questions adequately if:
- An appropriate study design is conceptualized
- Measures across levels are identified from existing datasets or carefully operationalized

### Multilevel Modeling may be appropriate if residual correlations exist due to violations of independence
- Benefit of partitioning variance even when Level-1 or Level-2 variables will not be modeled

---
# Key Takeaways


---
class: middle, inverse
# Synthesis and wrap-up

---
# Unit Goals

---
# To Dos

### Reading
-

### Assignments
- Assignment #X Due XX at 11:59PM
- Quiz #X Due on X XX at 5PM