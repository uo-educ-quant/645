---
title: "Nested Data"
subtitle: "EDUC 645 (Spring 2023)"
#author: "TBD"
#date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: ['default', 'uo', 'ki-fonts', 'my_custom.css', 'xaringanthemer.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
  editor_options: 
  chunk_output_type: console
---

```{R, setup, include = F}
library(pacman)
p_load(easystats, emmeans, DHARMa, ggeffects, haven,
       here, tidyverse, ggplot2, xaringan, knitr, kableExtra, foreign, broom, xaringanthemer, reshape2, DiagrammeR, lme4,
       performance, lmerTest, gtsummary)

i_am("slides/EDUC645_nested_data_Sp23.rmd")

extra_css <- list(
  ".red"   = list(color = "red"),
  ".blue"  =list(color = "blue"),
  ".green" = list(color = "#8bb174"),
  ".purple" = list(color = "#6A5ACD"),
  ".red-pink" = list(color= "#e64173"),
  ".grey-light" = list(color= "grey70"),
  ".slate" = list(color="#314f4f"),
  ".small" = list("font-size" = "90%"))

write_extra_css(css = extra_css, outfile = "my_custom.css")

# Knitr options
opts_chunk$set(
  comment = "#>",
  fig.align = "center",
  fig.height = 5,
  fig.width = 8,
  warning = F,
  message = F
)

seda <- read_csv(here::here("data", "seda_oregon.csv")) %>%
  filter(subject == "mth") %>% 
  select(-subject) %>% 
  mutate(grade = as_factor(recode(grade, '3' = "Grade 3", '4' = "Grade 4", '5' = "Grade 5", '6' = "Grade 6")))

popular <- haven::read_sav(file ="https://github.com/MultiLevelAnalysis/Datasets-third-edition-Multilevel-book/blob/master/chapter%202/popularity/SPSS/popular2.sav?raw=true") %>% 
  select(pupil:popteach) %>% 
  mutate(sex = as_factor(case_when(sex == 1 ~ "Female",
                                   sex == 0 ~ "Male"))) 

respire <- geepack::respiratory %>% 
  as_tibble() %>% 
  mutate(outcome = as_factor(case_when(outcome == 1 ~ "Yes", 
                                       outcome == 0 ~ "No")),
         visit = as_factor(case_when(visit == 1 ~ "Baseline",
                                     visit == 2 ~ "F1",
                                     visit == 3 ~ "F2",
                                     visit == 4 ~ "F3")),
         age.c = datawizard::standardize(age, center = TRUE, scale = FALSE),
         treat = case_when(treat == "P" ~ "Treatment", 
                           treat == "A" ~ "Placebo"),
         assignment = treat) %>% 
  select(-baseline, -treat)
```


# What is Nested Data?


--
Nesting occurs when variability in our outcome occurs at multiple levels.


--
A common example we've already discussed is student outcomes: If Oregon were interested in looking at factors that influence students' academic performance, performance is expected to differ from student to student. Can performance vary at other levels?


--
- Performance could also vary from classroom to classroom (on average) and from school to school (on average). If the federal government were interested in this question, performance may also vary from state to state.


--
What are other examples of nesting you can think of?


--
Another form of nesting occurs in longitudinal studies, where the outcome is assessed multiple times for each participant. 


---
# What is Nested Data?

One way of thinking of nesting is as a form of <span style = "color:green"> clustering</span>: There are clusters of observations in our data that share something in common. 


--
- Observations share the same context (e.g., the same classroom or school, when there are many classrooms or schools in our data) or could be clustered within each person in our dataset (multiple observations from the same person). 


--
Clustered observations are dependent, meaning they are more similar than they would be if they were unclustered (independent) observations.


--
In the academic performance example, what do you think happens if we don't account for variability in performance at the classroom and school levels? And what about the longitudinal study example, where we have dependent observations within each person?


--
- We get biased estimates of effects and precision.



---
# Analyzing Nested Data

--
In addition to more accurate estimates when we model nested or clustered data appropriately, we might be interested in different relationships at each level:


--
- Student level: Do students who are not routinely bullied have better performance than students who are routinely bullied?


--
- Classroom level: Does the presence of a teaching assistance result in better student performance, compared with classrooms that don't have a teaching assistant?


--
- School level: Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
With a model for nested data - generally called a multilevel model (MLM) or hierarchical linear model (HLM) - we can account for variability at multiple levels *and* test hypotheses at one or more levels using a single model. 


---
# Aggregation Bias

A final concern with nested data is that when we ignore nesting, we risk <span style = "color:green"> aggregation bias</span>. 


--
Imagine we were studying the school-level question on the previous slide, Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
- In this example, school-level data was the mean student performance level for each school, and classroom-level data was the mean student performance level for each classroom. The higher level data are <span style = "color:green"> aggregate </span>data, in the sense that means are aggregates of more granular data. 


--
If we modeled the relationship between two variables - whether or not the school had a sufficient number of academic counselors (IV) and mean student performance for each school (DV) - we might find that having adequate academic counseling was associated with greater average student performance. 


--
- Would this approach give us enough insight to conclude that policy-makers should provide more resources for academic counseling?


--
It might, but what if this relationship was different in each classroom? 





---
# Aggregation Bias

Maybe there was little or no relationship in small classrooms and a stronger relationship in larger classrooms. What's a plausible reason why this could be the case?


--
- Perhaps in small classrooms, teachers can give more attention to students, while in larger classrooms, students are not getting adequate support. 


--
- So, academic counseling makes more of a difference in the larger classrooms, because it is making up for support the teacher is not able to provide. In the smaller classrooms, the students don't need more support and are already high performing without academic counseling. 


--
Aggregation bias would occur if we concluded from our model that adequate academic counseling improves average student performance at the school level, when the relationship is actually more complex and depends on lower-level factors like class size (or presence of teaching assistants, etc). 


--
- If a policy-maker concluded from our school-level findings that the solution was for schools to have more funding for academic counseling, that might be wasting resources if more students would benefit from addressing the underlying, classroom-level issue (i.e., reducing class size by increasing funding to hire more teachers, have more classroom space, etc).


---
# Analyzing Nested Data

Several methods can be used to account for nesting or clustering of data so that we get more accurate estimates of associations (or effects) and error/significance. Some methods also allow for investigating relationships at multiple levels.


--
<span style = "color:green"> Adjusting standard errors: </span> The most straightforward approach is to adjust the model standard errors for clustering. This is based on the idea that a naive model (where the clusters are ignored) would consider every observation independent, in which case there would appear to be more information in the data than there really is (resulting in artificially narrow confidence intervals). 


--
<span style = "color:green"> Subgroup-level analyses: </span> With large enough sample sizes and appropriate adjustment for multiple comparisons, we could carry out our analyses within each subgroup or cluster.


--
<span style = "color:green"> Multilevel (or mixed-effects) models: </span> The most common analytic approach for nested data, which can address dependency among observations and can be used to examine the relationships between the outcome and (different) predictors at each level. 


--
<span style = "color:green"> Repeated-measures models: </span> With longitudinal data, there are several approaches that can be used to account for repeated observations from the same participant (e.g., growth curve modeling).


---
# Example

The Stanford Education Data Archive (SEDA) was launched in 2016 to provide nationally comparable, publicly available test score data for U.S. public school districts. 


--
SEDA allows researchers to study relationships between educational conditions, contexts, and outcomes (e.g., student math achievement) at the district level across the nation.


--
We'll look at district-level data for 103 Oregon school districts from the 2017-18 academic year. 

- Each district has 4 observations (rows), one for each grade from 3-6.

- Observations with missing values on any of the key variables were deleted for simplification.


---
# View the data 

```{r}
head(seda, n = 5) %>% print_md()
```


--
.small[Notice the 4 observations for each grade in each district. Whenever you have multiple observations that share something in common (share the same district, group, person), this is an indication that you probably have nested data!]


---
# Understanding clustered data

Let's look at the average math achievement score (`achievement`) across all grades for each Oregon district.


--
```{r, echo = FALSE, fig.retina=3}
seda %>% 
  group_by(district) %>% 
  mutate(mean = mean(achievement)) %>%
  ungroup() %>% 
  select(district, mean) %>%
  ggplot(aes(district, mean)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```


--
<span style = "color:green"> What is the relationship between proportion of ELL students and math achievement in Oregon districts? </span>


---
# % ELL and achievement


--
We might consider averaging all 4 grades' math achievement scores in each district and fit a simple linear regression for the association between % ELL and math achievement:


--
```{r, echo = FALSE, fig.retina=3}
seda %>%
  group_by(district) %>% 
  mutate(achievement = mean(achievement)) %>% 
  ungroup() %>% 
  ggplot(aes(percent_ell, achievement)) +
  geom_smooth(method='lm', se = FALSE) +
  geom_point(alpha = 0.1) + 
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


--
But do we have more information if we didn't average the outcome over the 4 grades?

---
# Achievement for all grades

```{r, echo = FALSE}
seda %>% 
  group_by(district) %>% 
  mutate(mean = mean(achievement)) %>%
  ungroup() %>% 
  select(district, mean) %>%
  ggplot(aes(district, mean)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```

---
# Achievement for each grade

```{r, echo = FALSE}
seda %>% 
  group_by(district, grade) %>% 
  mutate(mean = mean(achievement)) %>% 
  ungroup() %>% 
  select(district, grade, mean) %>%
  ggplot(aes(district, mean, color = grade)) +
  geom_point() +
  labs(x = "District",
       y = "Math Achievement")
```


--
If we fit a model for each grade, what could differ about the regression lines for each grade?


---
# Relationship by grade

Fitting a simple linear regression model for each grade (i.e., 4 separate models):


--
```{r, echo = FALSE, fig.retina=3}
seda %>%
  group_by(district, grade) %>% 
  mutate(achievement = mean(achievement)) %>% 
  ungroup() %>% 
  ggplot(aes(percent_ell, achievement, color = grade)) +
  geom_smooth(method='lm', se = FALSE) +
  geom_point(alpha = 0.1) +
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


--
Now that you see regression lines for each grade, what model parameters (intercepts, slopes) did end up differing for each grade?

---
# Intraclass correlation coefficient

We can estimate the differences we've been discussing using the <span style = "color:green"> intraclass correlation coefficient (ICC) </span>. The ICC gives us an estimate of how much of the variability in our outcome is simply because of differences in the outcome between groups (in our example, grades). 


--
The ICC ranges from 0 to 1. The larger the ICC, the greater the proportion of variability in the outcome that is due to group differences. The ICC is one tool we can use to gauge whether clustering is important enough to warrant multilevel modeling.


--
<span style = "color:green"> The ICC for the math achievement outcome in our data is 0.645 or 64.5%. </span> How would you interpret this? 


--
- 64.5% of the variation in the math achievement outcome is attributable to differences in achievement between grades. 


--
- Because the ICC is telling us that more than two-thirds of the variability in the outcome is because of grade (and has nothing to do with ELL proportion), we know that our findings could mask important differences in grade-level achievement.


---
# Simple linear regression

```{r, echo=T}
m1 <- lm(achievement ~ percent_ell, data = seda)
coef(m1) # Extracting intercept and %ELL coefficient
```


--
This is called a <span style = "color:green"> fixed-effect model. </span> We're assuming that grades are sufficiently similar that we can average the slopes and intercepts for each grade, and this will give us an accurate enough picture of the relationship of interest. 


--
Another way of saying this is each grade's slope and intercept are set - "fixed" - to be the average slope and intercept across grades.


---
# Simple linear regression

When thinking about whether a fixed-effect approach makes sense, we can apply the same reasoning we use when thinking about any average. 


--
Any mean is a single value (say, 31.5) that represents a set of values:


--
- If that set is close to the average (say, 29 and 34), then the single value of 31.5 is probably a good representation of the underlying values (i.e., it's not very different from 29 or 34). 


--
- But the same mean of 31.5 could represent the values of 0 and 63.


--
0-63 is the range of the Beck Depression Index (BDI): Does a mean of 31.5 convey that one person has no depression symptoms (0) and a second person has severe depression (63)?


---
# Mixed-effects model

Instead of the simple linear regression (fixed-effect model), we can fit a mixed-effects model that allows intercepts, or both intercepts and slopes, to vary across grades. 


--
In other words, this approach allows us to examine the underlying values of the intercepts and/or slopes that are contributing to the fixed-effect model estimates (the average of each grade's intercept and slope). 


--
- Another term for this is *disaggregating*, in the same sense as we used when talking about aggregation bias. We're potentially reducing our risk of aggregation bias by disaggregating results by grade. 


--
For now, let's look at what happens if we let the intercept (the average math achievement level at 0% ELL) vary by grade. This is called a <span style = "color:green"> random-intercepts model </span> because the intercept is allowed to vary for each grade. 


---
# Random-intercepts model

For mixed-effects models, we'll use the `lmer()` function from the base R package *lme4*, which uses a function and formula style similar to the models we've been fitting:

--
```{r}
m2 <- lmer(achievement ~ percent_ell + (1 | grade), data = seda)
```


--
The only difference from the usual linear regression specification is the element in parentheses. This is the "random effect" part of the model, or what we want to allow to differ across groups/clusters. 


--
- What is after the `|` is the variable that indicates the group/cluster. What is before the `|` is what we're interested in allowing to vary for each group. 


--
- Like the basic regression model where 1 is used to fit a model that estimates only the intercept, here the `(1 | grade)` means "only the intercept (1) can vary by grade."


---
# Random-intercepts model

```{r}
coef(m2)$grade # Specify the grouping variable
```


--
What do you notice is different for each grade?



---
# ICC and variability explained

In practice, before fitting a mixed-effects model, we'd fit the equivalent of the null model we're used to and use it to calculate the ICC. 


--
```{r}
m0 <- lmer(achievement ~ 1 + (1 | grade), data = seda)
performance::icc(m0)
```


--
There's a relationship between the ICC and variability explained, or $R^2$:


--
.pull-left[
```{r}
performance::r2_nakagawa(m0)
```
]


--
.pull-right[
In essence, the mixed-effect model is resetting our baseline of "explainable" variation from all of the variation in achievement (100%) to only the portion that is not attributable to differences in achievement by grade (100% - 65% = **35%**). 
]

---
# Adding random slopes

So far we've considered one intercept and slope for all grades or examining individual intercepts for each grade.


--
It's fairly clear from the earlier plot that the random-intercepts model is probably sufficient for our data. But let's illustrate what happens if we examine intercepts *and* slopes for each grade (random-slopes-and-intercepts model).


--
<span style = "color:green"> Random-slopes-and-intercepts model: </span>

```{r}
m3 <- lmer(achievement ~ percent_ell + (percent_ell|grade), data = seda)
coef(m3)$grade
```


--
.small[Note that we can look at slopes for each grade with only one intercept across grades (random-slopes model), but in practice we wouldn't typically test this model. If the random-intercepts model is better fitting than the fixed-effect model, the next logical step in model fitting is to add random slopes to the random-intercepts model (i.e., the random-slopes-and-intercepts model).]

---
# Fixed effect vs random effects

```{r, echo = FALSE, fig.retina=3}
seda %>% 
  ggplot(aes(percent_ell, achievement)) +
  geom_smooth(method='lm', se = FALSE, color = "deeppink") +
  geom_point(alpha = 0.1) + 
  geom_smooth(aes(color = grade), method='lm', se = 0) +
  geom_point(aes(color = grade), alpha = 0.1) +
  scale_color_hue(l=80, c=30) +
  labs(x = "ELL Proportion",
       y = "Math Achievement")
```


---
# Bringing it together

--
.pull-left[
The <span style = "color:green"> random-slopes-and-intercepts model </span> gives us the slope and intercept for each cluster:

```{r}
coef(m3)$grade
```
]

--
.pull-right[
The <span style = "color:green"> fixed-effect model </span> gives us the average (or aggregation) of the different intercepts and slopes across clusters:
 
```{r}
coef(m1)
```
]


--
.pull-left[
In other words, the fixed-effect results are the average of the intercepts and slopes for each grade:
]


--
.pull-right[
```{r, echo=FALSE}
cbind(`(Intercept)` = mean(unlist(coef(m3)$grade[1])), 
      percent_ell = mean(unlist(coef(m3)$grade[2])))
```
]


--
So, back to averages: Does the average of the slope and the intercept give a good enough representation of the slopes and intercepts across grades? If not, which one do you think we need to examine by grade?


---
# Model comparison

We can use many of the same tools we've already used to compare models, including deviance tests and AIC. We'll also use RMSE (root mean square error), which is the average difference between values predicted by the model and the actual values in our data (so, lower RMSE = better fitting).


--
```{r}
performance::test_likelihoodratio(m1, m2) %>% 
  print_md() # Only for slides
```


--
```{r}
performance::test_likelihoodratio(m2, m3) %>% print_md()
```


---
# Model comparison

```{r}
performance::compare_performance(m1, m2, m3, metrics = "common") %>% 
  print_md()
```


--
So, the random-intercepts model is better fitting than the fixed-effect model (the simple linear regression), but the random-slopes-and-intercepts model is not any better fitting than the random-intercepts model.


---
# Back to variability explained


--
.pull-left[

<span style = "color:green"> Our understanding from the fixed-effect model: </span>All variation in math achievement could be explained, but %ELL explained very little of it (marginal $R^2$ from previous slide, or $R^2$ from simple linear regression). We might have concluded that %ELL was not an important predictor of math achievement. 
]


.pull-right[
```{r, echo = FALSE, fig.retina=3}
m1r2 <- summary(m1)$r.squared
m0icc <- performance::icc(m0)$ICC_unadjusted

piedata1 <- tibble(
  Variability=c("Explained by ELL%", "Unexplained"),
  value=c(m1r2, 1-m1r2)
)

ggplot(piedata1, aes(x="", y=value, fill=Variability)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.position="top", 
        legend.title = element_blank(),
        legend.text = element_text(size=18),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "pt"))
```
]


--
.pull-left[
<span style = "color:green"> Our understanding from the random-intercepts model: </span>Only one-third of variation in math achievement could be explained after accounting for differences in achievement by grade (65%). %ELL and grade differences explain 72% of variability in math achievement (conditional $R^2$). This could lead to a very different conclusion compared with the fixed-effect model. 
]


.pull-right[
```{r, echo = FALSE, fig.retina=3}
piedata2 <- tibble(
  Variability=c("Explained by ELL% + Grade", "Unexplained"),
  value=c(m1r2+m0icc, 1-(m1r2+m0icc))
)

ggplot(piedata2, aes(x="", y=value, fill=Variability)) +
  geom_bar(stat="identity", width=1) +
  coord_polar("y", start=0) +
  theme_void() +
  theme(legend.position="top", 
        legend.title = element_blank(),
        legend.text = element_text(size=18),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 0, unit = "pt"))
```
]


---
# Covariates, interactions, etc

Just as with the fixed-effect models we've used up to now, we can improve mixed-effects model fit by adding covariates. 


--
However, as we've seen, with mixed-effects models we can also influence model fit by adding random intercepts or random intercepts and slopes. 


--
For this reason, in mixed-effects models it is especially important to use theory and existing research to select covariates and moderators of interest ahead of time.


--
- It also means that studies that will generate nested data (and analyses of existing data) should be carefully planned and informed by available theory and research. 


--
Finally, we must approach fit testing carefully, to be sure we are isolating and testing each change we've made (if we test the addition of a covariate and a random effect together, we don't know which improved model fit).

---
# Popular

The dataset `popular` includes data on students' levels of extraversion and popularity, students' sex, and teacher experience level (a classroom-level variable). 1,000 students are nested in 100 classrooms. 


--
```{r}
head(popular) %>% print_md()
```


--
How can you tell the difference between student-level variables and classroom-level variables?



---
# Including covariates and cross-level interaction

**We're interested in whether students who are more extroverted are, in general, more popular than students who are more introverted.**


--
Imagine existing studies have sometimes, but not always, found that this relationship differs by (or, is moderated by) student sex. Because this is not a consistent finding, we'll plan to control for sex but not examine a moderation relationship (i.e., interaction). 


--
Finally, we have an exploratory question about whether teacher experience is a moderator of the association between extraversion and popularity.


--
- Perhaps more experienced teachers assert more control of the classroom, resulting in less extraversion/attention-seeking from students (so, student popularity in those classrooms may be driven by other factors).


---
# Model fitting steps

--
1) Null model including a random effect for classroom, to calculate the ICC:

```{r}
pop_m0 <- lmer(popular ~ 1 + (1 | class), data = popular)
```

--
2) Random-intercepts model including the student-level independent variable (extraversion) and control variable (sex):

```{r}
pop_m1 <- lmer(popular ~ extrav + sex + (1 | class), 
               REML = FALSE, data = popular)
```


--
3) Random-intercepts-and-slopes model to test whether random slopes are needed for student-level predictors:

```{r}
pop_m2 <- lmer(popular ~ extrav + sex + (1 + extrav + sex | class), 
               REML = FALSE, data = popular)
```


--
4) Test the cross-level interaction using the best-fitting model, then run diagnostics.


---
# Initial look at the data

```{r, echo = FALSE, fig.retina=3}
popular %>%
  ggplot(aes(extrav, popular, color = class, group = class)) +
  geom_smooth(method='lm', se = FALSE, size = .5, alpha  = .8) +
  geom_point(size = 1, alpha = 0.2, position = "jitter") +
  labs(x = "Extraversion",
       y = "Popularity") +
  scale_color_gradientn(colors = rainbow(100)) +
  theme_minimal() +
  theme(legend.position = "none")
```


---
# Model fitting 


--
*Load the `lmertest` package, which includes supplementary tools for `lmer` models, before fitting any models.*


--
```{r}
performance::icc(pop_m0)
```


--
A substantial amount of variability in student popularity (37%) is attributable to differences in popularity across classrooms, so it will be important to account for this going forward. 


---
# Model fitting 

```{r, warnings=FALSE}
performance::compare_performance(pop_m1, pop_m2, metrics = "common") %>% 
  print_md()
```


--
Overall, the model with random intercepts and slopes is better fitting (AIC, RMSE) and somewhat more explanatory (conditional $R^2$). However, we added 2 random slopes, so we want to see if it's necessary to keep both of these or just one. 



---
# Model fitting 

First, we can see in the model summary that the variance for sex is very small, meaning the slope for sex doesn't vary much across classroom. 


```{r}
print(VarCorr(pop_m2), comp = "Variance")
```


--
This is a clue that sex may not need to be retained as a random slope (we're also getting a "failed to converge" warning at the end of the summary, which can be because of low variance of a random effect).


---
# Model fitting 

We can test the significance of the difference in fit between models with and without each of the random slopes in a similar way to the likelihood ratio test we've used before. We can test models that differ only in random effects by using the `ranova()` function from *lmertest*.

```{r, warnings = FALSE}
lmerTest::ranova(pop_m2, reduce.terms = TRUE)
```

---
# Model fitting 

We saw that the random slope for sex is nonsignificant, so our final model before testing the cross-level interaction should retain the random slope for extraversion and drop the random slope for sex. We can now add the interaction to that model:

```{r}
pop_m3 <- lmer(popular ~ extrav + sex + 
                 (1 + extrav | class), REML = FALSE, data = popular)
pop_m3_int <- lmer(popular ~ extrav + sex + texp*extrav + 
                     (1 + extrav | class), REML = FALSE, data = popular)
coef(summary(pop_m3_int)) # Assess interaction significance
```


---
# Model fitting 

```{r, warnings=FALSE}
performance::compare_performance(pop_m3, pop_m3_int, metrics = "common") %>% 
  print_md()
```

--
So, our interaction is significant and the model with the interaction seems better fitting. But before we interpret it, we should run diagnostics on the final model. Note that we don't need to standardize residuals because our outcome is continuous. 

<span style = "color:green"> Before diagnostics and interpretation of a final model, rerun the model without `REML = FALSE`. </span>


---
# Diagnostics

```{r, fig.retina=3}
pop_m3_int_fin <- lmer(popular ~ extrav + sex + texp*extrav + 
                         (1 + extrav | class), data = popular)
check_model(pop_m3_int_fin, check = c("linearity", "outliers", 
                                      "vif", "qq"))
```

---
# Interpreting the interaction

Overall, there aren't major concerns from model diagnostics. To interpret the interaction, we'll take a similar approach as we have for other interactions using predicted values, but here we'll use the `ggpredict` function and set *type* to `re` for random effects. 

Teacher experience is a continuous variable ranging from 2 to 25 years, so we'll also set a few levels of experience: new teacher (2 years), moderately experienced (10 years), highly experienced (25 years). 


--
```{r}
# using the ggeffects package
library(ggeffects)
pop_m3_int_pred <- ggpredict(pop_m3_int_fin,
                             type = "re",
                             terms = c("extrav", "texp [2, 10, 25]"))
```


---
# Interpreting the interaction

```{r}
pop_m3_int_pred
```

---
# Interpreting the interaction

```{r, fig.retina=3}
plot(pop_m3_int_pred, ci = FALSE)
```

---
# Longitudinal data

Analyzing longitudinal data with mixed models is approached in generally the same way as analyzing other nested data. 


--
With longitudinal (or repeated-measures) data, the main form of clustering is by participant: repeated measures taken of the outcome for each participant (e.g. at baseline, immediately after an intervention, and 1 year after the intervention).


--
This said, we can still have the same forms of clustering/nesting that we've talked about already:


--
- Multiple assessments of students in different classrooms, schools, etc.


--
- Multiple assessments of patients at different clinics, hospitals, etc. 


--
We can model these different kinds of dependencies using mixed models, and because we are still working in the GLM framework, we can also analyze different types of outcomes. 


---
# Respire data

The `respire` dataset is from a randomized controlled trial (RCT) of a drug for the treatment of a respiratory illness. The outcome was whether the patient developed the illness, and was measured in 111 participants across 2 medical centers. The outcome was measured 4 times for each participant: baseline (pretreatment) and at 3 follow-up visits. 

* `outcome`: developed respiratory illness (Yes or No)
* `visit`: measurement point (Baseline, F1, F2, F3)
* `assignment`: group assignment (placebo - P or active - A)
* `center`: treatment center (1/2)
* `id`: patient ID
* `sex`: patient sex (M or F)
* `age`: patient age (range 11-68 years)


---
# Respire data

```{r}
head(respire, n = 10) %>% print_md()
```

---
# Longitudinal data

When analyzing longitudinal data, an additional consideration is how the outcome changes over time. This is especially important when we are comparing outcomes across groups, such as a group that received an intervention/treatment and a group that did not (control group).


--
If we just look at the main effect of *treatment group assignment* (whether someone received the treatment or not), then we are combining the outcome values across measurement points.


--
- It's common to see a large treatment effect right after the intervention, and then the effect gets smaller over time. If we combine large and small outcome estimates together, we get a smaller overall estimate. 


--
If we just look at the main effect of *time* (e.g., follow-up visit or assessment number), then we are ignoring group assignment. This doesn't give us insight into whether the treatment is effective.


--
What do you think is the solution?


--
- `treatment x time` interaction


---
# Model fitting

--
1) Null model including a random effect for participant, to calculate the ICC:

```{r}
respire_m0 <- glmer(outcome ~ 1 + (1 | id), family = binomial, 
                    data = respire)
performance::icc(respire_m0)
```

--
2) Random-intercepts model including the `treatment x time` interaction and control variables (sex and age). For random-effects, we generally want at least 5 units. Here, we only have 2 centers, so we'll instead include a fixed effect for center (i.e., a control variable).

```{r}
respire_m1 <- glmer(outcome ~ assignment*visit + sex + age.c + center + 
                      (1 | id), family = binomial, 
                    control = glmerControl(optimizer = "bobyqa", 
                                           optCtrl = list(maxfun = 2e5)),
                    data = respire)
```

---
# Model fitting 

3) Random-intercepts-and-slopes model to test whether random slopes are needed for time:

```{r, warning=FALSE}
respire_m2 <- glmer(outcome ~ assignment*visit + sex + age.c + center + 
                      (visit | id), family = binomial, 
                    control = glmerControl(optimizer = "bobyqa", 
                                           optCtrl = list(maxfun = 2e5)),
                    data = respire)
```

--
```{r}
anova(respire_m1, respire_m2)
```

---
# Model fitting 

```{r}
performance::compare_performance(respire_m1, respire_m2, metrics = "common") %>% 
  print_md()
```


---
# Model diagnostics 

```{r, fig.retina=3}
respire_m1_residuals <- simulateResiduals(respire_m1, n = 1000) 
plot(respire_m1_residuals)
```

---
# Model diagnostics 

```{r, fig.retina=3}
check_model(respire_m1, check = "vif")
```

---
# Interpretation

```{r}
# using the ggeffects package
respire_m1_probs <- ggeffect(respire_m1,
                             type = "re",
                             terms = c("visit", "assignment"))
respire_m1_probs
```

---
# Interpretation

```{r, fig.retina=3}
plot(respire_m1_probs)
```

---
# A final note about cluster-robust SEs

Cluster-robust (or sandwich) standard errors are sometimes used instead of mixed-effects models (that is, instead of explicitly incorporating the nested structure of the data into the analysis). Although they're more straightforward to implement, these SEs have several limitations:


--
- They don't give us insight into whether the *relationship* between the predictor(s) and outcome differs across clusters.


--
- They don't address the issue of misinterpreting the variability explained by our model, meaning they shouldn't be used unless the null ICC is very small (e.g., less than 5%).


--
- They're only appropriate for simple forms of clustering and when sampling or randomization was conducted at the same level of the cluster. 


--
For all of these reasons, it's generally better to use a model that is more informative about, and better accounts for, the nested structure of your data (and you can still use cluster-robust SEs!). 





