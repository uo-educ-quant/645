<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Nested Data</title>
    <meta charset="utf-8" />
    <script src="EDUC645_nested_data_Sp23_files/header-attrs-2.21/header-attrs.js"></script>
    <link href="EDUC645_nested_data_Sp23_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="EDUC645_nested_data_Sp23_files/remark-css-0.0.1/uo.css" rel="stylesheet" />
    <link href="EDUC645_nested_data_Sp23_files/remark-css-0.0.1/ki-fonts.css" rel="stylesheet" />
    <link rel="stylesheet" href="my_custom.css" type="text/css" />
    <link rel="stylesheet" href="xaringanthemer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Nested Data
]
.subtitle[
## EDUC 645 (Spring 2023)
]

---





# What is Nested Data?


--
Nesting occurs when variability in our outcome occurs at multiple levels.


--
A common example we've already discussed is student outcomes: If Oregon were interested in looking at factors that influence students' academic performance, performance is expected to differ from student to student. Can performance vary at other levels?


--
- Performance could also vary from classroom to classroom (on average) and from school to school (on average). If the federal government were interested in this question, performance may also vary from state to state.


--
What are other examples of nesting you can think of?


--
Another form of nesting occurs in longitudinal studies, where the outcome is assessed multiple times for each participant. 


---
# What is Nested Data?

One way of thinking of nesting is as a form of &lt;span style = "color:green"&gt; clustering&lt;/span&gt;: There are clusters of observations in our data that share something in common. 


--
- Observations share the same context (e.g., the same classroom or school, when there are many classrooms or schools in our data) or could be clustered within each person in our dataset (multiple observations from the same person). 


--
Clustered observations are dependent, meaning they are more similar than they would be if they were unclustered (independent) observations.


--
In the academic performance example, what do you think happens if we don't account for variability in performance at the classroom and school levels? And what about the longitudinal study example, where we have dependent observations within each person?


--
- We get biased estimates of effects and precision.



---
# Analyzing Nested Data

--
In addition to more accurate estimates when we model nested or clustered data appropriately, we might be interested in different relationships at each level:


--
- Student level: Do students who are not routinely bullied have better performance than students who are routinely bullied?


--
- Classroom level: Does the presence of a teaching assistance result in better student performance, compared with classrooms that don't have a teaching assistant?


--
- School level: Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
With a model for nested data - generally called a multilevel model (MLM) or hierarchical linear model (HLM) - we can account for variability at multiple levels *and* test hypotheses at one or more levels using a single model. 


---
# Aggregation Bias

A final concern with nested data is that when we ignore nesting, we risk &lt;span style = "color:green"&gt; aggregation bias&lt;/span&gt;. 


--
Imagine we were studying the school-level question on the previous slide, Do schools with a sufficient number of academic counselors report better student performance than schools with inadequate academic counseling?


--
- In this example, school-level data was the mean student performance level for each school, and classroom-level data was the mean student performance level for each classroom. The higher level data are &lt;span style = "color:green"&gt; aggregate &lt;/span&gt;data, in the sense that means are aggregates of more granular data. 


--
If we modeled the relationship between two variables - whether or not the school had a sufficient number of academic counselors (IV) and mean student performance for each school (DV) - we might find that having adequate academic counseling was associated with greater average student performance. 


--
- Would this approach give us enough insight to conclude that policy-makers should provide more resources for academic counseling?


--
It might, but what if this relationship was different in each classroom? 





---
# Aggregation Bias

Maybe there was little or no relationship in small classrooms and a stronger relationship in larger classrooms. What's a plausible reason why this could be the case?


--
- Perhaps in small classrooms, teachers can give more attention to students, while in larger classrooms, students are not getting adequate support. 


--
- So, academic counseling makes more of a difference in the larger classrooms, because it is making up for support the teacher is not able to provide. In the smaller classrooms, the students don't need more support and are already high performing without academic counseling. 


--
Aggregation bias would occur if we concluded from our model that adequate academic counseling improves average student performance at the school level, when the relationship is actually more complex and depends on lower-level factors like class size (or presence of teaching assistants, etc). 


--
- If a policy-maker concluded from our school-level findings that the solution was for schools to have more funding for academic counseling, that might be wasting resources if more students would benefit from addressing the underlying, classroom-level issue (i.e., reducing class size by increasing funding to hire more teachers, have more classroom space, etc).


---
# Analyzing Nested Data

Several methods can be used to account for nesting or clustering of data so that we get more accurate estimates of associations (or effects) and error/significance. Some methods also allow for investigating relationships at multiple levels.


--
&lt;span style = "color:green"&gt; Adjusting standard errors: &lt;/span&gt; The most straightforward approach is to adjust the model standard errors for clustering. This is based on the idea that a naive model (where the clusters are ignored) would consider every observation independent, in which case there would appear to be more information in the data than there really is (resulting in artificially narrow confidence intervals). 


--
&lt;span style = "color:green"&gt; Subgroup-level analyses: &lt;/span&gt; With large enough sample sizes and appropriate adjustment for multiple comparisons, we could carry out our analyses within each subgroup or cluster.


--
&lt;span style = "color:green"&gt; Multilevel (or mixed-effects) models: &lt;/span&gt; The most common analytic approach for nested data, which can address dependency among observations and can be used to examine the relationships between the outcome and (different) predictors at each level. 


--
&lt;span style = "color:green"&gt; Repeated-measures models: &lt;/span&gt; With longitudinal data, there are several approaches that can be used to account for repeated observations from the same participant (e.g., growth curve modeling).


---
# Example

The Stanford Education Data Archive (SEDA) was launched in 2016 to provide nationally comparable, publicly available test score data for U.S. public school districts. 


--
SEDA allows researchers to study relationships between educational conditions, contexts, and outcomes (e.g., student math achievement) at the district level across the nation.


--
We'll look at district-level data for 103 Oregon school districts from the 2017-18 academic year. 

- Each district has 4 observations (rows), one for each grade from 3-6.

- Observations with missing values on any of the key variables were deleted for simplification.


---
# View the data 


```r
head(seda, n = 5) %&gt;% 
  print_md() 
```



|district             |  grade | achievement| gap_gender| percent_ell| percent_sped| percent_frl|
|:--------------------|:-------|-----------:|----------:|-----------:|------------:|-----------:|
|VALE SD 84           |Grade 3 |        2.54|       0.97|        0.05|         0.13|        0.62|
|VALE SD 84           |Grade 4 |        3.09|      -0.79|        0.05|         0.13|        0.62|
|VALE SD 84           |Grade 5 |        4.57|       0.84|        0.05|         0.13|        0.62|
|VALE SD 84           |Grade 6 |        6.03|       0.86|        0.05|         0.13|        0.62|
|YAMHILL CARLTON SD 1 |Grade 3 |        3.15|      -1.09|        0.01|         0.14|        0.43|


--
Notice there are 4 observations for each grade in each district. 


--
Whenever you have multiple observations that share something in common (share the same district, group, person), this is an indication that you probably have nested data! 


---
# Understanding clustered data

Let's look at the average math achievement score (`achievement`) across all grades for each Oregon district.


--
&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-2-1.png" width="576" style="display: block; margin: auto;" /&gt;


--
&lt;span style = "color:green"&gt; What is the relationship between proportion of ELL students and math achievement in Oregon districts? &lt;/span&gt;


---
# % ELL and achievement


--
We might consider averaging all 4 grades' math achievement scores in each district and fit a simple linear regression for the association between % ELL and math achievement:


--
&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-3-1.png" width="576" style="display: block; margin: auto;" /&gt;


--
But do we have more information if we didn't average the outcome over the 4 grades?

---
# Achievement for all grades

&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;

---
# Achievement for each grade

&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;


--
If we fit a model for each grade, what could differ about the regression lines for each grade?


---
# Relationship by grade

Fitting a simple linear regression model for each grade (i.e., 4 separate models):


--
&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-6-1.png" width="576" style="display: block; margin: auto;" /&gt;


--
Now that you see regression lines for each grade, what model parameters (intercepts, slopes) did end up differing for each grade?

---
# Intraclass correlation coefficient

We can estimate the differences we've been discussing using the &lt;span style = "color:green"&gt; intraclass correlation coefficient (ICC) &lt;/span&gt;. The ICC gives us an estimate of how much of the variability in our outcome is simply because of differences in the outcome between groups (in our example, grades). 


--
The ICC ranges from 0 to 1. The larger the ICC, the greater the proportion of variability in the outcome that is due to group differences. The ICC is one tool we can use to gauge whether clustering is important enough to warrant multilevel modeling.


--
&lt;span style = "color:green"&gt; The ICC for the math achievement outcome in our data is 0.645 or 64.5%. &lt;/span&gt; How would you interpret this? 


--
- 64.5% of the variation in the math achievement outcome is attributable to differences in achievement between grades. 


--
- Because the ICC is telling us that more than two-thirds of the variability in the outcome is because of grade (and has nothing to do with ELL proportion), we know that our findings could mask important differences in grade-level achievement.


---
# Simple linear regression


```r
m1 &lt;- lm(achievement ~ percent_ell, data = seda)
coef(m1) # Extracting intercept and %ELL coefficient
```

```
#&gt; (Intercept) percent_ell 
#&gt;    4.504827   -6.466889
```


--
This is called a &lt;span style = "color:green"&gt; fixed-effect model. &lt;/span&gt; We're assuming that groups (grades) are sufficiently similar that a single (or fixed) slope and intercept would give us an accurate enough picture of the relationship of interest.  

---
# Mixed-effects model

Instead of the simple linear regression (fixed-effect model), we can fit a mixed-effects model that allows intercepts, slopes, or both to vary across grade. For now, let's look at what happens if we let the intercept (the average math achievement level at 0% ELL) vary by grade. We'll use the `lmer()` function from the base R package *lme4*.


--

```r
m2 &lt;- lmer(achievement ~ percent_ell + (1 | grade), data = seda)
```


--
The same function and formula style are used for all the mixed-effects models we'll use. The only difference from the linear regression specification you're used to is the element in parentheses. 


--
- This is the "random effect" part of the model, or what we want to allow to differ across groups/clusters. 


--
- What is after the `|` is the variable that indicates the group/cluster. What is before the `|` is what we're interested in estimating for each group. Like the basic regression model where 1 is used to fit a model that estimates only the intercept, here the `(1 | grade)` means "only the intercept (1) can vary by grade." 

---
# Mixed-effects models


```r
coef(m2)$grade # Specify the grouping variable
```

```
#&gt;         (Intercept) percent_ell
#&gt; Grade 3    2.856961   -6.466889
#&gt; Grade 4    3.913716   -6.466889
#&gt; Grade 5    5.065668   -6.466889
#&gt; Grade 6    6.182964   -6.466889
```


--
This is called a &lt;span style = "color:green"&gt; random intercepts model &lt;/span&gt; because the intercept is allowed to vary for each grade. In practice, before fitting a mixed-effects model, we'd fit the equivalent of the null model we're used to and use it to calculate the ICC. 


--

```r
m0 &lt;- lmer(achievement ~ 1 + (1 | grade), data = seda)
performance::icc(m0)
```

```
#&gt; # Intraclass Correlation Coefficient
#&gt; 
#&gt;     Adjusted ICC: 0.645
#&gt;   Unadjusted ICC: 0.645
```

---
# Back to variability explained


```r
performance::icc(m0)
```

```
#&gt; # Intraclass Correlation Coefficient
#&gt; 
#&gt;     Adjusted ICC: 0.645
#&gt;   Unadjusted ICC: 0.645
```



--

```r
performance::r2_nakagawa(m0)
```

```
#&gt; # R2 for Mixed Models
#&gt; 
#&gt;   Conditional R2: 0.645
#&gt;      Marginal R2: 0.000
```

---
# Back to variability explained

&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-13-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Mixed-effects models

So far we've considered one intercept and slope for all grades or examining individual intercepts for each grade, and we have a good sense from our plot earlier that the random-intercepts model is probably sufficient for our data. 


--
But let's illustrate what happens with the remaining two modeling options:


--
- We can look at slopes for each grade, but only one intercept across grades (random-slopes model). 


--
- Or, we can examine intercepts *and* slopes for each grade (random-effects model)


---
# Mixed-effects models


--
&lt;span style = "color:green"&gt; Random-slopes model: &lt;/span&gt;Individual slope for each grade (`percent_ell|grade`), but intercept is fixed (0). Note that in practice, we'd often move from fixed-effect &gt; random-intercepts &gt; random-slopes-and-intercepts (the full "random-effects model").


--

```r
m3 &lt;- lmer(achievement ~ 1 + (0 + percent_ell|grade), data = seda)
coef(m3)$grade
```

```
#&gt;         percent_ell (Intercept)
#&gt; Grade 3  -16.307389     4.47197
#&gt; Grade 4  -10.263599     4.47197
#&gt; Grade 5   -2.428400     4.47197
#&gt; Grade 6    4.669357     4.47197
```


--
&lt;span style = "color:green"&gt; Random-effects model: &lt;/span&gt;Slopes (`percent_ell|grade`) and intercepts (1) for each grade


--

```r
m4 &lt;- lmer(achievement ~ percent_ell + (percent_ell|grade), data = seda)
coef(m4)$grade
```

```
#&gt;         (Intercept) percent_ell
#&gt; Grade 3    2.826638   -6.120839
#&gt; Grade 4    3.904292   -6.343056
#&gt; Grade 5    5.075495   -6.584562
#&gt; Grade 6    6.212885   -6.819096
```


---
# Fixed effect vs random effects

&lt;img src="EDUC645_nested_data_Sp23_files/figure-html/unnamed-chunk-16-1.png" width="576" style="display: block; margin: auto;" /&gt;

---
# Bringing it together

--
.pull-left[
The &lt;span style = "color:green"&gt; random-effects model &lt;/span&gt; gives us the slope and intercept for each cluster:


```r
coef(m4)$grade
```

```
#&gt;         (Intercept) percent_ell
#&gt; Grade 3    2.826638   -6.120839
#&gt; Grade 4    3.904292   -6.343056
#&gt; Grade 5    5.075495   -6.584562
#&gt; Grade 6    6.212885   -6.819096
```
]

--
.pull-right[
The &lt;span style = "color:green"&gt; fixed-effect model &lt;/span&gt; gives us the average (or aggregation) of the different intercepts and slopes across grades:
 

```r
coef(m1)
```

```
#&gt; (Intercept) percent_ell 
#&gt;    4.504827   -6.466889
```
]


--
.pull-left[
In other words, the fixed-effect results are the average of the random-effects model results:
]


--
.pull-right[

```
#&gt;      (Intercept) percent_ell
#&gt; [1,]    4.504827   -6.466889
```
]


--
So, the primary question guiding us through our model fitting process is the same as it is for any average: Is an average a good enough representation of the underlying range of estimates (values of intercepts and slopes) to be interpretable on its own, or do we need to look at the underlying range to have a better understanding. 


---
# Comparison

We can use many of the same tools we've already used to compare models, including deviance tests and AIC. We'll also use RMSE (root mean square error), which is the average difference between values predicted by the model and the actual values in our data (so, lower RMSE = better fitting).


--

```r
performance::test_likelihoodratio(m1, m2) %&gt;% 
  print_md() # Only for slides
```



|Name |           Model| df| df_diff|   Chi2|        p|
|:----|---------------:|--:|-------:|------:|--------:|
|m1   |              lm|  3|        |       |         |
|m2   | lmerModLmerTest|  4|       1| 159.56| 1.41e-36|


--

```r
performance::test_likelihoodratio(m2, m4) %&gt;% 
  print_md()
```



|Name |          Model | df| df_diff| Chi2|    p|
|:----|:---------------|--:|-------:|----:|----:|
|m2   |lmerModLmerTest |  4|        |     |     |
|m4   |lmerModLmerTest |  6|       2| 0.08| 0.96|


---
# Comparison


```r
performance::compare_performance(m1, m2, m4, metrics = "common") %&gt;% 
  print_md()
```



Table: Comparison of Model Performance Indices

|Name |           Model | AIC (weights) |    BIC (weights) | RMSE | R2 (cond.) | R2 (marg.) |  ICC |   R2 | R2 (adj.) |
|:----|:---------------:|:-------------:|:----------------:|:----:|:----------:|:----------:|:----:|:----:|:---------:|
|m1   |              lm | 663.7 (&lt;.001) |    673.2 (&lt;.001) | 1.57 |            |            |      | 0.09 |      0.08 |
|m2   | lmerModLmerTest |  506.2 (0.88) |     518.8 (0.99) | 0.94 |       0.72 |       0.07 | 0.70 |      |           |
|m4   | lmerModLmerTest |  510.1 (0.12) | 529.1 (6.00e-03) | 0.94 |       0.72 |       0.07 | 0.70 |      |           |


--
So, the random-intercepts model is better fitting than the fixed-effect model (the simple linear regression), but the random-effects model is not any better fitting than the random-intercepts model.


&lt;!-- --- --&gt;
&lt;!-- # To Dos --&gt;

&lt;!-- ### Assignments --&gt;

&lt;!-- - Assignment 2 is due May 12. --&gt;

&lt;!-- ### Reading --&gt;

&lt;!-- - Complete prior to May 16 meeting: BMLR Ch. 8.6-8.11 (pp. 234-251). --&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
